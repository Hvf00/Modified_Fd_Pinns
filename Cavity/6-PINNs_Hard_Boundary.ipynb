{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "cYWnsVAw64I9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "EYj9dt0m8YPK"
   },
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self,num_input=2,layers=[64, 64, 64],num_output=3):\n",
    "        super(Network,self).__init__()\n",
    "        self.input_layer=nn.Linear(num_input,layers[0])\n",
    "        self.hidden_layer=nn.ModuleList()\n",
    "        for i in range(len(layers)-1):\n",
    "            self.hidden_layer.append(nn.Linear(layers[i],layers[i+1]))\n",
    "        self.output_layer=nn.Linear(layers[-1],num_output)\n",
    "    def forward(self,out):\n",
    "        out=torch.tanh(self.input_layer(out))\n",
    "        for layer in self.hidden_layer:\n",
    "            out=torch.tanh(layer(out))\n",
    "        out=self.output_layer(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pinns:\n",
    "    def __init__(self):\n",
    "        # Transfer to GPU if it is possible\n",
    "        self.device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        self.network = Network().to(self.device)\n",
    "\n",
    "        # Assumptions\n",
    "        self.rho=1.0\n",
    "        self.nu=0.01\n",
    "        self.u0=1.0\n",
    "\n",
    "        # Domain_Definition\n",
    "        dx=0.01\n",
    "        dy=0.01\n",
    "        self.x = torch.arange(0, 1 + dx, dx).to(self.device)\n",
    "        self.y = torch.arange(0, 1 + dy, dy).to(self.device)\n",
    "        self.X = torch.stack(torch.meshgrid(self.x,self.y)).reshape(2,-1).T\n",
    "        self.X.requires_grad = True\n",
    "\n",
    "        # Boundary_condition\n",
    "        dx_b=0.001\n",
    "        dy_b=0.001\n",
    "        self.x_b = torch.arange(0,1+dx_b,dx_b)\n",
    "        self.y_b = torch.arange(0,1+dy_b,dy_b)        \n",
    "        self.X_train = torch.stack(torch.meshgrid(self.x_b,self.y_b[-1])).reshape(2,-1).T\n",
    "        self.uv_train = torch.stack(torch.meshgrid(self.u0*self.x_b[-1],torch.zeros_like(self.y_b))).reshape(2,-1).T        \n",
    "        \n",
    "        # Transfer tensor to GPU\n",
    "        self.uv_train = self.uv_train.to(self.device)\n",
    "        self.X_train = self.X_train.to(self.device)\n",
    "        self.X = self.X.to(self.device)\n",
    "        \n",
    "        # Error criterion Definition\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "        # Optimizer setting\n",
    "        self.adam = torch.optim.Adam(self.network.parameters())\n",
    "\n",
    "        # Limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS)\n",
    "        self.optimizer = torch.optim.LBFGS(\n",
    "        self.network.parameters(),\n",
    "            lr=0.01,\n",
    "            max_iter = 2000,\n",
    "            max_eval = 2000,\n",
    "            history_size = 50,\n",
    "            tolerance_grad = 1e-8,\n",
    "            tolerance_change = 1.0* np.finfo(float).eps,\n",
    "            line_search_fn =\"strong_wolfe\"\n",
    "        )\n",
    "    # Compute derivatives\n",
    "    def gradient(self,input,index):\n",
    "        output = torch.autograd.grad(\n",
    "            input,\n",
    "            self.X,\n",
    "            grad_outputs=torch.ones_like(input),\n",
    "            create_graph=True,\n",
    "            retain_graph=True\n",
    "        )[0]\n",
    "        return output[:,index]\n",
    "\n",
    "    def loss_f(self):\n",
    "        #Restart Optimizer\n",
    "        self.adam.zero_grad()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        #output of NN for boundary\n",
    "        self.uv_P_b = self.network(self.X_train)\n",
    "        self.u_P_b = self.uv_P_b[:,0]\n",
    "        \n",
    "        #loss data definition\n",
    "        self.loss_data = self.criterion(self.u_P_b,self.uv_train[:,0])\n",
    "        \n",
    "\n",
    "        #output of NN\n",
    "        self.uvp_P = self.network(self.X)\n",
    "        self.u_P_hat = self.uvp_P[:,0]\n",
    "        self.v_P_hat= self.uvp_P[:,1]\n",
    "        self.p_P = self.uvp_P[:,2]\n",
    "\n",
    "        # Trial function\n",
    "        x = self.X[:, 0]\n",
    "        y = self.X[:, 1]\n",
    "        self.u_P = x * (1 - x) * y * self.u_P_hat\n",
    "        self.v_P = x * (1 - x) * y * (1 - y) * self.v_P_hat\n",
    "        #compute derivations\n",
    "        self.du_dx = self.gradient(self.u_P,0)\n",
    "        self.du_dy = self.gradient(self.u_P,1)\n",
    "        self.du_dxx= self.gradient(self.du_dx,0)\n",
    "        self.du_dyy= self.gradient(self.du_dy,1)\n",
    "        self.dv_dx = self.gradient(self.v_P,0)\n",
    "        self.dv_dy = self.gradient(self.v_P,1)\n",
    "        self.dv_dxx= self.gradient(self.dv_dx,0)\n",
    "        self.dv_dyy= self.gradient(self.dv_dy,1)\n",
    "        self.dp_dx = self.gradient(self.p_P,0)\n",
    "        self.dp_dy = self.gradient(self.p_P,1)\n",
    "\n",
    "        # compute equation loss\n",
    "        self.con_equ=self.du_dx+self.dv_dy\n",
    "        self.u_eqn = self.uvp_P[:,0]*self.du_dx + self.uvp_P[:,1]*self.du_dy + self.dp_dx/self.rho - self.nu*(self.du_dxx + self.du_dyy)\n",
    "        self.v_eqn = self.uvp_P[:,0]*self.dv_dx + self.uvp_P[:,1]*self.dv_dy + self.dp_dy/self.rho - self.nu*(self.dv_dxx + self.dv_dyy)\n",
    "\n",
    "        #loss PDE definition\n",
    "        self.loss_pde = self.criterion(self.con_equ,torch.zeros_like(self.con_equ))+self.criterion(self.u_eqn,torch.zeros_like(self.u_eqn))+\\\n",
    "                        self.criterion(self.v_eqn,torch.zeros_like(self.v_eqn))\n",
    "\n",
    "        # The total loss is now only the PDE loss, as BCs are enforced mathematically.\n",
    "        self.loss = self.loss_pde+self.loss_data\n",
    "        \n",
    "        self.loss.backward()\n",
    "        return self.loss\n",
    "    def train(self, num_epochs=1):\n",
    "        self.network.train()\n",
    "        for i in range(num_epochs):\n",
    "            loss = self.loss_f()\n",
    "            if i % 10 == 0:\n",
    "                print(f\"Iteration {i}, Loss: {self.loss.item():.12f}\")\n",
    "            if i % 1000 == 0:\n",
    "                torch.save(self.network.state_dict(), r'/home/hossein_vasheghani/saving_model/model_new.pth')\n",
    "            self.adam.step(self.loss_f)\n",
    "        self.optimizer.step(self.loss_f)\n",
    "    def plot(self):\n",
    "        import os\n",
    "        os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "        self.network.eval()\n",
    "        with torch.no_grad():\n",
    "            self.u = self.uvp_P[:,0].reshape(len(self.x), len(self.y)).cpu().numpy().T\n",
    "            self.v = self.uvp_P[:,1].reshape(len(self.x), len(self.y)).cpu().numpy().T\n",
    "            self.p = self.uvp_P[:,2].reshape(len(self.x), len(self.y)).cpu().numpy().T\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.subplot(2, 2, 1)\n",
    "        contour1 = plt.contourf(self.x.detach().cpu().numpy(), self.y.detach().cpu().numpy(), self.u, levels=50, cmap=\"jet\")\n",
    "        plt.colorbar(contour1)\n",
    "        plt.xlabel(\"X\")\n",
    "        plt.ylabel(\"Y\")\n",
    "        plt.title(\"u\")\n",
    "        plt.subplot(2, 2, 2)\n",
    "        contour2 = plt.contourf(self.x.detach().cpu().numpy(), self.y.detach().cpu().numpy(), self.v, levels=50, cmap=\"jet\")\n",
    "        plt.colorbar(contour2)\n",
    "        plt.xlabel(\"X\")\n",
    "        plt.ylabel(\"Y\")\n",
    "        plt.title(\"v\")\n",
    "        plt.subplot(2, 2, 3)\n",
    "        contour3 = plt.contourf(self.x.detach().cpu().numpy(), self.y.detach().cpu().numpy(), self.p, levels=50, cmap=\"jet\")\n",
    "        plt.colorbar(contour3)\n",
    "        plt.xlabel(\"X\")\n",
    "        plt.ylabel(\"Y\")\n",
    "        plt.title(\"pressure\")\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.subplot(2, 2, 4)\n",
    "        contour4 = plt.streamplot(self.x.detach().cpu().numpy(), self.y.detach().cpu().numpy(), self.u, self.v, color=self.u, linewidth=2, cmap='autumn')\n",
    "        plt.xlabel(\"X\")\n",
    "        plt.ylabel(\"Y\")\n",
    "        plt.title(\"streamline\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "m-djFuhswO91",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 0.820908248425\n",
      "Iteration 10, Loss: 0.216875359416\n",
      "Iteration 20, Loss: 0.140392571688\n",
      "Iteration 30, Loss: 0.115414842963\n",
      "Iteration 40, Loss: 0.106712803245\n",
      "Iteration 50, Loss: 0.098279431462\n",
      "Iteration 60, Loss: 0.089764669538\n",
      "Iteration 70, Loss: 0.082293979824\n",
      "Iteration 80, Loss: 0.075790271163\n",
      "Iteration 90, Loss: 0.069477818906\n",
      "Iteration 100, Loss: 0.063431009650\n",
      "Iteration 110, Loss: 0.058568183333\n",
      "Iteration 120, Loss: 0.055509045720\n",
      "Iteration 130, Loss: 0.054061312228\n",
      "Iteration 140, Loss: 0.053247697651\n",
      "Iteration 150, Loss: 0.052526142448\n",
      "Iteration 160, Loss: 0.051756143570\n",
      "Iteration 170, Loss: 0.050943344831\n",
      "Iteration 180, Loss: 0.050135124475\n",
      "Iteration 190, Loss: 0.049373049289\n",
      "Iteration 200, Loss: 0.048687547445\n",
      "Iteration 210, Loss: 0.048064410686\n",
      "Iteration 220, Loss: 0.047513373196\n",
      "Iteration 230, Loss: 0.047041323036\n",
      "Iteration 240, Loss: 0.046628773212\n",
      "Iteration 250, Loss: 0.046258486807\n",
      "Iteration 260, Loss: 0.045917812735\n",
      "Iteration 270, Loss: 0.045601379126\n",
      "Iteration 280, Loss: 0.045308098197\n",
      "Iteration 290, Loss: 0.045034721494\n",
      "Iteration 300, Loss: 0.044771775603\n",
      "Iteration 310, Loss: 0.044506289065\n",
      "Iteration 320, Loss: 0.044227421284\n",
      "Iteration 330, Loss: 0.043929483742\n",
      "Iteration 340, Loss: 0.043611414731\n",
      "Iteration 350, Loss: 0.043276030570\n",
      "Iteration 360, Loss: 0.042928501964\n",
      "Iteration 370, Loss: 0.042573057115\n",
      "Iteration 380, Loss: 0.042211133987\n",
      "Iteration 390, Loss: 0.041863676161\n",
      "Iteration 400, Loss: 0.041491299868\n",
      "Iteration 410, Loss: 0.041101019830\n",
      "Iteration 420, Loss: 0.040715023875\n",
      "Iteration 430, Loss: 0.040309567004\n",
      "Iteration 440, Loss: 0.039875335991\n",
      "Iteration 450, Loss: 0.039414405823\n",
      "Iteration 460, Loss: 0.038920529187\n",
      "Iteration 470, Loss: 0.038393851370\n",
      "Iteration 480, Loss: 0.037838838995\n",
      "Iteration 490, Loss: 0.037266153842\n",
      "Iteration 500, Loss: 0.036687750369\n",
      "Iteration 510, Loss: 0.036119129509\n",
      "Iteration 520, Loss: 0.035607222468\n",
      "Iteration 530, Loss: 0.035006590188\n",
      "Iteration 540, Loss: 0.034424569458\n",
      "Iteration 550, Loss: 0.033871158957\n",
      "Iteration 560, Loss: 0.033329069614\n",
      "Iteration 570, Loss: 0.032788157463\n",
      "Iteration 580, Loss: 0.032238490880\n",
      "Iteration 590, Loss: 0.031672362238\n",
      "Iteration 600, Loss: 0.031080817804\n",
      "Iteration 610, Loss: 0.030456740409\n",
      "Iteration 620, Loss: 0.029801204801\n",
      "Iteration 630, Loss: 0.029093962163\n",
      "Iteration 640, Loss: 0.028356334195\n",
      "Iteration 650, Loss: 0.027568487450\n",
      "Iteration 660, Loss: 0.026745794341\n",
      "Iteration 670, Loss: 0.025886148214\n",
      "Iteration 680, Loss: 0.025002803653\n",
      "Iteration 690, Loss: 0.024102620780\n",
      "Iteration 700, Loss: 0.023197837174\n",
      "Iteration 710, Loss: 0.023669390008\n",
      "Iteration 720, Loss: 0.021872535348\n",
      "Iteration 730, Loss: 0.020872544497\n",
      "Iteration 740, Loss: 0.019935127348\n",
      "Iteration 750, Loss: 0.019180161878\n",
      "Iteration 760, Loss: 0.018471103162\n",
      "Iteration 770, Loss: 0.018102915958\n",
      "Iteration 780, Loss: 0.017439110205\n",
      "Iteration 790, Loss: 0.016574565321\n",
      "Iteration 800, Loss: 0.016066484153\n",
      "Iteration 810, Loss: 0.016529947519\n",
      "Iteration 820, Loss: 0.015502900817\n",
      "Iteration 830, Loss: 0.014715295285\n",
      "Iteration 840, Loss: 0.014249992557\n",
      "Iteration 850, Loss: 0.013807461597\n",
      "Iteration 860, Loss: 0.017555464059\n",
      "Iteration 870, Loss: 0.013230772689\n",
      "Iteration 880, Loss: 0.013412073255\n",
      "Iteration 890, Loss: 0.012709434144\n",
      "Iteration 900, Loss: 0.012481722049\n",
      "Iteration 910, Loss: 0.012244495563\n",
      "Iteration 920, Loss: 0.012073044665\n",
      "Iteration 930, Loss: 0.013891032897\n",
      "Iteration 940, Loss: 0.012097338215\n",
      "Iteration 950, Loss: 0.011880803853\n",
      "Iteration 960, Loss: 0.011487489566\n",
      "Iteration 970, Loss: 0.011260898784\n",
      "Iteration 980, Loss: 0.011099909432\n",
      "Iteration 990, Loss: 0.010935665108\n",
      "Iteration 1000, Loss: 0.011483738199\n",
      "Iteration 1010, Loss: 0.014135854319\n",
      "Iteration 1020, Loss: 0.011037641205\n",
      "Iteration 1030, Loss: 0.010793974623\n",
      "Iteration 1040, Loss: 0.010532288812\n",
      "Iteration 3160, Loss: 0.006509884726\n",
      "Iteration 3170, Loss: 0.006419941783\n",
      "Iteration 3180, Loss: 0.006491388194\n",
      "Iteration 3190, Loss: 0.006414209027\n",
      "Iteration 3200, Loss: 0.006397165824\n",
      "Iteration 3210, Loss: 0.006379992235\n",
      "Iteration 3220, Loss: 0.006375093013\n",
      "Iteration 3230, Loss: 0.006368607748\n",
      "Iteration 3240, Loss: 0.006362313870\n",
      "Iteration 3250, Loss: 0.006356453523\n",
      "Iteration 3260, Loss: 0.006350681651\n",
      "Iteration 3270, Loss: 0.006345066708\n",
      "Iteration 3280, Loss: 0.006341041997\n",
      "Iteration 3290, Loss: 0.006601005327\n",
      "Iteration 3300, Loss: 0.015082816593\n",
      "Iteration 3310, Loss: 0.009749282151\n",
      "Iteration 3320, Loss: 0.007427438162\n",
      "Iteration 3330, Loss: 0.006677068304\n",
      "Iteration 3340, Loss: 0.006352217402\n",
      "Iteration 3350, Loss: 0.006381803658\n",
      "Iteration 3360, Loss: 0.006339921150\n",
      "Iteration 3370, Loss: 0.006333357189\n",
      "Iteration 3380, Loss: 0.006320523564\n",
      "Iteration 3390, Loss: 0.006314566359\n",
      "Iteration 3400, Loss: 0.006309193559\n",
      "Iteration 3410, Loss: 0.006305456161\n",
      "Iteration 3420, Loss: 0.006485220045\n",
      "Iteration 3430, Loss: 0.006589183584\n",
      "Iteration 3440, Loss: 0.007181754801\n",
      "Iteration 3450, Loss: 0.007939746603\n",
      "Iteration 3460, Loss: 0.007199621294\n",
      "Iteration 3470, Loss: 0.006413484924\n",
      "Iteration 3480, Loss: 0.006388386711\n",
      "Iteration 3490, Loss: 0.006278987043\n",
      "Iteration 3500, Loss: 0.006271322258\n",
      "Iteration 3510, Loss: 0.006309796590\n",
      "Iteration 3520, Loss: 0.013283781707\n",
      "Iteration 3530, Loss: 0.009449368343\n",
      "Iteration 3540, Loss: 0.008004231378\n",
      "Iteration 3550, Loss: 0.006654234603\n",
      "Iteration 3560, Loss: 0.006285039708\n",
      "Iteration 3570, Loss: 0.006351999938\n",
      "Iteration 3580, Loss: 0.006315166596\n",
      "Iteration 3590, Loss: 0.006477040704\n",
      "Iteration 3600, Loss: 0.006308550946\n",
      "Iteration 3610, Loss: 0.006264223717\n",
      "Iteration 3620, Loss: 0.006251590326\n",
      "Iteration 3630, Loss: 0.006241466850\n",
      "Iteration 3640, Loss: 0.006233328488\n",
      "Iteration 3650, Loss: 0.006237099413\n",
      "Iteration 3660, Loss: 0.006761343684\n",
      "Iteration 3670, Loss: 0.010472904891\n",
      "Iteration 3680, Loss: 0.007262248546\n",
      "Iteration 3690, Loss: 0.006590779871\n",
      "Iteration 3700, Loss: 0.006398788653\n",
      "Iteration 3710, Loss: 0.006342364009\n",
      "Iteration 3720, Loss: 0.006247562356\n",
      "Iteration 3730, Loss: 0.006225334480\n",
      "Iteration 3740, Loss: 0.006212868728\n",
      "Iteration 3750, Loss: 0.006207413506\n",
      "Iteration 3760, Loss: 0.006200909615\n",
      "Iteration 3770, Loss: 0.006211822852\n",
      "Iteration 3780, Loss: 0.007328859065\n",
      "Iteration 3790, Loss: 0.006795868278\n",
      "Iteration 3800, Loss: 0.007981362753\n",
      "Iteration 3810, Loss: 0.006306022406\n",
      "Iteration 3820, Loss: 0.006365295965\n",
      "Iteration 3830, Loss: 0.006272901781\n",
      "Iteration 3840, Loss: 0.006228405517\n",
      "Iteration 3850, Loss: 0.006188235711\n",
      "Iteration 3860, Loss: 0.006180036813\n",
      "Iteration 3870, Loss: 0.006173518952\n",
      "Iteration 3880, Loss: 0.006180713885\n",
      "Iteration 3890, Loss: 0.006592642516\n",
      "Iteration 3900, Loss: 0.011433296837\n",
      "Iteration 3910, Loss: 0.006372929085\n",
      "Iteration 3920, Loss: 0.006539127324\n",
      "Iteration 3930, Loss: 0.006447373889\n",
      "Iteration 3940, Loss: 0.006276144646\n",
      "Iteration 3950, Loss: 0.006208783481\n",
      "Iteration 3960, Loss: 0.006152987015\n",
      "Iteration 3970, Loss: 0.006168465596\n",
      "Iteration 3980, Loss: 0.006246068515\n",
      "Iteration 3990, Loss: 0.008092798293\n",
      "Iteration 4000, Loss: 0.006246164907\n",
      "Iteration 4010, Loss: 0.006165781990\n",
      "Iteration 4020, Loss: 0.006169405766\n",
      "Iteration 4030, Loss: 0.006173790433\n",
      "Iteration 4040, Loss: 0.006229707506\n",
      "Iteration 4050, Loss: 0.006317996886\n",
      "Iteration 4060, Loss: 0.006782092154\n",
      "Iteration 4070, Loss: 0.009297639132\n",
      "Iteration 4080, Loss: 0.006613461766\n",
      "Iteration 4090, Loss: 0.006145138759\n",
      "Iteration 4100, Loss: 0.006254431792\n",
      "Iteration 4110, Loss: 0.006142554805\n",
      "Iteration 4120, Loss: 0.006472634617\n",
      "Iteration 4130, Loss: 0.009672489017\n",
      "Iteration 4140, Loss: 0.007258294150\n",
      "Iteration 4150, Loss: 0.006459461059\n",
      "Iteration 4160, Loss: 0.006187768653\n",
      "Iteration 4170, Loss: 0.006200416479\n",
      "Iteration 4180, Loss: 0.006107219961\n",
      "Iteration 4190, Loss: 0.006101335865\n",
      "Iteration 4200, Loss: 0.006408074871\n",
      "Iteration 4210, Loss: 0.013219769113\n",
      "Iteration 4220, Loss: 0.006267931778\n",
      "Iteration 4230, Loss: 0.006878378335\n",
      "Iteration 4240, Loss: 0.006151611451\n",
      "Iteration 4250, Loss: 0.006122532301\n",
      "Iteration 4260, Loss: 0.006096630357\n",
      "Iteration 4270, Loss: 0.006097773556\n",
      "Iteration 4280, Loss: 0.006180348806\n",
      "Iteration 4290, Loss: 0.006101804320\n",
      "Iteration 4300, Loss: 0.006119444035\n",
      "Iteration 4310, Loss: 0.006580942310\n",
      "Iteration 4320, Loss: 0.011273286305\n",
      "Iteration 4330, Loss: 0.006778010633\n",
      "Iteration 4340, Loss: 0.006089108065\n",
      "Iteration 4350, Loss: 0.006110210437\n",
      "Iteration 4360, Loss: 0.006066108122\n",
      "Iteration 4370, Loss: 0.006114905700\n",
      "Iteration 4380, Loss: 0.006061470602\n",
      "Iteration 4390, Loss: 0.006075747777\n",
      "Iteration 4400, Loss: 0.006462985184\n",
      "Iteration 4410, Loss: 0.011679813266\n",
      "Iteration 4420, Loss: 0.008287209086\n",
      "Iteration 4430, Loss: 0.006563154049\n",
      "Iteration 4440, Loss: 0.006091478746\n",
      "Iteration 4450, Loss: 0.006055677775\n",
      "Iteration 4460, Loss: 0.006084965542\n",
      "Iteration 4470, Loss: 0.006060671527\n",
      "Iteration 4480, Loss: 0.006096684840\n",
      "Iteration 4490, Loss: 0.006509339903\n",
      "Iteration 4500, Loss: 0.009678244591\n",
      "Iteration 4510, Loss: 0.007165667601\n",
      "Iteration 4520, Loss: 0.006376897916\n",
      "Iteration 4530, Loss: 0.006061112974\n",
      "Iteration 4540, Loss: 0.006216899492\n",
      "Iteration 4550, Loss: 0.006210913882\n",
      "Iteration 4560, Loss: 0.007226591464\n",
      "Iteration 4570, Loss: 0.006864001974\n",
      "Iteration 4580, Loss: 0.006752753165\n",
      "Iteration 4590, Loss: 0.006129593123\n",
      "Iteration 4600, Loss: 0.006294186227\n",
      "Iteration 4610, Loss: 0.006446133368\n",
      "Iteration 4620, Loss: 0.007809435017\n",
      "Iteration 4630, Loss: 0.006051705685\n",
      "Iteration 4640, Loss: 0.006549377460\n",
      "Iteration 4650, Loss: 0.006030758843\n",
      "Iteration 4660, Loss: 0.006339834072\n",
      "Iteration 4670, Loss: 0.009033629671\n",
      "Iteration 4680, Loss: 0.006613483652\n",
      "Iteration 4690, Loss: 0.006073005963\n",
      "Iteration 4700, Loss: 0.006032578647\n",
      "Iteration 4710, Loss: 0.006212824956\n",
      "Iteration 4720, Loss: 0.006659871899\n",
      "Iteration 4730, Loss: 0.007169217337\n",
      "Iteration 4740, Loss: 0.006268602330\n",
      "Iteration 4750, Loss: 0.006137446500\n",
      "Iteration 4760, Loss: 0.007050490007\n",
      "Iteration 4770, Loss: 0.007248500828\n",
      "Iteration 4780, Loss: 0.006756831426\n",
      "Iteration 4790, Loss: 0.006024145056\n",
      "Iteration 4800, Loss: 0.006328984164\n",
      "Iteration 4810, Loss: 0.006654688157\n",
      "Iteration 4820, Loss: 0.006985877641\n",
      "Iteration 4830, Loss: 0.006014045328\n",
      "Iteration 4840, Loss: 0.006149498746\n",
      "Iteration 4850, Loss: 0.007869424298\n",
      "Iteration 4860, Loss: 0.006124765612\n",
      "Iteration 4870, Loss: 0.006260779686\n",
      "Iteration 4880, Loss: 0.006295402069\n",
      "Iteration 4890, Loss: 0.006155084819\n",
      "Iteration 4900, Loss: 0.006388588808\n",
      "Iteration 4910, Loss: 0.006569448393\n",
      "Iteration 4920, Loss: 0.008929625154\n",
      "Iteration 4930, Loss: 0.007128473837\n",
      "Iteration 4940, Loss: 0.006307324395\n",
      "Iteration 4950, Loss: 0.005975895561\n",
      "Iteration 4960, Loss: 0.006190688349\n",
      "Iteration 4970, Loss: 0.006488956511\n",
      "Iteration 4980, Loss: 0.007672930136\n",
      "Iteration 4990, Loss: 0.005975829903\n",
      "Iteration 5000, Loss: 0.006473751273\n",
      "Iteration 5010, Loss: 0.005999165587\n",
      "Iteration 5020, Loss: 0.005937194917\n",
      "Iteration 5030, Loss: 0.005953674670\n",
      "Iteration 5040, Loss: 0.006353180856\n",
      "Iteration 5050, Loss: 0.012320082635\n",
      "Iteration 5060, Loss: 0.008888440207\n",
      "Iteration 5070, Loss: 0.006742051337\n",
      "Iteration 5080, Loss: 0.005993197206\n",
      "Iteration 5090, Loss: 0.006094865967\n",
      "Iteration 5100, Loss: 0.005936572794\n",
      "Iteration 5110, Loss: 0.005941899028\n",
      "Iteration 5120, Loss: 0.005933432840\n",
      "Iteration 5130, Loss: 0.005925034638\n",
      "Iteration 5140, Loss: 0.005919428542\n",
      "Iteration 5150, Loss: 0.005929420236\n",
      "Iteration 5160, Loss: 0.006478137337\n",
      "Iteration 5170, Loss: 0.006349499337\n",
      "Iteration 5180, Loss: 0.007352094166\n",
      "Iteration 5190, Loss: 0.006391873118\n",
      "Iteration 5200, Loss: 0.006642979570\n",
      "Iteration 5210, Loss: 0.005976421759\n",
      "Iteration 5220, Loss: 0.006433980074\n",
      "Iteration 5230, Loss: 0.006936212536\n",
      "Iteration 5240, Loss: 0.006055589300\n",
      "Iteration 5250, Loss: 0.005967646837\n",
      "Iteration 5260, Loss: 0.006915266626\n",
      "Iteration 5270, Loss: 0.007740639616\n",
      "Iteration 5280, Loss: 0.006639617961\n",
      "Iteration 5290, Loss: 0.006245211698\n",
      "Iteration 5300, Loss: 0.006046381313\n",
      "Iteration 5310, Loss: 0.006043851376\n",
      "Iteration 5320, Loss: 0.006083732005\n",
      "Iteration 5330, Loss: 0.007597411051\n",
      "Iteration 5340, Loss: 0.006017813459\n",
      "Iteration 5350, Loss: 0.006234481465\n",
      "Iteration 5360, Loss: 0.006182278972\n",
      "Iteration 5370, Loss: 0.005892897490\n",
      "Iteration 5380, Loss: 0.005979436450\n",
      "Iteration 5390, Loss: 0.007313934620\n",
      "Iteration 5400, Loss: 0.007033921778\n",
      "Iteration 5410, Loss: 0.005911017302\n",
      "Iteration 5420, Loss: 0.005948273931\n",
      "Iteration 5430, Loss: 0.005889581516\n",
      "Iteration 5440, Loss: 0.005961297080\n",
      "Iteration 5450, Loss: 0.005875550210\n",
      "Iteration 5460, Loss: 0.005869409069\n",
      "Iteration 5470, Loss: 0.006083171815\n",
      "Iteration 5480, Loss: 0.013924328610\n",
      "Iteration 5490, Loss: 0.006923493464\n",
      "Iteration 5500, Loss: 0.006142062135\n",
      "Iteration 5510, Loss: 0.006223541219\n",
      "Iteration 5520, Loss: 0.005993557163\n",
      "Iteration 5530, Loss: 0.005928981118\n",
      "Iteration 5540, Loss: 0.005907181185\n",
      "Iteration 5550, Loss: 0.006019307300\n",
      "Iteration 5560, Loss: 0.005935123190\n",
      "Iteration 5570, Loss: 0.005987058859\n",
      "Iteration 5580, Loss: 0.009311224334\n",
      "Iteration 5590, Loss: 0.007184671238\n",
      "Iteration 5600, Loss: 0.006716776174\n",
      "Iteration 5610, Loss: 0.006108376663\n",
      "Iteration 5620, Loss: 0.006000523455\n",
      "Iteration 5630, Loss: 0.005890520755\n",
      "Iteration 5640, Loss: 0.005975421052\n",
      "Iteration 5650, Loss: 0.006050831638\n",
      "Iteration 5660, Loss: 0.006801069714\n",
      "Iteration 5670, Loss: 0.007477559615\n",
      "Iteration 5680, Loss: 0.006648398004\n",
      "Iteration 5690, Loss: 0.005996312946\n",
      "Iteration 5700, Loss: 0.006053630728\n",
      "Iteration 5710, Loss: 0.005979313515\n",
      "Iteration 5720, Loss: 0.006980621256\n",
      "Iteration 5730, Loss: 0.006993634161\n",
      "Iteration 5740, Loss: 0.006493558176\n",
      "Iteration 5750, Loss: 0.006166883744\n",
      "Iteration 5760, Loss: 0.005866880994\n",
      "Iteration 5770, Loss: 0.006316868123\n",
      "Iteration 5780, Loss: 0.006927114446\n",
      "Iteration 5790, Loss: 0.006922616158\n",
      "Iteration 5800, Loss: 0.006399047561\n",
      "Iteration 5810, Loss: 0.005841983948\n",
      "Iteration 5820, Loss: 0.006050904747\n",
      "Iteration 5830, Loss: 0.007721817121\n",
      "Iteration 5840, Loss: 0.005865407176\n",
      "Iteration 5850, Loss: 0.006154164672\n",
      "Iteration 5860, Loss: 0.006065831520\n",
      "Iteration 5870, Loss: 0.006078614853\n",
      "Iteration 5880, Loss: 0.006298290100\n",
      "Iteration 5890, Loss: 0.007488152012\n",
      "Iteration 5900, Loss: 0.005930203013\n",
      "Iteration 5910, Loss: 0.006251415703\n",
      "Iteration 5920, Loss: 0.005874237977\n",
      "Iteration 5930, Loss: 0.005917208735\n",
      "Iteration 5940, Loss: 0.008751021698\n",
      "Iteration 5950, Loss: 0.006345607340\n",
      "Iteration 5960, Loss: 0.006556564942\n",
      "Iteration 5970, Loss: 0.006120145321\n",
      "Iteration 5980, Loss: 0.005915421527\n",
      "Iteration 5990, Loss: 0.005783037283\n",
      "Iteration 6000, Loss: 0.005854099058\n",
      "Iteration 6010, Loss: 0.006507346872\n",
      "Iteration 6020, Loss: 0.008352039382\n",
      "Iteration 6030, Loss: 0.006029228680\n",
      "Iteration 6040, Loss: 0.005817145575\n",
      "Iteration 6050, Loss: 0.006071087904\n",
      "Iteration 6060, Loss: 0.005912865046\n",
      "Iteration 6070, Loss: 0.006232818123\n",
      "Iteration 6080, Loss: 0.008557460271\n",
      "Iteration 6090, Loss: 0.006517988630\n",
      "Iteration 6100, Loss: 0.005836435128\n",
      "Iteration 6110, Loss: 0.005893405527\n",
      "Iteration 6120, Loss: 0.005768689793\n",
      "Iteration 6130, Loss: 0.005847908556\n",
      "Iteration 6140, Loss: 0.009361656383\n",
      "Iteration 6150, Loss: 0.007099378854\n",
      "Iteration 6160, Loss: 0.006717358716\n",
      "Iteration 6170, Loss: 0.005863383412\n",
      "Iteration 6180, Loss: 0.005801987369\n",
      "Iteration 6190, Loss: 0.005838667508\n",
      "Iteration 6200, Loss: 0.005758969113\n",
      "Iteration 6210, Loss: 0.005788374227\n",
      "Iteration 6220, Loss: 0.006218546536\n",
      "Iteration 6230, Loss: 0.009108608589\n",
      "Iteration 6240, Loss: 0.006716183387\n",
      "Iteration 6250, Loss: 0.005944827106\n",
      "Iteration 6260, Loss: 0.005775800906\n",
      "Iteration 6270, Loss: 0.005905075930\n",
      "Iteration 6280, Loss: 0.006576420274\n",
      "Iteration 6290, Loss: 0.007253671065\n",
      "Iteration 6300, Loss: 0.006448977161\n",
      "Iteration 6310, Loss: 0.005743506365\n",
      "Iteration 6320, Loss: 0.006156522781\n",
      "Iteration 6330, Loss: 0.006642351393\n",
      "Iteration 6340, Loss: 0.006186681334\n",
      "Iteration 6350, Loss: 0.005751204211\n",
      "Iteration 6360, Loss: 0.005889408756\n",
      "Iteration 6370, Loss: 0.010281966999\n",
      "Iteration 6380, Loss: 0.007438510656\n",
      "Iteration 6390, Loss: 0.006475980859\n",
      "Iteration 6400, Loss: 0.005918847863\n",
      "Iteration 6410, Loss: 0.005866348278\n",
      "Iteration 6420, Loss: 0.005768185016\n",
      "Iteration 6430, Loss: 0.005943330470\n",
      "Iteration 6440, Loss: 0.006076319143\n",
      "Iteration 6450, Loss: 0.007814403623\n",
      "Iteration 6460, Loss: 0.005755250342\n",
      "Iteration 6470, Loss: 0.005913358182\n",
      "Iteration 6480, Loss: 0.006010755431\n",
      "Iteration 6490, Loss: 0.006092634052\n",
      "Iteration 6500, Loss: 0.006429988891\n",
      "Iteration 6510, Loss: 0.006599415094\n",
      "Iteration 6520, Loss: 0.005734433886\n",
      "Iteration 6530, Loss: 0.006139789242\n",
      "Iteration 6540, Loss: 0.007879285142\n",
      "Iteration 6550, Loss: 0.005723373499\n",
      "Iteration 6560, Loss: 0.006078013219\n",
      "Iteration 6570, Loss: 0.005878177471\n",
      "Iteration 6580, Loss: 0.006549586542\n",
      "Iteration 6590, Loss: 0.006621452980\n",
      "Iteration 6600, Loss: 0.005745898467\n",
      "Iteration 6610, Loss: 0.006237237249\n",
      "Iteration 6620, Loss: 0.007511652540\n",
      "Iteration 6630, Loss: 0.005792459473\n",
      "Iteration 6640, Loss: 0.006065502297\n",
      "Iteration 6650, Loss: 0.005716831423\n",
      "Iteration 6660, Loss: 0.006026169285\n",
      "Iteration 6670, Loss: 0.010003161617\n",
      "Iteration 6680, Loss: 0.007342324592\n",
      "Iteration 6690, Loss: 0.006206292193\n",
      "Iteration 6700, Loss: 0.005945006851\n",
      "Iteration 6710, Loss: 0.005766015965\n",
      "Iteration 6720, Loss: 0.006013344508\n",
      "Iteration 6730, Loss: 0.005838335957\n",
      "Iteration 6740, Loss: 0.006651919335\n",
      "Iteration 6750, Loss: 0.007033941802\n",
      "Iteration 6760, Loss: 0.006490534171\n",
      "Iteration 6770, Loss: 0.005977905821\n",
      "Iteration 6780, Loss: 0.005744121969\n",
      "Iteration 6790, Loss: 0.005673765671\n",
      "Iteration 6800, Loss: 0.005699028261\n",
      "Iteration 6810, Loss: 0.008033537306\n",
      "Iteration 6820, Loss: 0.006735457107\n",
      "Iteration 6830, Loss: 0.006648260634\n",
      "Iteration 6840, Loss: 0.005964734592\n",
      "Iteration 6850, Loss: 0.005881737918\n",
      "Iteration 6860, Loss: 0.005778556690\n",
      "Iteration 6870, Loss: 0.005812917370\n",
      "Iteration 6880, Loss: 0.005670246668\n",
      "Iteration 6890, Loss: 0.005679720081\n",
      "Iteration 6900, Loss: 0.005730949808\n",
      "Iteration 6910, Loss: 0.006887895521\n",
      "Iteration 6920, Loss: 0.006635274272\n",
      "Iteration 6930, Loss: 0.005887371954\n",
      "Iteration 6940, Loss: 0.005767214578\n",
      "Iteration 6950, Loss: 0.005889285821\n",
      "Iteration 6960, Loss: 0.005656176247\n",
      "Iteration 6970, Loss: 0.005766941234\n",
      "Iteration 6980, Loss: 0.005702657159\n",
      "Iteration 6990, Loss: 0.005844592117\n",
      "Iteration 7000, Loss: 0.012884410098\n",
      "Iteration 7010, Loss: 0.007033221889\n",
      "Iteration 7020, Loss: 0.006224215962\n",
      "Iteration 7030, Loss: 0.005953676533\n",
      "Iteration 7040, Loss: 0.005722355098\n",
      "Iteration 7050, Loss: 0.005847509485\n",
      "Iteration 7060, Loss: 0.005696926266\n",
      "Iteration 7070, Loss: 0.005642849952\n",
      "Iteration 7080, Loss: 0.005639290903\n",
      "Iteration 7090, Loss: 0.005687159020\n",
      "Iteration 7100, Loss: 0.007531500421\n",
      "Iteration 7110, Loss: 0.005724961869\n",
      "Iteration 7120, Loss: 0.006409704685\n",
      "Iteration 7130, Loss: 0.006068373565\n",
      "Iteration 7140, Loss: 0.005838891957\n",
      "Iteration 7150, Loss: 0.005675748922\n",
      "Iteration 7160, Loss: 0.005717215594\n",
      "Iteration 7170, Loss: 0.005786173977\n",
      "Iteration 7180, Loss: 0.007304967847\n",
      "Iteration 7190, Loss: 0.006213891786\n",
      "Iteration 7200, Loss: 0.006193365902\n",
      "Iteration 7210, Loss: 0.005817223340\n",
      "Iteration 7220, Loss: 0.006063862704\n",
      "Iteration 7230, Loss: 0.006340052001\n",
      "Iteration 7240, Loss: 0.006535573862\n",
      "Iteration 7250, Loss: 0.005662319250\n",
      "Iteration 7260, Loss: 0.005767282564\n",
      "Iteration 7270, Loss: 0.007904372178\n",
      "Iteration 7280, Loss: 0.005788858049\n",
      "Iteration 7290, Loss: 0.005767908413\n",
      "Iteration 7300, Loss: 0.005649692845\n",
      "Iteration 7310, Loss: 0.005772740580\n",
      "Iteration 7320, Loss: 0.005620963871\n",
      "Iteration 7330, Loss: 0.005679892376\n",
      "Iteration 7340, Loss: 0.007691270672\n",
      "Iteration 7350, Loss: 0.005940759555\n",
      "Iteration 7360, Loss: 0.006050736178\n",
      "Iteration 7370, Loss: 0.005959377624\n",
      "Iteration 7380, Loss: 0.005653102882\n",
      "Iteration 7390, Loss: 0.005688521080\n",
      "Iteration 7400, Loss: 0.005883648526\n",
      "Iteration 7410, Loss: 0.005708239507\n",
      "Iteration 7420, Loss: 0.006314661819\n",
      "Iteration 7430, Loss: 0.008208800107\n",
      "Iteration 7440, Loss: 0.005845735781\n",
      "Iteration 7450, Loss: 0.006221204530\n",
      "Iteration 7460, Loss: 0.005843217485\n",
      "Iteration 7470, Loss: 0.005718037952\n",
      "Iteration 7480, Loss: 0.005616639275\n",
      "Iteration 7490, Loss: 0.005642995704\n",
      "Iteration 7500, Loss: 0.005735558923\n",
      "Iteration 7510, Loss: 0.007065802813\n",
      "Iteration 7520, Loss: 0.005806857720\n",
      "Iteration 7530, Loss: 0.005965314340\n",
      "Iteration 7540, Loss: 0.005871946458\n",
      "Iteration 7550, Loss: 0.005709834397\n",
      "Iteration 7560, Loss: 0.005651905667\n",
      "Iteration 7570, Loss: 0.006343376357\n",
      "Iteration 7580, Loss: 0.009012941271\n",
      "Iteration 7590, Loss: 0.005798557308\n",
      "Iteration 7600, Loss: 0.005756918341\n",
      "Iteration 7610, Loss: 0.005715598352\n",
      "Iteration 7620, Loss: 0.005595024209\n",
      "Iteration 7630, Loss: 0.005705017596\n",
      "Iteration 7640, Loss: 0.005753874779\n",
      "Iteration 7650, Loss: 0.006546323188\n",
      "Iteration 7660, Loss: 0.006740522105\n",
      "Iteration 7670, Loss: 0.006304664072\n",
      "Iteration 7680, Loss: 0.005643980112\n",
      "Iteration 7690, Loss: 0.006092786323\n",
      "Iteration 7700, Loss: 0.006483290344\n",
      "Iteration 7710, Loss: 0.005876846146\n",
      "Iteration 7720, Loss: 0.005635629874\n",
      "Iteration 7730, Loss: 0.005970476661\n",
      "Iteration 7740, Loss: 0.009967489168\n",
      "Iteration 7750, Loss: 0.007081408985\n",
      "Iteration 7760, Loss: 0.005909602623\n",
      "Iteration 7770, Loss: 0.005802640691\n",
      "Iteration 7780, Loss: 0.005794559605\n",
      "Iteration 7790, Loss: 0.005661130883\n",
      "Iteration 7800, Loss: 0.005757707171\n",
      "Iteration 7810, Loss: 0.007625692990\n",
      "Iteration 7820, Loss: 0.005607032683\n",
      "Iteration 7830, Loss: 0.005581897683\n",
      "Iteration 7840, Loss: 0.005717323627\n",
      "Iteration 7850, Loss: 0.005644926801\n",
      "Iteration 7860, Loss: 0.006143989041\n",
      "Iteration 7870, Loss: 0.007832094096\n",
      "Iteration 7880, Loss: 0.006158211734\n",
      "Iteration 7890, Loss: 0.005664489698\n",
      "Iteration 7900, Loss: 0.005849919282\n",
      "Iteration 7910, Loss: 0.006373592187\n",
      "Iteration 7920, Loss: 0.006982008461\n",
      "Iteration 7930, Loss: 0.005795696285\n",
      "Iteration 7940, Loss: 0.005968733225\n",
      "Iteration 7950, Loss: 0.005800753832\n",
      "Iteration 7960, Loss: 0.006576514337\n",
      "Iteration 7970, Loss: 0.006681837607\n",
      "Iteration 7980, Loss: 0.006297796965\n",
      "Iteration 7990, Loss: 0.005569996778\n",
      "Iteration 8000, Loss: 0.006064292975\n",
      "Iteration 8010, Loss: 0.006750145927\n",
      "Iteration 8020, Loss: 0.005591393448\n",
      "Iteration 8030, Loss: 0.006008433178\n",
      "Iteration 8040, Loss: 0.006818705704\n",
      "Iteration 8050, Loss: 0.005810925271\n",
      "Iteration 8060, Loss: 0.005768449046\n",
      "Iteration 8070, Loss: 0.006841621827\n",
      "Iteration 8080, Loss: 0.006142675877\n",
      "Iteration 8090, Loss: 0.006171867251\n",
      "Iteration 8100, Loss: 0.005692254286\n",
      "Iteration 8110, Loss: 0.006074421573\n",
      "Iteration 8120, Loss: 0.006714808289\n",
      "Iteration 8130, Loss: 0.005615325645\n",
      "Iteration 8140, Loss: 0.005820500664\n",
      "Iteration 8150, Loss: 0.007000057958\n",
      "Iteration 8160, Loss: 0.005835051648\n",
      "Iteration 8170, Loss: 0.006103949621\n",
      "Iteration 8180, Loss: 0.005606963299\n",
      "Iteration 8190, Loss: 0.005575349089\n",
      "Iteration 8200, Loss: 0.006041326560\n",
      "Iteration 8210, Loss: 0.011173227802\n",
      "Iteration 8220, Loss: 0.005707009230\n",
      "Iteration 8230, Loss: 0.006082207430\n",
      "Iteration 8240, Loss: 0.005848971661\n",
      "Iteration 8250, Loss: 0.005638214760\n",
      "Iteration 8260, Loss: 0.005574397277\n",
      "Iteration 8270, Loss: 0.005532972980\n",
      "Iteration 8280, Loss: 0.005533150397\n",
      "Iteration 8290, Loss: 0.005550120026\n",
      "Iteration 8300, Loss: 0.007532220799\n",
      "Iteration 8310, Loss: 0.006023033522\n",
      "Iteration 8320, Loss: 0.006910537370\n",
      "Iteration 8330, Loss: 0.005563355517\n",
      "Iteration 8340, Loss: 0.005593029782\n",
      "Iteration 8350, Loss: 0.005541958380\n",
      "Iteration 8360, Loss: 0.005559251644\n",
      "Iteration 8370, Loss: 0.005524642766\n",
      "Iteration 8380, Loss: 0.005569269415\n",
      "Iteration 8390, Loss: 0.006239961367\n",
      "Iteration 8400, Loss: 0.007658488117\n",
      "Iteration 8410, Loss: 0.006271048449\n",
      "Iteration 8420, Loss: 0.005827988498\n",
      "Iteration 8430, Loss: 0.005704445764\n",
      "Iteration 8440, Loss: 0.005695913918\n",
      "Iteration 8450, Loss: 0.005875709932\n",
      "Iteration 8460, Loss: 0.007843869738\n",
      "Iteration 8470, Loss: 0.005695444997\n",
      "Iteration 8480, Loss: 0.005584716331\n",
      "Iteration 8490, Loss: 0.005845557433\n",
      "Iteration 8500, Loss: 0.006106665824\n",
      "Iteration 8510, Loss: 0.006395369768\n",
      "Iteration 8520, Loss: 0.005950174760\n",
      "Iteration 8530, Loss: 0.005592727568\n",
      "Iteration 8540, Loss: 0.005518279970\n",
      "Iteration 8550, Loss: 0.005589798559\n",
      "Iteration 8560, Loss: 0.013161863200\n",
      "Iteration 8570, Loss: 0.006721289828\n",
      "Iteration 8580, Loss: 0.006833431311\n",
      "Iteration 8590, Loss: 0.005836321972\n",
      "Iteration 8600, Loss: 0.005679544527\n",
      "Iteration 8610, Loss: 0.005527538713\n",
      "Iteration 8620, Loss: 0.005517688114\n",
      "Iteration 8630, Loss: 0.005512007978\n",
      "Iteration 8640, Loss: 0.005508762319\n",
      "Iteration 8650, Loss: 0.005499918945\n",
      "Iteration 8660, Loss: 0.005518056918\n",
      "Iteration 8670, Loss: 0.006206328981\n",
      "Iteration 8680, Loss: 0.008876261301\n",
      "Iteration 8690, Loss: 0.006359970663\n",
      "Iteration 8700, Loss: 0.005948788486\n",
      "Iteration 8710, Loss: 0.005571797956\n",
      "Iteration 8720, Loss: 0.005679619033\n",
      "Iteration 8730, Loss: 0.005537865218\n",
      "Iteration 8740, Loss: 0.005760956556\n",
      "Iteration 8750, Loss: 0.009756257758\n",
      "Iteration 8760, Loss: 0.007129085716\n",
      "Iteration 8770, Loss: 0.005918658338\n",
      "Iteration 8780, Loss: 0.005743095186\n",
      "Iteration 8790, Loss: 0.005587696098\n",
      "Iteration 8800, Loss: 0.005611749832\n",
      "Iteration 8810, Loss: 0.005833639763\n",
      "Iteration 8820, Loss: 0.007583430037\n",
      "Iteration 8830, Loss: 0.005714942235\n",
      "Iteration 8840, Loss: 0.005562285427\n",
      "Iteration 8850, Loss: 0.005803670734\n",
      "Iteration 8860, Loss: 0.006241158582\n",
      "Iteration 8870, Loss: 0.006112024654\n",
      "Iteration 8880, Loss: 0.005497863051\n",
      "Iteration 8890, Loss: 0.005558346398\n",
      "Iteration 8900, Loss: 0.007478639483\n",
      "Iteration 8910, Loss: 0.005640825722\n",
      "Iteration 8920, Loss: 0.006330662407\n",
      "Iteration 8930, Loss: 0.005804117303\n",
      "Iteration 8940, Loss: 0.005607160274\n",
      "Iteration 8950, Loss: 0.005509496666\n",
      "Iteration 8960, Loss: 0.005653326400\n",
      "Iteration 8970, Loss: 0.005994562060\n",
      "Iteration 8980, Loss: 0.007418456953\n",
      "Iteration 8990, Loss: 0.006110859104\n",
      "Iteration 9000, Loss: 0.005492489785\n",
      "Iteration 9010, Loss: 0.005874204449\n",
      "Iteration 9020, Loss: 0.006340968423\n",
      "Iteration 9030, Loss: 0.005848720204\n",
      "Iteration 9040, Loss: 0.005499136169\n",
      "Iteration 9050, Loss: 0.005806697533\n",
      "Iteration 9060, Loss: 0.010361204855\n",
      "Iteration 9070, Loss: 0.006931507960\n",
      "Iteration 9080, Loss: 0.005946047138\n",
      "Iteration 9090, Loss: 0.005745726172\n",
      "Iteration 9100, Loss: 0.005510779098\n",
      "Iteration 9110, Loss: 0.005691223778\n",
      "Iteration 9120, Loss: 0.006499353331\n",
      "Iteration 9130, Loss: 0.006099314895\n",
      "Iteration 9140, Loss: 0.005912004039\n",
      "Iteration 9150, Loss: 0.005589592271\n",
      "Iteration 9160, Loss: 0.005465612281\n",
      "Iteration 9170, Loss: 0.005471525714\n",
      "Iteration 9180, Loss: 0.005891020410\n",
      "Iteration 9190, Loss: 0.012219272554\n",
      "Iteration 9200, Loss: 0.006861991715\n",
      "Iteration 9210, Loss: 0.005564689171\n",
      "Iteration 9220, Loss: 0.005838368554\n",
      "Iteration 9230, Loss: 0.005631547887\n",
      "Iteration 9240, Loss: 0.005542986095\n",
      "Iteration 9250, Loss: 0.005498111714\n",
      "Iteration 9260, Loss: 0.005595956929\n",
      "Iteration 9270, Loss: 0.005527406931\n",
      "Iteration 9280, Loss: 0.005716697779\n",
      "Iteration 9290, Loss: 0.007532923948\n",
      "Iteration 9300, Loss: 0.005490784999\n",
      "Iteration 9310, Loss: 0.005635047331\n",
      "Iteration 9320, Loss: 0.005775129888\n",
      "Iteration 9330, Loss: 0.005989413243\n",
      "Iteration 9340, Loss: 0.006140371785\n",
      "Iteration 9350, Loss: 0.005924947094\n",
      "Iteration 9360, Loss: 0.005573799368\n",
      "Iteration 9370, Loss: 0.005819215905\n",
      "Iteration 9380, Loss: 0.009737208486\n",
      "Iteration 9390, Loss: 0.006801610347\n",
      "Iteration 9400, Loss: 0.005994805135\n",
      "Iteration 9410, Loss: 0.005692009348\n",
      "Iteration 9420, Loss: 0.005497359671\n",
      "Iteration 9430, Loss: 0.005473640747\n",
      "Iteration 9440, Loss: 0.005943209864\n",
      "Iteration 9450, Loss: 0.010079886764\n",
      "Iteration 9460, Loss: 0.006626068614\n",
      "Iteration 9470, Loss: 0.005689022597\n",
      "Iteration 9480, Loss: 0.005648338236\n",
      "Iteration 9490, Loss: 0.005540294107\n",
      "Iteration 9500, Loss: 0.005665765144\n",
      "Iteration 9510, Loss: 0.006680916063\n",
      "Iteration 9520, Loss: 0.005785718095\n",
      "Iteration 9530, Loss: 0.006005121861\n",
      "Iteration 9540, Loss: 0.005517394282\n",
      "Iteration 9550, Loss: 0.005818875507\n",
      "Iteration 9560, Loss: 0.008046775125\n",
      "Iteration 9570, Loss: 0.005793792196\n",
      "Iteration 9580, Loss: 0.005449759774\n",
      "Iteration 9590, Loss: 0.005786422640\n",
      "Iteration 9600, Loss: 0.005651274230\n",
      "Iteration 9610, Loss: 0.006112421397\n",
      "Iteration 9620, Loss: 0.007155344822\n",
      "Iteration 9630, Loss: 0.005846040323\n",
      "Iteration 9640, Loss: 0.005497119855\n",
      "Iteration 9650, Loss: 0.005670160055\n",
      "Iteration 9660, Loss: 0.008021750487\n",
      "Iteration 9670, Loss: 0.005688917823\n",
      "Iteration 9680, Loss: 0.005453202408\n",
      "Iteration 9690, Loss: 0.005628692918\n",
      "Iteration 9700, Loss: 0.005504289642\n",
      "Iteration 9710, Loss: 0.005497256760\n",
      "Iteration 9720, Loss: 0.008388187736\n",
      "Iteration 9730, Loss: 0.006651693024\n",
      "Iteration 9740, Loss: 0.006371038035\n",
      "Iteration 9750, Loss: 0.005535391625\n",
      "Iteration 9760, Loss: 0.005488036666\n",
      "Iteration 9770, Loss: 0.005521800835\n",
      "Iteration 9780, Loss: 0.005436765030\n",
      "Iteration 9790, Loss: 0.005426552147\n",
      "Iteration 9800, Loss: 0.005469447002\n",
      "Iteration 9810, Loss: 0.007848882116\n",
      "Iteration 9820, Loss: 0.006203609519\n",
      "Iteration 9830, Loss: 0.006478883792\n",
      "Iteration 9840, Loss: 0.005503337365\n",
      "Iteration 9850, Loss: 0.005448364187\n",
      "Iteration 9860, Loss: 0.005482350476\n",
      "Iteration 9870, Loss: 0.005453811493\n",
      "Iteration 9880, Loss: 0.005421618931\n",
      "Iteration 9890, Loss: 0.005463769659\n",
      "Iteration 9900, Loss: 0.007722337265\n",
      "Iteration 9910, Loss: 0.006716805045\n",
      "Iteration 9920, Loss: 0.006374393124\n",
      "Iteration 9930, Loss: 0.005445614923\n",
      "Iteration 9940, Loss: 0.005429469980\n",
      "Iteration 9950, Loss: 0.005478702020\n",
      "Iteration 9960, Loss: 0.005500018131\n",
      "Iteration 9970, Loss: 0.005556197371\n",
      "Iteration 9980, Loss: 0.005777193699\n",
      "Iteration 9990, Loss: 0.007587397005\n",
      "Iteration 10000, Loss: 0.005769866053\n",
      "Iteration 10010, Loss: 0.005467697512\n",
      "Iteration 10020, Loss: 0.005784391891\n",
      "Iteration 10030, Loss: 0.006205981132\n",
      "Iteration 10040, Loss: 0.005848383531\n",
      "Iteration 10050, Loss: 0.005418948829\n",
      "Iteration 10060, Loss: 0.005459203385\n",
      "Iteration 10070, Loss: 0.008029038087\n",
      "Iteration 10080, Loss: 0.006505462341\n",
      "Iteration 10090, Loss: 0.006413696799\n",
      "Iteration 10100, Loss: 0.005482217763\n",
      "Iteration 10110, Loss: 0.005433828104\n",
      "Iteration 10120, Loss: 0.005938253365\n",
      "Iteration 10130, Loss: 0.005576441996\n",
      "Iteration 10140, Loss: 0.005476215854\n",
      "Iteration 10150, Loss: 0.005776472855\n",
      "Iteration 10160, Loss: 0.008676961064\n",
      "Iteration 10170, Loss: 0.006420627236\n",
      "Iteration 10180, Loss: 0.005697020795\n",
      "Iteration 10190, Loss: 0.005442584399\n",
      "Iteration 10200, Loss: 0.005682582036\n",
      "Iteration 10210, Loss: 0.006635923404\n",
      "Iteration 10220, Loss: 0.005610336084\n",
      "Iteration 10230, Loss: 0.005955248605\n",
      "Iteration 10240, Loss: 0.005409747828\n",
      "Iteration 10250, Loss: 0.005825233646\n",
      "Iteration 10260, Loss: 0.008183132857\n",
      "Iteration 10270, Loss: 0.006430937909\n",
      "Iteration 10280, Loss: 0.005662390031\n",
      "Iteration 10290, Loss: 0.005443347618\n",
      "Iteration 10300, Loss: 0.005480526015\n",
      "Iteration 10310, Loss: 0.006611655932\n",
      "Iteration 10320, Loss: 0.005895033013\n",
      "Iteration 10330, Loss: 0.005499313120\n",
      "Iteration 10340, Loss: 0.005511526950\n",
      "Iteration 10350, Loss: 0.005562183913\n",
      "Iteration 10360, Loss: 0.005617567338\n",
      "Iteration 10370, Loss: 0.006248177961\n",
      "Iteration 10380, Loss: 0.006360291503\n",
      "Iteration 10390, Loss: 0.006043199915\n",
      "Iteration 10400, Loss: 0.005500396248\n",
      "Iteration 10410, Loss: 0.005933503155\n",
      "Iteration 10420, Loss: 0.007224136498\n",
      "Iteration 10430, Loss: 0.005743183661\n",
      "Iteration 10440, Loss: 0.005587365013\n",
      "Iteration 10450, Loss: 0.005490961950\n",
      "Iteration 10460, Loss: 0.006701322272\n",
      "Iteration 10470, Loss: 0.005693820771\n",
      "Iteration 10480, Loss: 0.005715485662\n",
      "Iteration 10490, Loss: 0.005627552979\n",
      "Iteration 10500, Loss: 0.005552676041\n",
      "Iteration 10510, Loss: 0.005570866633\n",
      "Iteration 10520, Loss: 0.008421566337\n",
      "Iteration 10530, Loss: 0.006500968710\n",
      "Iteration 10540, Loss: 0.005990787409\n",
      "Iteration 10550, Loss: 0.005614901427\n",
      "Iteration 10560, Loss: 0.005411983468\n",
      "Iteration 10570, Loss: 0.005455480423\n",
      "Iteration 10580, Loss: 0.006336996332\n",
      "Iteration 10590, Loss: 0.006717883982\n",
      "Iteration 10600, Loss: 0.006107522175\n",
      "Iteration 10610, Loss: 0.005684799515\n",
      "Iteration 10620, Loss: 0.005523412023\n",
      "Iteration 10630, Loss: 0.005501961336\n",
      "Iteration 10640, Loss: 0.006378835533\n",
      "Iteration 10650, Loss: 0.006286534015\n",
      "Iteration 10660, Loss: 0.005418192130\n",
      "Iteration 10670, Loss: 0.005411166698\n",
      "Iteration 10680, Loss: 0.005554388743\n",
      "Iteration 10690, Loss: 0.005398462992\n",
      "Iteration 10700, Loss: 0.005606835242\n",
      "Iteration 10710, Loss: 0.008186564781\n",
      "Iteration 10720, Loss: 0.006274205633\n",
      "Iteration 10730, Loss: 0.005622983444\n",
      "Iteration 10740, Loss: 0.005409550387\n",
      "Iteration 10750, Loss: 0.005726450123\n",
      "Iteration 10760, Loss: 0.006209494080\n",
      "Iteration 10770, Loss: 0.006182963960\n",
      "Iteration 10780, Loss: 0.005721271969\n",
      "Iteration 10790, Loss: 0.005878219847\n",
      "Iteration 10800, Loss: 0.005653562490\n",
      "Iteration 10810, Loss: 0.006023736205\n",
      "Iteration 10820, Loss: 0.007168804761\n",
      "Iteration 10830, Loss: 0.006117913872\n",
      "Iteration 10840, Loss: 0.005382103380\n",
      "Iteration 10850, Loss: 0.005895121954\n",
      "Iteration 10860, Loss: 0.006307845935\n",
      "Iteration 10870, Loss: 0.005418939516\n",
      "Iteration 10880, Loss: 0.005816526711\n",
      "Iteration 10890, Loss: 0.007653207984\n",
      "Iteration 10900, Loss: 0.005608916748\n",
      "Iteration 10910, Loss: 0.005634931847\n",
      "Iteration 10920, Loss: 0.005575458053\n",
      "Iteration 10930, Loss: 0.006639845204\n",
      "Iteration 10940, Loss: 0.005552329123\n",
      "Iteration 10950, Loss: 0.005818439648\n",
      "Iteration 10960, Loss: 0.005919798277\n",
      "Iteration 10970, Loss: 0.005612215959\n",
      "Iteration 10980, Loss: 0.005790182389\n",
      "Iteration 10990, Loss: 0.007652100641\n",
      "Iteration 11000, Loss: 0.005937227979\n",
      "Iteration 11010, Loss: 0.005429709330\n",
      "Iteration 11020, Loss: 0.005711884238\n",
      "Iteration 11030, Loss: 0.006150135305\n",
      "Iteration 11040, Loss: 0.005765807815\n",
      "Iteration 11050, Loss: 0.005363657605\n",
      "Iteration 11060, Loss: 0.005424149334\n",
      "Iteration 11070, Loss: 0.005799887702\n",
      "Iteration 11080, Loss: 0.014268038794\n",
      "Iteration 11090, Loss: 0.005446142051\n",
      "Iteration 11100, Loss: 0.006246427074\n",
      "Iteration 11110, Loss: 0.005489909556\n",
      "Iteration 11120, Loss: 0.005371132400\n",
      "Iteration 11130, Loss: 0.005391785875\n",
      "Iteration 11140, Loss: 0.005373456050\n",
      "Iteration 11150, Loss: 0.005379906856\n",
      "Iteration 11160, Loss: 0.005418913905\n",
      "Iteration 11170, Loss: 0.006423220038\n",
      "Iteration 11180, Loss: 0.005870245863\n",
      "Iteration 11190, Loss: 0.005513319280\n",
      "Iteration 11200, Loss: 0.005557397380\n",
      "Iteration 11210, Loss: 0.005481866188\n",
      "Iteration 11220, Loss: 0.005825853441\n",
      "Iteration 11230, Loss: 0.006874135695\n",
      "Iteration 11240, Loss: 0.005451089703\n",
      "Iteration 11250, Loss: 0.005792281590\n",
      "Iteration 11260, Loss: 0.005434140097\n",
      "Iteration 11270, Loss: 0.005668662488\n",
      "Iteration 11280, Loss: 0.009189002216\n",
      "Iteration 11290, Loss: 0.006762651727\n",
      "Iteration 11300, Loss: 0.005776984617\n",
      "Iteration 11310, Loss: 0.005559704732\n",
      "Iteration 11320, Loss: 0.005412858911\n",
      "Iteration 11330, Loss: 0.005338333081\n",
      "Iteration 11340, Loss: 0.005348885898\n",
      "Iteration 11350, Loss: 0.006039524451\n",
      "Iteration 11360, Loss: 0.007608055603\n",
      "Iteration 11370, Loss: 0.006481203716\n",
      "Iteration 11380, Loss: 0.005854940973\n",
      "Iteration 11390, Loss: 0.005465170369\n",
      "Iteration 11400, Loss: 0.005442495458\n",
      "Iteration 11410, Loss: 0.005357943010\n",
      "Iteration 11420, Loss: 0.005404161755\n",
      "Iteration 11430, Loss: 0.005597594660\n",
      "Iteration 11440, Loss: 0.007284964435\n",
      "Iteration 11450, Loss: 0.005406687036\n",
      "Iteration 11460, Loss: 0.005589501001\n",
      "Iteration 11470, Loss: 0.005570564885\n",
      "Iteration 11480, Loss: 0.006483562291\n",
      "Iteration 11490, Loss: 0.005535532720\n",
      "Iteration 11500, Loss: 0.005676608998\n",
      "Iteration 11510, Loss: 0.006285420619\n",
      "Iteration 11520, Loss: 0.005690449383\n",
      "Iteration 11530, Loss: 0.005412142724\n",
      "Iteration 11540, Loss: 0.006768384483\n",
      "Iteration 11550, Loss: 0.005546277389\n",
      "Iteration 11560, Loss: 0.005420777481\n",
      "Iteration 11570, Loss: 0.005555925425\n",
      "Iteration 11580, Loss: 0.005428305827\n",
      "Iteration 11590, Loss: 0.005935381632\n",
      "Iteration 11600, Loss: 0.006617662497\n",
      "Iteration 11610, Loss: 0.005741489585\n",
      "Iteration 11620, Loss: 0.005543739069\n",
      "Iteration 11630, Loss: 0.005410271697\n",
      "Iteration 11640, Loss: 0.005442899652\n",
      "Iteration 11650, Loss: 0.011983033270\n",
      "Iteration 11660, Loss: 0.006584721152\n",
      "Iteration 11670, Loss: 0.005585325416\n",
      "Iteration 11680, Loss: 0.005680360831\n",
      "Iteration 11690, Loss: 0.005441114772\n",
      "Iteration 11700, Loss: 0.005322629120\n",
      "Iteration 11710, Loss: 0.005365732126\n",
      "Iteration 11720, Loss: 0.005486990791\n",
      "Iteration 11730, Loss: 0.006495412905\n",
      "Iteration 11740, Loss: 0.005863898899\n",
      "Iteration 11750, Loss: 0.005932048894\n",
      "Iteration 11760, Loss: 0.005355478264\n",
      "Iteration 11770, Loss: 0.005431624129\n",
      "Iteration 11780, Loss: 0.008104745299\n",
      "Iteration 11790, Loss: 0.006093154661\n",
      "Iteration 11800, Loss: 0.005949570797\n",
      "Iteration 11810, Loss: 0.005548863672\n",
      "Iteration 11820, Loss: 0.005344155245\n",
      "Iteration 11830, Loss: 0.005428177305\n",
      "Iteration 11840, Loss: 0.006214372814\n",
      "Iteration 11850, Loss: 0.006245936733\n",
      "Iteration 11860, Loss: 0.005924232304\n",
      "Iteration 11870, Loss: 0.005413105711\n",
      "Iteration 11880, Loss: 0.005782076158\n",
      "Iteration 11890, Loss: 0.006965748966\n",
      "Iteration 11900, Loss: 0.005450643599\n",
      "Iteration 11910, Loss: 0.005889344029\n",
      "Iteration 11920, Loss: 0.005321134813\n",
      "Iteration 11930, Loss: 0.005387323443\n",
      "Iteration 11940, Loss: 0.007812176831\n",
      "Iteration 11950, Loss: 0.006104993168\n",
      "Iteration 11960, Loss: 0.006270563696\n",
      "Iteration 11970, Loss: 0.005716592539\n",
      "Iteration 11980, Loss: 0.005489293952\n",
      "Iteration 11990, Loss: 0.005319000687\n",
      "Iteration 12000, Loss: 0.005429121200\n",
      "Iteration 12010, Loss: 0.005478271749\n",
      "Iteration 12020, Loss: 0.008268462494\n",
      "Iteration 12030, Loss: 0.006440132856\n",
      "Iteration 12040, Loss: 0.005766891874\n",
      "Iteration 12050, Loss: 0.005400119815\n",
      "Iteration 12060, Loss: 0.005503867753\n",
      "Iteration 12070, Loss: 0.005470906384\n",
      "Iteration 12080, Loss: 0.006573654711\n",
      "Iteration 12090, Loss: 0.005524309352\n",
      "Iteration 12100, Loss: 0.005793673918\n",
      "Iteration 12110, Loss: 0.005531196482\n",
      "Iteration 12120, Loss: 0.005948072765\n",
      "Iteration 12130, Loss: 0.005694492720\n",
      "Iteration 12140, Loss: 0.005475664977\n",
      "Iteration 12150, Loss: 0.005788133480\n",
      "Iteration 12160, Loss: 0.008551739156\n",
      "Iteration 12170, Loss: 0.006390371360\n",
      "Iteration 12180, Loss: 0.005622810218\n",
      "Iteration 12190, Loss: 0.005368572194\n",
      "Iteration 12200, Loss: 0.005405194592\n",
      "Iteration 12210, Loss: 0.006629594602\n",
      "Iteration 12220, Loss: 0.005392579362\n",
      "Iteration 12230, Loss: 0.005356768146\n",
      "Iteration 12240, Loss: 0.005464523565\n",
      "Iteration 12250, Loss: 0.005404416937\n",
      "Iteration 12260, Loss: 0.005813390482\n",
      "Iteration 12270, Loss: 0.007077874616\n",
      "Iteration 12280, Loss: 0.005787536502\n",
      "Iteration 12290, Loss: 0.005400120281\n",
      "Iteration 12300, Loss: 0.005485054106\n",
      "Iteration 12310, Loss: 0.006369446404\n",
      "Iteration 12320, Loss: 0.005994210951\n",
      "Iteration 12330, Loss: 0.005785050336\n",
      "Iteration 12340, Loss: 0.005663493648\n",
      "Iteration 12350, Loss: 0.005403341725\n",
      "Iteration 12360, Loss: 0.005337778945\n",
      "Iteration 12370, Loss: 0.006247795653\n",
      "Iteration 12380, Loss: 0.005973594263\n",
      "Iteration 12390, Loss: 0.005384454969\n",
      "Iteration 12400, Loss: 0.005439964123\n",
      "Iteration 12410, Loss: 0.005342424382\n",
      "Iteration 12420, Loss: 0.005527710076\n",
      "Iteration 12430, Loss: 0.005458303727\n",
      "Iteration 12440, Loss: 0.006364327855\n",
      "Iteration 12450, Loss: 0.005691832397\n",
      "Iteration 12460, Loss: 0.006001218222\n",
      "Iteration 12470, Loss: 0.005308058579\n",
      "Iteration 12480, Loss: 0.005610838067\n",
      "Iteration 12490, Loss: 0.007979838178\n",
      "Iteration 12500, Loss: 0.005977878347\n",
      "Iteration 12510, Loss: 0.005385397933\n",
      "Iteration 12520, Loss: 0.005487036426\n",
      "Iteration 12530, Loss: 0.005388265476\n",
      "Iteration 12540, Loss: 0.005712694488\n",
      "Iteration 12550, Loss: 0.008876107633\n",
      "Iteration 12560, Loss: 0.006012063473\n",
      "Iteration 12570, Loss: 0.005547928158\n",
      "Iteration 12580, Loss: 0.005545171909\n",
      "Iteration 12590, Loss: 0.005355208181\n",
      "Iteration 12600, Loss: 0.005451851059\n",
      "Iteration 12610, Loss: 0.006637395360\n",
      "Iteration 12620, Loss: 0.005497835111\n",
      "Iteration 12630, Loss: 0.005593223963\n",
      "Iteration 12640, Loss: 0.005605642684\n",
      "Iteration 12650, Loss: 0.005412383471\n",
      "Iteration 12660, Loss: 0.005485814530\n",
      "Iteration 12670, Loss: 0.008382551372\n",
      "Iteration 12680, Loss: 0.006177434232\n",
      "Iteration 12690, Loss: 0.005596266594\n",
      "Iteration 12700, Loss: 0.005303743295\n",
      "Iteration 12710, Loss: 0.005702431314\n",
      "Iteration 12720, Loss: 0.005912770983\n",
      "Iteration 12730, Loss: 0.005845653825\n",
      "Iteration 12740, Loss: 0.005274140276\n",
      "Iteration 12750, Loss: 0.005595821887\n",
      "Iteration 12760, Loss: 0.009395954199\n",
      "Iteration 12770, Loss: 0.006437984295\n",
      "Iteration 12780, Loss: 0.005665034987\n",
      "Iteration 12790, Loss: 0.005497606471\n",
      "Iteration 12800, Loss: 0.005288446788\n",
      "Iteration 12810, Loss: 0.005275313277\n",
      "Iteration 12820, Loss: 0.005543994252\n",
      "Iteration 12830, Loss: 0.010252017528\n",
      "Iteration 12840, Loss: 0.005867510103\n",
      "Iteration 12850, Loss: 0.005362063181\n",
      "Iteration 12860, Loss: 0.005364447366\n",
      "Iteration 12870, Loss: 0.005560966674\n",
      "Iteration 12880, Loss: 0.005326586310\n",
      "Iteration 12890, Loss: 0.005286619999\n",
      "Iteration 12900, Loss: 0.005511002149\n",
      "Iteration 12910, Loss: 0.010299086571\n",
      "Iteration 12920, Loss: 0.005554548465\n",
      "Iteration 12930, Loss: 0.005516193341\n",
      "Iteration 12940, Loss: 0.005484943744\n",
      "Iteration 12950, Loss: 0.005272600800\n",
      "Iteration 12960, Loss: 0.005392508581\n",
      "Iteration 12970, Loss: 0.005299673881\n",
      "Iteration 12980, Loss: 0.005378270987\n",
      "Iteration 12990, Loss: 0.007903205231\n",
      "Iteration 13000, Loss: 0.005961352028\n",
      "Iteration 13010, Loss: 0.005651667248\n",
      "Iteration 13020, Loss: 0.005318172276\n",
      "Iteration 13030, Loss: 0.005542406347\n",
      "Iteration 13040, Loss: 0.005436460488\n",
      "Iteration 13050, Loss: 0.005653164349\n",
      "Iteration 13060, Loss: 0.007329938002\n",
      "Iteration 13070, Loss: 0.005508913659\n",
      "Iteration 13080, Loss: 0.005350081716\n",
      "Iteration 13090, Loss: 0.005604039878\n",
      "Iteration 13100, Loss: 0.006010578480\n",
      "Iteration 13110, Loss: 0.005474839825\n",
      "Iteration 13120, Loss: 0.005247227848\n",
      "Iteration 13130, Loss: 0.005240426864\n",
      "Iteration 13140, Loss: 0.005238777027\n",
      "Iteration 13150, Loss: 0.005373515189\n",
      "Iteration 13160, Loss: 0.014073008671\n",
      "Iteration 13170, Loss: 0.007135339081\n",
      "Iteration 13180, Loss: 0.005672183819\n",
      "Iteration 13190, Loss: 0.005517278332\n",
      "Iteration 13200, Loss: 0.005429086741\n",
      "Iteration 13210, Loss: 0.005293973722\n",
      "Iteration 13220, Loss: 0.005264267791\n",
      "Iteration 13230, Loss: 0.005250740331\n",
      "Iteration 13240, Loss: 0.005288519897\n",
      "Iteration 13250, Loss: 0.005487727933\n",
      "Iteration 13260, Loss: 0.005279088393\n",
      "Iteration 13270, Loss: 0.005281615537\n",
      "Iteration 13280, Loss: 0.005877832882\n",
      "Iteration 13290, Loss: 0.006773769390\n",
      "Iteration 13300, Loss: 0.005358324386\n",
      "Iteration 13310, Loss: 0.005306571722\n",
      "Iteration 13320, Loss: 0.005445371382\n",
      "Iteration 13330, Loss: 0.005245572887\n",
      "Iteration 13340, Loss: 0.005479630083\n",
      "Iteration 13350, Loss: 0.007352125831\n",
      "Iteration 13360, Loss: 0.005707144737\n",
      "Iteration 13370, Loss: 0.005258121062\n",
      "Iteration 13380, Loss: 0.005592359696\n",
      "Iteration 13390, Loss: 0.005564662628\n",
      "Iteration 13400, Loss: 0.005756669678\n",
      "Iteration 13410, Loss: 0.006351232063\n",
      "Iteration 13420, Loss: 0.005306577310\n",
      "Iteration 13430, Loss: 0.006324817427\n",
      "Iteration 13440, Loss: 0.005478534847\n",
      "Iteration 13450, Loss: 0.005323205609\n",
      "Iteration 13460, Loss: 0.006631829310\n",
      "Iteration 13470, Loss: 0.005289772060\n",
      "Iteration 13480, Loss: 0.005266870372\n",
      "Iteration 13490, Loss: 0.005262175575\n",
      "Iteration 13500, Loss: 0.005508008413\n",
      "Iteration 13510, Loss: 0.005406785291\n",
      "Iteration 13520, Loss: 0.005960774608\n",
      "Iteration 13530, Loss: 0.006340947002\n",
      "Iteration 13540, Loss: 0.005795954727\n",
      "Iteration 13550, Loss: 0.005353801884\n",
      "Iteration 13560, Loss: 0.005391175859\n",
      "Iteration 13570, Loss: 0.007685009390\n",
      "Iteration 13580, Loss: 0.005617960356\n",
      "Iteration 13590, Loss: 0.005343340337\n",
      "Iteration 13600, Loss: 0.005313185509\n",
      "Iteration 13610, Loss: 0.005243700929\n",
      "Iteration 13620, Loss: 0.005488102790\n",
      "Iteration 13630, Loss: 0.009629166685\n",
      "Iteration 13640, Loss: 0.006085423753\n",
      "Iteration 13650, Loss: 0.005313984584\n",
      "Iteration 13660, Loss: 0.005276528653\n",
      "Iteration 13670, Loss: 0.005340001080\n",
      "Iteration 13680, Loss: 0.005224386696\n",
      "Iteration 13690, Loss: 0.005235321820\n",
      "Iteration 13700, Loss: 0.005612391047\n",
      "Iteration 13710, Loss: 0.010449415073\n",
      "Iteration 13720, Loss: 0.006028015632\n",
      "Iteration 13730, Loss: 0.005255913362\n",
      "Iteration 13740, Loss: 0.005243788473\n",
      "Iteration 13750, Loss: 0.005383593496\n",
      "Iteration 13760, Loss: 0.005227263086\n",
      "Iteration 13770, Loss: 0.005389741622\n",
      "Iteration 13780, Loss: 0.005251217168\n",
      "Iteration 13790, Loss: 0.006521788426\n",
      "Iteration 13800, Loss: 0.005837896839\n",
      "Iteration 13810, Loss: 0.005997819360\n",
      "Iteration 13820, Loss: 0.005567580927\n",
      "Iteration 13830, Loss: 0.005406707991\n",
      "Iteration 13840, Loss: 0.005279879086\n",
      "Iteration 13850, Loss: 0.005208514631\n",
      "Iteration 13860, Loss: 0.005230130628\n",
      "Iteration 13870, Loss: 0.005285947118\n",
      "Iteration 13880, Loss: 0.006515818182\n",
      "Iteration 13890, Loss: 0.005318432581\n",
      "Iteration 13900, Loss: 0.005302426871\n",
      "Iteration 13910, Loss: 0.005408425350\n",
      "Iteration 13920, Loss: 0.005291425623\n",
      "Iteration 13930, Loss: 0.005784131121\n",
      "Iteration 13940, Loss: 0.006516765337\n",
      "Iteration 13950, Loss: 0.005866491236\n",
      "Iteration 13960, Loss: 0.005374592263\n",
      "Iteration 13970, Loss: 0.005555584561\n",
      "Iteration 13980, Loss: 0.006496010348\n",
      "Iteration 13990, Loss: 0.005203498993\n",
      "Iteration 14000, Loss: 0.005887479521\n",
      "Iteration 14010, Loss: 0.005500282627\n",
      "Iteration 14020, Loss: 0.005233648699\n",
      "Iteration 14030, Loss: 0.005564813968\n",
      "Iteration 14040, Loss: 0.009762909263\n",
      "Iteration 14050, Loss: 0.005253437907\n",
      "Iteration 14060, Loss: 0.005535983481\n",
      "Iteration 14070, Loss: 0.005412663799\n",
      "Iteration 14080, Loss: 0.005232133903\n",
      "Iteration 14090, Loss: 0.005352820270\n",
      "Iteration 14100, Loss: 0.005893223919\n",
      "Iteration 14110, Loss: 0.006026152521\n",
      "Iteration 14120, Loss: 0.005319035612\n",
      "Iteration 14130, Loss: 0.006100373808\n",
      "Iteration 14140, Loss: 0.005652870983\n",
      "Iteration 14150, Loss: 0.005355278496\n",
      "Iteration 14160, Loss: 0.006596455351\n",
      "Iteration 14170, Loss: 0.005218298640\n",
      "Iteration 14180, Loss: 0.005990084261\n",
      "Iteration 14190, Loss: 0.005218700971\n",
      "Iteration 14200, Loss: 0.005927179474\n",
      "Iteration 14210, Loss: 0.006528418977\n",
      "Iteration 14220, Loss: 0.005771246739\n",
      "Iteration 14230, Loss: 0.005213789642\n",
      "Iteration 14240, Loss: 0.005733206403\n",
      "Iteration 14250, Loss: 0.006601427216\n",
      "Iteration 14260, Loss: 0.005658079870\n",
      "Iteration 14270, Loss: 0.005341230892\n",
      "Iteration 14280, Loss: 0.005450190511\n",
      "Iteration 14290, Loss: 0.007098905277\n",
      "Iteration 14300, Loss: 0.005348220002\n",
      "Iteration 14310, Loss: 0.005279865116\n",
      "Iteration 14320, Loss: 0.005559460726\n",
      "Iteration 14330, Loss: 0.005665001459\n",
      "Iteration 14340, Loss: 0.005583443679\n",
      "Iteration 14350, Loss: 0.005813457537\n",
      "Iteration 14360, Loss: 0.005720682442\n",
      "Iteration 14370, Loss: 0.005283466075\n",
      "Iteration 14380, Loss: 0.005494341720\n",
      "Iteration 14390, Loss: 0.008830148727\n",
      "Iteration 14400, Loss: 0.006319338456\n",
      "Iteration 14410, Loss: 0.005525812972\n",
      "Iteration 14420, Loss: 0.005467829295\n",
      "Iteration 14430, Loss: 0.005203458015\n",
      "Iteration 14440, Loss: 0.005468843970\n",
      "Iteration 14450, Loss: 0.005227305926\n",
      "Iteration 14460, Loss: 0.005649083294\n",
      "Iteration 14470, Loss: 0.006767230574\n",
      "Iteration 14480, Loss: 0.006887957919\n",
      "Iteration 14490, Loss: 0.005499603692\n",
      "Iteration 14500, Loss: 0.005468179006\n",
      "Iteration 14510, Loss: 0.005256678443\n",
      "Iteration 14520, Loss: 0.005209986586\n",
      "Iteration 14530, Loss: 0.005179326050\n",
      "Iteration 14540, Loss: 0.005169084296\n",
      "Iteration 14550, Loss: 0.005182527471\n",
      "Iteration 14560, Loss: 0.006019159220\n",
      "Iteration 14570, Loss: 0.005455949344\n",
      "Iteration 14580, Loss: 0.006120055914\n",
      "Iteration 14590, Loss: 0.005614800844\n",
      "Iteration 14600, Loss: 0.005362017080\n",
      "Iteration 14610, Loss: 0.005271072965\n",
      "Iteration 14620, Loss: 0.005226612557\n",
      "Iteration 14630, Loss: 0.005208036397\n",
      "Iteration 14640, Loss: 0.005725223105\n",
      "Iteration 14650, Loss: 0.007440965157\n",
      "Iteration 14660, Loss: 0.005820993334\n",
      "Iteration 14670, Loss: 0.005540148355\n",
      "Iteration 14680, Loss: 0.005297601223\n",
      "Iteration 14690, Loss: 0.005597596522\n",
      "Iteration 14700, Loss: 0.006107696798\n",
      "Iteration 14710, Loss: 0.005174160935\n",
      "Iteration 14720, Loss: 0.005635919515\n",
      "Iteration 14730, Loss: 0.006750953384\n",
      "Iteration 14740, Loss: 0.005410572048\n",
      "Iteration 14750, Loss: 0.005624402314\n",
      "Iteration 14760, Loss: 0.005195626058\n",
      "Iteration 14770, Loss: 0.006145946681\n",
      "Iteration 14780, Loss: 0.006179177668\n",
      "Iteration 14790, Loss: 0.005660426337\n",
      "Iteration 14800, Loss: 0.005524455104\n",
      "Iteration 14810, Loss: 0.005193431862\n",
      "Iteration 14820, Loss: 0.005224322900\n",
      "Iteration 14830, Loss: 0.007490810007\n",
      "Iteration 14840, Loss: 0.006340989843\n",
      "Iteration 14850, Loss: 0.005901293363\n",
      "Iteration 14860, Loss: 0.005399619229\n",
      "Iteration 14870, Loss: 0.005268037785\n",
      "Iteration 14880, Loss: 0.005179273430\n",
      "Iteration 14890, Loss: 0.005206122529\n",
      "Iteration 14900, Loss: 0.006030058488\n",
      "Iteration 14910, Loss: 0.006029304583\n",
      "Iteration 14920, Loss: 0.005208412185\n",
      "Iteration 14930, Loss: 0.005230509676\n",
      "Iteration 14940, Loss: 0.005200583953\n",
      "Iteration 14950, Loss: 0.005307145882\n",
      "Iteration 14960, Loss: 0.005790051073\n",
      "Iteration 14970, Loss: 0.005771042779\n",
      "Iteration 14980, Loss: 0.005302091595\n",
      "Iteration 14990, Loss: 0.006002042443\n",
      "Iteration 15000, Loss: 0.005707898643\n",
      "Iteration 15010, Loss: 0.005310080014\n",
      "Iteration 15020, Loss: 0.006540552713\n",
      "Iteration 15030, Loss: 0.005309507251\n",
      "Iteration 15040, Loss: 0.005813960452\n",
      "Iteration 15050, Loss: 0.005256706849\n",
      "Iteration 15060, Loss: 0.005162823945\n",
      "Iteration 15070, Loss: 0.005359706935\n",
      "Iteration 15080, Loss: 0.005197154824\n",
      "Iteration 15090, Loss: 0.012987764552\n",
      "Iteration 15100, Loss: 0.005528162699\n",
      "Iteration 15110, Loss: 0.005711177364\n",
      "Iteration 15120, Loss: 0.005712821148\n",
      "Iteration 15130, Loss: 0.005168557633\n",
      "Iteration 15140, Loss: 0.005167345516\n",
      "Iteration 15150, Loss: 0.005149828736\n",
      "Iteration 15160, Loss: 0.005150516983\n",
      "Iteration 15170, Loss: 0.005147557240\n",
      "Iteration 15180, Loss: 0.005147885531\n",
      "Iteration 15190, Loss: 0.005308383610\n",
      "Iteration 15200, Loss: 0.009785098024\n",
      "Iteration 15210, Loss: 0.006488451734\n",
      "Iteration 15220, Loss: 0.005371708423\n",
      "Iteration 15230, Loss: 0.005258857738\n",
      "Iteration 15240, Loss: 0.005299621727\n",
      "Iteration 15250, Loss: 0.005227820016\n",
      "Iteration 15260, Loss: 0.005356804933\n",
      "Iteration 15270, Loss: 0.007269924507\n",
      "Iteration 15280, Loss: 0.005417233333\n",
      "Iteration 15290, Loss: 0.005176339298\n",
      "Iteration 15300, Loss: 0.005504714791\n",
      "Iteration 15310, Loss: 0.005356181413\n",
      "Iteration 15320, Loss: 0.005811086390\n",
      "Iteration 15330, Loss: 0.006311695091\n",
      "Iteration 15340, Loss: 0.005518732127\n",
      "Iteration 15350, Loss: 0.005519438535\n",
      "Iteration 15360, Loss: 0.005145945121\n",
      "Iteration 15370, Loss: 0.005365730263\n",
      "Iteration 15380, Loss: 0.010903783143\n",
      "Iteration 15390, Loss: 0.005276745651\n",
      "Iteration 15400, Loss: 0.005549516529\n",
      "Iteration 15410, Loss: 0.005381742492\n",
      "Iteration 15420, Loss: 0.005199221428\n",
      "Iteration 15430, Loss: 0.005212116521\n",
      "Iteration 15440, Loss: 0.005308237858\n",
      "Iteration 15450, Loss: 0.005448268726\n",
      "Iteration 15460, Loss: 0.008070387878\n",
      "Iteration 15470, Loss: 0.006333272904\n",
      "Iteration 15480, Loss: 0.005495449528\n",
      "Iteration 15490, Loss: 0.005186028313\n",
      "Iteration 15500, Loss: 0.005451233592\n",
      "Iteration 15510, Loss: 0.006412050687\n",
      "Iteration 15520, Loss: 0.005136899184\n",
      "Iteration 15530, Loss: 0.005770995282\n",
      "Iteration 15540, Loss: 0.005261013284\n",
      "Iteration 15550, Loss: 0.005155759864\n",
      "Iteration 15560, Loss: 0.005269170273\n",
      "Iteration 15570, Loss: 0.006895036437\n",
      "Iteration 15580, Loss: 0.006680338643\n",
      "Iteration 15590, Loss: 0.005541844293\n",
      "Iteration 15600, Loss: 0.005592606496\n",
      "Iteration 15610, Loss: 0.005370933097\n",
      "Iteration 15620, Loss: 0.005248586182\n",
      "Iteration 15630, Loss: 0.005135160871\n",
      "Iteration 15640, Loss: 0.005173248239\n",
      "Iteration 15650, Loss: 0.005285887048\n",
      "Iteration 15660, Loss: 0.006924130954\n",
      "Iteration 15670, Loss: 0.005460059270\n",
      "Iteration 15680, Loss: 0.005240007304\n",
      "Iteration 15690, Loss: 0.005410494749\n",
      "Iteration 15700, Loss: 0.005194558762\n",
      "Iteration 15710, Loss: 0.005474123172\n",
      "Iteration 15720, Loss: 0.007940092124\n",
      "Iteration 15730, Loss: 0.006242270116\n",
      "Iteration 15740, Loss: 0.005510875024\n",
      "Iteration 15750, Loss: 0.005278439727\n",
      "Iteration 15760, Loss: 0.005410869606\n",
      "Iteration 15770, Loss: 0.005798770115\n",
      "Iteration 15780, Loss: 0.005806037225\n",
      "Iteration 15790, Loss: 0.005416086875\n",
      "Iteration 15800, Loss: 0.005736146588\n",
      "Iteration 15810, Loss: 0.006717236247\n",
      "Iteration 15820, Loss: 0.005620517768\n",
      "Iteration 15830, Loss: 0.005359486677\n",
      "Iteration 15840, Loss: 0.005453176796\n",
      "Iteration 15850, Loss: 0.006328808144\n",
      "Iteration 15860, Loss: 0.005152138881\n",
      "Iteration 15870, Loss: 0.005757532548\n",
      "Iteration 15880, Loss: 0.005637188442\n",
      "Iteration 15890, Loss: 0.005217439961\n",
      "Iteration 15900, Loss: 0.005516454577\n",
      "Iteration 15910, Loss: 0.008515557274\n",
      "Iteration 15920, Loss: 0.005869099870\n",
      "Iteration 15930, Loss: 0.005272418726\n",
      "Iteration 15940, Loss: 0.005426425952\n",
      "Iteration 15950, Loss: 0.005491474178\n",
      "Iteration 15960, Loss: 0.005189015530\n",
      "Iteration 15970, Loss: 0.006162094884\n",
      "Iteration 15980, Loss: 0.005438846536\n",
      "Iteration 15990, Loss: 0.005137302447\n",
      "Iteration 16000, Loss: 0.005129248835\n",
      "Iteration 16010, Loss: 0.005319526419\n",
      "Iteration 16020, Loss: 0.005112854764\n",
      "Iteration 16030, Loss: 0.005227282643\n",
      "Iteration 16040, Loss: 0.008536752313\n",
      "Iteration 16050, Loss: 0.006558518857\n",
      "Iteration 16060, Loss: 0.005719168577\n",
      "Iteration 16070, Loss: 0.005434639286\n",
      "Iteration 16080, Loss: 0.005140926689\n",
      "Iteration 16090, Loss: 0.005440621637\n",
      "Iteration 16100, Loss: 0.006228314247\n",
      "Iteration 16110, Loss: 0.005235999357\n",
      "Iteration 16120, Loss: 0.005424416624\n",
      "Iteration 16130, Loss: 0.006003676448\n",
      "Iteration 16140, Loss: 0.005702974275\n",
      "Iteration 16150, Loss: 0.005698156077\n",
      "Iteration 16160, Loss: 0.005335314665\n",
      "Iteration 16170, Loss: 0.005153844133\n",
      "Iteration 16180, Loss: 0.006513789296\n",
      "Iteration 16190, Loss: 0.005191748496\n",
      "Iteration 16200, Loss: 0.005413315725\n",
      "Iteration 16210, Loss: 0.005327350926\n",
      "Iteration 16220, Loss: 0.005136433989\n",
      "Iteration 16230, Loss: 0.005290050060\n",
      "Iteration 16240, Loss: 0.006185819861\n",
      "Iteration 16250, Loss: 0.005227079149\n",
      "Iteration 16260, Loss: 0.005809157621\n",
      "Iteration 16270, Loss: 0.005232568830\n",
      "Iteration 16280, Loss: 0.005363778211\n",
      "Iteration 16290, Loss: 0.005424906500\n",
      "Iteration 16300, Loss: 0.010366387665\n",
      "Iteration 16310, Loss: 0.005288816988\n",
      "Iteration 16320, Loss: 0.005742907524\n",
      "Iteration 16330, Loss: 0.005362500437\n",
      "Iteration 16340, Loss: 0.005236710422\n",
      "Iteration 16350, Loss: 0.005109592341\n",
      "Iteration 16360, Loss: 0.005139296409\n",
      "Iteration 16370, Loss: 0.005654795095\n",
      "Iteration 16380, Loss: 0.006832206622\n",
      "Iteration 16390, Loss: 0.005703295581\n",
      "Iteration 16400, Loss: 0.005366182886\n",
      "Iteration 16410, Loss: 0.005235054530\n",
      "Iteration 16420, Loss: 0.005398553330\n",
      "Iteration 16430, Loss: 0.006611317396\n",
      "Iteration 16440, Loss: 0.005214096047\n",
      "Iteration 16450, Loss: 0.005581663921\n",
      "Iteration 16460, Loss: 0.005209173076\n",
      "Iteration 16470, Loss: 0.006391793024\n",
      "Iteration 16480, Loss: 0.005189983174\n",
      "Iteration 16490, Loss: 0.005735002458\n",
      "Iteration 16500, Loss: 0.005166059826\n",
      "Iteration 16510, Loss: 0.006217619870\n",
      "Iteration 16520, Loss: 0.005319789983\n",
      "Iteration 16530, Loss: 0.005678648129\n",
      "Iteration 16540, Loss: 0.005204868503\n",
      "Iteration 16550, Loss: 0.005370370578\n",
      "Iteration 16560, Loss: 0.008262458257\n",
      "Iteration 16570, Loss: 0.006484254729\n",
      "Iteration 16580, Loss: 0.005553078838\n",
      "Iteration 16590, Loss: 0.005327326711\n",
      "Iteration 16600, Loss: 0.005185430869\n",
      "Iteration 16610, Loss: 0.005139210261\n",
      "Iteration 16620, Loss: 0.005990055390\n",
      "Iteration 16630, Loss: 0.005486078560\n",
      "Iteration 16640, Loss: 0.005223229062\n",
      "Iteration 16650, Loss: 0.005322221667\n",
      "Iteration 16660, Loss: 0.005207129754\n",
      "Iteration 16670, Loss: 0.005464775488\n",
      "Iteration 16680, Loss: 0.005296925083\n",
      "Iteration 16690, Loss: 0.005704355892\n",
      "Iteration 16700, Loss: 0.006372491829\n",
      "Iteration 16710, Loss: 0.005574099254\n",
      "Iteration 16720, Loss: 0.005264862906\n",
      "Iteration 16730, Loss: 0.005337553099\n",
      "Iteration 16740, Loss: 0.006990127731\n",
      "Iteration 16750, Loss: 0.005309928209\n",
      "Iteration 16760, Loss: 0.005364359822\n",
      "Iteration 16770, Loss: 0.005472810939\n",
      "Iteration 16780, Loss: 0.006120087113\n",
      "Iteration 16790, Loss: 0.005515119527\n",
      "Iteration 16800, Loss: 0.005675349850\n",
      "Iteration 16810, Loss: 0.005284694023\n",
      "Iteration 16820, Loss: 0.005172677804\n",
      "Iteration 16830, Loss: 0.006665837020\n",
      "Iteration 16840, Loss: 0.005220447667\n",
      "Iteration 16850, Loss: 0.005149336997\n",
      "Iteration 16860, Loss: 0.005097509827\n",
      "Iteration 16870, Loss: 0.005341655109\n",
      "Iteration 16880, Loss: 0.005242682528\n",
      "Iteration 16890, Loss: 0.005924296565\n",
      "Iteration 16900, Loss: 0.005629568361\n",
      "Iteration 16910, Loss: 0.005771378055\n",
      "Iteration 16920, Loss: 0.005190505181\n",
      "Iteration 16930, Loss: 0.005588617176\n",
      "Iteration 16940, Loss: 0.006973979995\n",
      "Iteration 16950, Loss: 0.005512985401\n",
      "Iteration 16960, Loss: 0.005212555639\n",
      "Iteration 16970, Loss: 0.005619520787\n",
      "Iteration 16980, Loss: 0.005644374527\n",
      "Iteration 16990, Loss: 0.005094955210\n",
      "Iteration 17000, Loss: 0.005648027174\n",
      "Iteration 17010, Loss: 0.006963703781\n",
      "Iteration 17020, Loss: 0.005854590796\n",
      "Iteration 17030, Loss: 0.005417736247\n",
      "Iteration 17040, Loss: 0.005214031320\n",
      "Iteration 17050, Loss: 0.005414967425\n",
      "Iteration 17060, Loss: 0.006686604582\n",
      "Iteration 17070, Loss: 0.005361336283\n",
      "Iteration 17080, Loss: 0.005369089544\n",
      "Iteration 17090, Loss: 0.005427493248\n",
      "Iteration 17100, Loss: 0.006122730207\n",
      "Iteration 17110, Loss: 0.005190408789\n",
      "Iteration 17120, Loss: 0.005815914832\n",
      "Iteration 17130, Loss: 0.005645625759\n",
      "Iteration 17140, Loss: 0.005147253163\n",
      "Iteration 17150, Loss: 0.005170661025\n",
      "Iteration 17160, Loss: 0.008690922521\n",
      "Iteration 17170, Loss: 0.007093387656\n",
      "Iteration 17180, Loss: 0.005334071349\n",
      "Iteration 17190, Loss: 0.005250087939\n",
      "Iteration 17200, Loss: 0.005126574542\n",
      "Iteration 17210, Loss: 0.005124984309\n",
      "Iteration 17220, Loss: 0.005407444201\n",
      "Iteration 17230, Loss: 0.007221930660\n",
      "Iteration 17240, Loss: 0.006075696088\n",
      "Iteration 17250, Loss: 0.005258223042\n",
      "Iteration 17260, Loss: 0.005308485590\n",
      "Iteration 17270, Loss: 0.005077778362\n",
      "Iteration 17280, Loss: 0.005161236972\n",
      "Iteration 17290, Loss: 0.009329326451\n",
      "Iteration 17300, Loss: 0.006909527816\n",
      "Iteration 17310, Loss: 0.005266088527\n",
      "Iteration 17320, Loss: 0.005134894047\n",
      "Iteration 17330, Loss: 0.005132915918\n",
      "Iteration 17340, Loss: 0.005370832048\n",
      "Iteration 17350, Loss: 0.005270407069\n",
      "Iteration 17360, Loss: 0.005979630165\n",
      "Iteration 17370, Loss: 0.005433197599\n",
      "Iteration 17380, Loss: 0.005803691689\n",
      "Iteration 17390, Loss: 0.005116314162\n",
      "Iteration 17400, Loss: 0.005957956426\n",
      "Iteration 17410, Loss: 0.005380086601\n",
      "Iteration 17420, Loss: 0.005388502963\n",
      "Iteration 17430, Loss: 0.005973829422\n",
      "Iteration 17440, Loss: 0.005168691743\n",
      "Iteration 17450, Loss: 0.005129596684\n",
      "Iteration 17460, Loss: 0.007065639831\n",
      "Iteration 17470, Loss: 0.005898553412\n",
      "Iteration 17480, Loss: 0.005732240155\n",
      "Iteration 17490, Loss: 0.005455209874\n",
      "Iteration 17500, Loss: 0.005138925742\n",
      "Iteration 17510, Loss: 0.005547652487\n",
      "Iteration 17520, Loss: 0.005503575318\n",
      "Iteration 17530, Loss: 0.005663816351\n",
      "Iteration 17540, Loss: 0.005266408902\n",
      "Iteration 17550, Loss: 0.005347799975\n",
      "Iteration 17560, Loss: 0.007627090439\n",
      "Iteration 17570, Loss: 0.006151810288\n",
      "Iteration 17580, Loss: 0.005334277172\n",
      "Iteration 17590, Loss: 0.005109286401\n",
      "Iteration 17600, Loss: 0.005258119665\n",
      "Iteration 17610, Loss: 0.006786447950\n",
      "Iteration 17620, Loss: 0.005183459260\n",
      "Iteration 17630, Loss: 0.005266953260\n",
      "Iteration 17640, Loss: 0.005445912946\n",
      "Iteration 17650, Loss: 0.005390197504\n",
      "Iteration 17660, Loss: 0.005699942820\n",
      "Iteration 17670, Loss: 0.006077512167\n",
      "Iteration 17680, Loss: 0.005408218596\n",
      "Iteration 17690, Loss: 0.005483755376\n",
      "Iteration 17700, Loss: 0.005150919780\n",
      "Iteration 17710, Loss: 0.005132897291\n",
      "Iteration 17720, Loss: 0.005795897450\n",
      "Iteration 17730, Loss: 0.005223452114\n",
      "Iteration 17740, Loss: 0.006531974766\n",
      "Iteration 17750, Loss: 0.005416374654\n",
      "Iteration 17760, Loss: 0.005313294474\n",
      "Iteration 17770, Loss: 0.005199639592\n",
      "Iteration 17780, Loss: 0.005206698552\n",
      "Iteration 17790, Loss: 0.005082943942\n",
      "Iteration 17800, Loss: 0.005179070868\n",
      "Iteration 17810, Loss: 0.007107637357\n",
      "Iteration 17820, Loss: 0.005716111977\n",
      "Iteration 17830, Loss: 0.005307005253\n",
      "Iteration 17840, Loss: 0.005135986488\n",
      "Iteration 17850, Loss: 0.005209259689\n",
      "Iteration 17860, Loss: 0.005423497874\n",
      "Iteration 17870, Loss: 0.006804795936\n",
      "Iteration 17880, Loss: 0.005361196119\n",
      "Iteration 17890, Loss: 0.005230620503\n",
      "Iteration 17900, Loss: 0.005472713150\n",
      "Iteration 17910, Loss: 0.005962894764\n",
      "Iteration 17920, Loss: 0.005172135308\n",
      "Iteration 17930, Loss: 0.005207094830\n",
      "Iteration 17940, Loss: 0.006296992768\n",
      "Iteration 17950, Loss: 0.005159398075\n",
      "Iteration 17960, Loss: 0.005256817210\n",
      "Iteration 17970, Loss: 0.005228189752\n",
      "Iteration 17980, Loss: 0.005094735418\n",
      "Iteration 17990, Loss: 0.005256524310\n",
      "Iteration 18000, Loss: 0.006358284503\n",
      "Iteration 18010, Loss: 0.005074986257\n",
      "Iteration 18020, Loss: 0.005623643287\n",
      "Iteration 18030, Loss: 0.005296369549\n",
      "Iteration 18040, Loss: 0.005631049629\n",
      "Iteration 18050, Loss: 0.007045597304\n",
      "Iteration 18060, Loss: 0.006118042395\n",
      "Iteration 18070, Loss: 0.005422090180\n",
      "Iteration 18080, Loss: 0.005077758338\n",
      "Iteration 18090, Loss: 0.005249259062\n",
      "Iteration 18100, Loss: 0.007543976419\n",
      "Iteration 18110, Loss: 0.005762544461\n",
      "Iteration 18120, Loss: 0.005227760412\n",
      "Iteration 18130, Loss: 0.005160456989\n",
      "Iteration 18140, Loss: 0.005124850664\n",
      "Iteration 18150, Loss: 0.006115462165\n",
      "Iteration 18160, Loss: 0.005172040313\n",
      "Iteration 18170, Loss: 0.005197406281\n",
      "Iteration 18180, Loss: 0.005250255112\n",
      "Iteration 18190, Loss: 0.005213181954\n",
      "Iteration 18200, Loss: 0.005817016121\n",
      "Iteration 18210, Loss: 0.005645512603\n",
      "Iteration 18220, Loss: 0.005472491961\n",
      "Iteration 18230, Loss: 0.005346980877\n",
      "Iteration 18240, Loss: 0.005246029235\n",
      "Iteration 18250, Loss: 0.005847022403\n",
      "Iteration 18260, Loss: 0.005673693493\n",
      "Iteration 18270, Loss: 0.005095563363\n",
      "Iteration 18280, Loss: 0.005149250384\n",
      "Iteration 18290, Loss: 0.005090442486\n",
      "Iteration 18300, Loss: 0.005255124532\n",
      "Iteration 18310, Loss: 0.005999967456\n",
      "Iteration 18320, Loss: 0.005280506797\n",
      "Iteration 18330, Loss: 0.005564397667\n",
      "Iteration 18340, Loss: 0.005370785482\n",
      "Iteration 18350, Loss: 0.005091590341\n",
      "Iteration 18360, Loss: 0.005592818838\n",
      "Iteration 18370, Loss: 0.006703610532\n",
      "Iteration 18380, Loss: 0.005238146987\n",
      "Iteration 18390, Loss: 0.005360838491\n",
      "Iteration 18400, Loss: 0.005177938379\n",
      "Iteration 18410, Loss: 0.005112313200\n",
      "Iteration 18420, Loss: 0.005203003064\n",
      "Iteration 18430, Loss: 0.007510333788\n",
      "Iteration 18440, Loss: 0.005877072457\n",
      "Iteration 18450, Loss: 0.005359751172\n",
      "Iteration 18460, Loss: 0.005099594593\n",
      "Iteration 18470, Loss: 0.005270088091\n",
      "Iteration 18480, Loss: 0.005677190609\n",
      "Iteration 18490, Loss: 0.005717124324\n",
      "Iteration 18500, Loss: 0.005336028058\n",
      "Iteration 18510, Loss: 0.005632035900\n",
      "Iteration 18520, Loss: 0.005182054825\n",
      "Iteration 18530, Loss: 0.005343752913\n",
      "Iteration 18540, Loss: 0.008403072134\n",
      "Iteration 18550, Loss: 0.006169690751\n",
      "Iteration 18560, Loss: 0.005420212634\n",
      "Iteration 18570, Loss: 0.005364067852\n",
      "Iteration 18580, Loss: 0.005207072943\n",
      "Iteration 18590, Loss: 0.005050906446\n",
      "Iteration 18600, Loss: 0.005071800668\n",
      "Iteration 18610, Loss: 0.006265774835\n",
      "Iteration 18620, Loss: 0.006999101490\n",
      "Iteration 18630, Loss: 0.005247747526\n",
      "Iteration 18640, Loss: 0.005445105955\n",
      "Iteration 18650, Loss: 0.005160728469\n",
      "Iteration 18660, Loss: 0.005066461861\n",
      "Iteration 18670, Loss: 0.005111497361\n",
      "Iteration 18680, Loss: 0.005117257126\n",
      "Iteration 18690, Loss: 0.005178241059\n",
      "Iteration 18700, Loss: 0.007656725589\n",
      "Iteration 18710, Loss: 0.006267073564\n",
      "Iteration 18720, Loss: 0.005451169796\n",
      "Iteration 18730, Loss: 0.005173390266\n",
      "Iteration 18740, Loss: 0.005157210864\n",
      "Iteration 18750, Loss: 0.005204393528\n",
      "Iteration 18760, Loss: 0.007344787475\n",
      "Iteration 18770, Loss: 0.005692747422\n",
      "Iteration 18780, Loss: 0.005200146697\n",
      "Iteration 18790, Loss: 0.005106020253\n",
      "Iteration 18800, Loss: 0.005172655918\n",
      "Iteration 18810, Loss: 0.006238352042\n",
      "Iteration 18820, Loss: 0.005064860452\n",
      "Iteration 18830, Loss: 0.005553884432\n",
      "Iteration 18840, Loss: 0.005400248338\n",
      "Iteration 18850, Loss: 0.005419485737\n",
      "Iteration 18860, Loss: 0.005064522382\n",
      "Iteration 18870, Loss: 0.005065236241\n",
      "Iteration 18880, Loss: 0.006004390307\n",
      "Iteration 18890, Loss: 0.008717607707\n",
      "Iteration 18900, Loss: 0.005206354894\n",
      "Iteration 18910, Loss: 0.005547101144\n",
      "Iteration 18920, Loss: 0.005305424798\n",
      "Iteration 18930, Loss: 0.005079572089\n",
      "Iteration 18940, Loss: 0.005384513643\n",
      "Iteration 18950, Loss: 0.005664887372\n",
      "Iteration 18960, Loss: 0.005197324324\n",
      "Iteration 18970, Loss: 0.005037930794\n",
      "Iteration 18980, Loss: 0.005235284567\n",
      "Iteration 18990, Loss: 0.009346639737\n",
      "Iteration 19000, Loss: 0.005099121947\n",
      "Iteration 19010, Loss: 0.005567005370\n",
      "Iteration 19020, Loss: 0.005271086469\n",
      "Iteration 19030, Loss: 0.005168077536\n",
      "Iteration 19040, Loss: 0.005165569019\n",
      "Iteration 19050, Loss: 0.005225852132\n",
      "Iteration 19060, Loss: 0.006541464012\n",
      "Iteration 19070, Loss: 0.005085144192\n",
      "Iteration 19080, Loss: 0.005373218097\n",
      "Iteration 19090, Loss: 0.005378266331\n",
      "Iteration 19100, Loss: 0.005728825461\n",
      "Iteration 19110, Loss: 0.005065405741\n",
      "Iteration 19120, Loss: 0.005079851951\n",
      "Iteration 19130, Loss: 0.005045856815\n",
      "Iteration 19140, Loss: 0.008652793244\n",
      "Iteration 19150, Loss: 0.007476235274\n",
      "Iteration 19160, Loss: 0.006234714296\n",
      "Iteration 19170, Loss: 0.005488738883\n",
      "Iteration 19180, Loss: 0.005104430951\n",
      "Iteration 19190, Loss: 0.005090374034\n",
      "Iteration 19200, Loss: 0.005079612602\n",
      "Iteration 19210, Loss: 0.005051311571\n",
      "Iteration 19220, Loss: 0.005082534160\n",
      "Iteration 19230, Loss: 0.005812658928\n",
      "Iteration 19240, Loss: 0.005526326597\n",
      "Iteration 19250, Loss: 0.005088692531\n",
      "Iteration 19260, Loss: 0.005053065252\n",
      "Iteration 19270, Loss: 0.005184738431\n",
      "Iteration 19280, Loss: 0.005041261669\n",
      "Iteration 19290, Loss: 0.005312044639\n",
      "Iteration 19300, Loss: 0.008140067570\n",
      "Iteration 19310, Loss: 0.005949989893\n",
      "Iteration 19320, Loss: 0.005335983820\n",
      "Iteration 19330, Loss: 0.005355176516\n",
      "Iteration 19340, Loss: 0.005048989318\n",
      "Iteration 19350, Loss: 0.005210716743\n",
      "Iteration 19360, Loss: 0.008316026069\n",
      "Iteration 19370, Loss: 0.006328668911\n",
      "Iteration 19380, Loss: 0.005480191670\n",
      "Iteration 19390, Loss: 0.005268265493\n",
      "Iteration 19400, Loss: 0.005070189014\n",
      "Iteration 19410, Loss: 0.005261986051\n",
      "Iteration 19420, Loss: 0.007729419973\n",
      "Iteration 19430, Loss: 0.006033121143\n",
      "Iteration 19440, Loss: 0.005393946078\n",
      "Iteration 19450, Loss: 0.005086974707\n",
      "Iteration 19460, Loss: 0.005338176619\n",
      "Iteration 19470, Loss: 0.006270449609\n",
      "Iteration 19480, Loss: 0.005070497282\n",
      "Iteration 19490, Loss: 0.005674356129\n",
      "Iteration 19500, Loss: 0.005049724132\n",
      "Iteration 19510, Loss: 0.005142300390\n",
      "Iteration 19520, Loss: 0.009056949988\n",
      "Iteration 19530, Loss: 0.006415895186\n",
      "Iteration 19540, Loss: 0.005149109755\n",
      "Iteration 19550, Loss: 0.005158309359\n",
      "Iteration 19560, Loss: 0.005104358308\n",
      "Iteration 19570, Loss: 0.005106986966\n",
      "Iteration 19580, Loss: 0.006117983721\n",
      "Iteration 19590, Loss: 0.005066415295\n",
      "Iteration 19600, Loss: 0.005080572329\n",
      "Iteration 19610, Loss: 0.005132898688\n",
      "Iteration 19620, Loss: 0.005182692781\n",
      "Iteration 19630, Loss: 0.005620253272\n",
      "Iteration 19640, Loss: 0.005774602294\n",
      "Iteration 19650, Loss: 0.005502795801\n",
      "Iteration 19660, Loss: 0.005152331665\n",
      "Iteration 19670, Loss: 0.005138466600\n",
      "Iteration 19680, Loss: 0.007084221579\n",
      "Iteration 19690, Loss: 0.005807897542\n",
      "Iteration 19700, Loss: 0.005362009164\n",
      "Iteration 19710, Loss: 0.005176858976\n",
      "Iteration 19720, Loss: 0.005425880197\n",
      "Iteration 19730, Loss: 0.005266844761\n",
      "Iteration 19740, Loss: 0.006758367177\n",
      "Iteration 19750, Loss: 0.005252399947\n",
      "Iteration 19760, Loss: 0.005113854539\n",
      "Iteration 19770, Loss: 0.005587928928\n",
      "Iteration 19780, Loss: 0.005120927002\n",
      "Iteration 19790, Loss: 0.005038493779\n",
      "Iteration 19800, Loss: 0.005956300534\n",
      "Iteration 19810, Loss: 0.005135825370\n",
      "Iteration 19820, Loss: 0.005894831382\n",
      "Iteration 19830, Loss: 0.005386353936\n",
      "Iteration 19840, Loss: 0.005432738923\n",
      "Iteration 19850, Loss: 0.005227429792\n",
      "Iteration 19860, Loss: 0.005460380111\n",
      "Iteration 19870, Loss: 0.006167123094\n",
      "Iteration 19880, Loss: 0.005198601633\n",
      "Iteration 19890, Loss: 0.005465981085\n",
      "Iteration 19900, Loss: 0.005021211691\n",
      "Iteration 19910, Loss: 0.005318408366\n",
      "Iteration 19920, Loss: 0.008291269653\n",
      "Iteration 19930, Loss: 0.005339483265\n",
      "Iteration 19940, Loss: 0.005105135031\n",
      "Iteration 19950, Loss: 0.005025115330\n",
      "Iteration 19960, Loss: 0.005218618549\n",
      "Iteration 19970, Loss: 0.005274786614\n",
      "Iteration 19980, Loss: 0.006345251575\n",
      "Iteration 19990, Loss: 0.005043459591\n",
      "Iteration 20000, Loss: 0.005703048781\n",
      "Iteration 20010, Loss: 0.005084281322\n",
      "Iteration 20020, Loss: 0.005976474378\n",
      "Iteration 20030, Loss: 0.005496349186\n",
      "Iteration 20040, Loss: 0.005658287555\n",
      "Iteration 20050, Loss: 0.005093570799\n",
      "Iteration 20060, Loss: 0.005825123284\n",
      "Iteration 20070, Loss: 0.005565168336\n",
      "Iteration 20080, Loss: 0.005340555217\n",
      "Iteration 20090, Loss: 0.005777668208\n",
      "Iteration 20100, Loss: 0.005150603596\n",
      "Iteration 20110, Loss: 0.005031131208\n",
      "Iteration 20120, Loss: 0.005716177635\n",
      "Iteration 20130, Loss: 0.005208680872\n",
      "Iteration 20140, Loss: 0.006385712884\n",
      "Iteration 20150, Loss: 0.005226606503\n",
      "Iteration 20160, Loss: 0.005062166136\n",
      "Iteration 20170, Loss: 0.005084232427\n",
      "Iteration 20180, Loss: 0.005086542573\n",
      "Iteration 20190, Loss: 0.005077357404\n",
      "Iteration 20200, Loss: 0.007002766710\n",
      "Iteration 20210, Loss: 0.006174809765\n",
      "Iteration 20220, Loss: 0.005794595927\n",
      "Iteration 20230, Loss: 0.005267261527\n",
      "Iteration 20240, Loss: 0.005178969353\n",
      "Iteration 20250, Loss: 0.005021779332\n",
      "Iteration 20260, Loss: 0.005133869592\n",
      "Iteration 20270, Loss: 0.007448130287\n",
      "Iteration 20280, Loss: 0.006184376311\n",
      "Iteration 20290, Loss: 0.005439648405\n",
      "Iteration 20300, Loss: 0.005114618689\n",
      "Iteration 20310, Loss: 0.005178262480\n",
      "Iteration 20320, Loss: 0.005473363213\n",
      "Iteration 20330, Loss: 0.006809133105\n",
      "Iteration 20340, Loss: 0.005441769026\n",
      "Iteration 20350, Loss: 0.005048594903\n",
      "Iteration 20360, Loss: 0.005611699075\n",
      "Iteration 20370, Loss: 0.005311813671\n",
      "Iteration 20380, Loss: 0.005021056160\n",
      "Iteration 20390, Loss: 0.005016983952\n",
      "Iteration 20400, Loss: 0.005555806682\n",
      "Iteration 20410, Loss: 0.005277243443\n",
      "Iteration 20420, Loss: 0.006171477959\n",
      "Iteration 20430, Loss: 0.005707793869\n",
      "Iteration 20440, Loss: 0.005075121298\n",
      "Iteration 20450, Loss: 0.005248764530\n",
      "Iteration 20460, Loss: 0.005074990448\n",
      "Iteration 20470, Loss: 0.005032382440\n",
      "Iteration 20480, Loss: 0.005045707338\n",
      "Iteration 20490, Loss: 0.005852886941\n",
      "Iteration 20500, Loss: 0.005147690885\n",
      "Iteration 20510, Loss: 0.005117749330\n",
      "Iteration 20520, Loss: 0.005134036765\n",
      "Iteration 20530, Loss: 0.005018072668\n",
      "Iteration 20540, Loss: 0.005239364691\n",
      "Iteration 20550, Loss: 0.006082235835\n",
      "Iteration 20560, Loss: 0.005020858720\n",
      "Iteration 20570, Loss: 0.005553339608\n",
      "Iteration 20580, Loss: 0.005258396734\n",
      "Iteration 20590, Loss: 0.005081046373\n",
      "Iteration 20600, Loss: 0.005195283797\n",
      "Iteration 20610, Loss: 0.009210860357\n",
      "Iteration 20620, Loss: 0.006214436144\n",
      "Iteration 20630, Loss: 0.005159604363\n",
      "Iteration 20640, Loss: 0.005133307073\n",
      "Iteration 20650, Loss: 0.005032048095\n",
      "Iteration 20660, Loss: 0.005334897898\n",
      "Iteration 20670, Loss: 0.005917135626\n",
      "Iteration 20680, Loss: 0.005013347603\n",
      "Iteration 20690, Loss: 0.005473677535\n",
      "Iteration 20700, Loss: 0.005784948356\n",
      "Iteration 20710, Loss: 0.005155639257\n",
      "Iteration 20720, Loss: 0.005839609541\n",
      "Iteration 20730, Loss: 0.005425236188\n",
      "Iteration 20740, Loss: 0.005194555037\n",
      "Iteration 20750, Loss: 0.006293571554\n",
      "Iteration 20760, Loss: 0.005025630351\n",
      "Iteration 20770, Loss: 0.005858534016\n",
      "Iteration 20780, Loss: 0.005009300075\n",
      "Iteration 20790, Loss: 0.005645231344\n",
      "Iteration 20800, Loss: 0.005903339479\n",
      "Iteration 20810, Loss: 0.005713506602\n",
      "Iteration 20820, Loss: 0.005375234876\n",
      "Iteration 20830, Loss: 0.005023514852\n",
      "Iteration 20840, Loss: 0.005328964908\n",
      "Iteration 20850, Loss: 0.008035236970\n",
      "Iteration 20860, Loss: 0.005946433172\n",
      "Iteration 20870, Loss: 0.005281748716\n",
      "Iteration 20880, Loss: 0.005180141423\n",
      "Iteration 20890, Loss: 0.005133680534\n",
      "Iteration 20900, Loss: 0.006367585156\n",
      "Iteration 20910, Loss: 0.005120914429\n",
      "Iteration 20920, Loss: 0.005112943705\n",
      "Iteration 20930, Loss: 0.005078122020\n",
      "Iteration 20940, Loss: 0.005142172333\n",
      "Iteration 20950, Loss: 0.005656153429\n",
      "Iteration 20960, Loss: 0.005832042545\n",
      "Iteration 20970, Loss: 0.005716227461\n",
      "Iteration 20980, Loss: 0.005223586690\n",
      "Iteration 20990, Loss: 0.005347513594\n",
      "Iteration 21000, Loss: 0.005169206765\n",
      "Iteration 21010, Loss: 0.006773944944\n",
      "Iteration 21020, Loss: 0.005230478477\n",
      "Iteration 21030, Loss: 0.005102016497\n",
      "Iteration 21040, Loss: 0.005233695265\n",
      "Iteration 21050, Loss: 0.005033763126\n",
      "Iteration 21060, Loss: 0.005047232844\n",
      "Iteration 21070, Loss: 0.006734695751\n",
      "Iteration 21080, Loss: 0.007516641170\n",
      "Iteration 21090, Loss: 0.005178860389\n",
      "Iteration 21100, Loss: 0.005450685509\n",
      "Iteration 21110, Loss: 0.005214408971\n",
      "Iteration 21120, Loss: 0.005129549652\n",
      "Iteration 21130, Loss: 0.005042858887\n",
      "Iteration 21140, Loss: 0.005028387066\n",
      "Iteration 21150, Loss: 0.005488222465\n",
      "Iteration 21160, Loss: 0.006009772420\n",
      "Iteration 21170, Loss: 0.005082088523\n",
      "Iteration 21180, Loss: 0.005092465319\n",
      "Iteration 21190, Loss: 0.005121053662\n",
      "Iteration 21200, Loss: 0.005052528810\n",
      "Iteration 21210, Loss: 0.006042914931\n",
      "Iteration 21220, Loss: 0.005042139906\n",
      "Iteration 21230, Loss: 0.005070336163\n",
      "Iteration 21240, Loss: 0.005085005425\n",
      "Iteration 21250, Loss: 0.005209658761\n",
      "Iteration 21260, Loss: 0.005654462613\n",
      "Iteration 21270, Loss: 0.005613258108\n",
      "Iteration 21280, Loss: 0.005401792470\n",
      "Iteration 21290, Loss: 0.005201572552\n",
      "Iteration 21300, Loss: 0.005283053499\n",
      "Iteration 21310, Loss: 0.005049803294\n",
      "Iteration 21320, Loss: 0.006451196037\n",
      "Iteration 21330, Loss: 0.006563640200\n",
      "Iteration 21340, Loss: 0.005306727719\n",
      "Iteration 21350, Loss: 0.005405204836\n",
      "Iteration 21360, Loss: 0.005177586339\n",
      "Iteration 21370, Loss: 0.005105791148\n",
      "Iteration 21380, Loss: 0.005082935560\n",
      "Iteration 21390, Loss: 0.005237299949\n",
      "Iteration 21400, Loss: 0.006591295823\n",
      "Iteration 21410, Loss: 0.005293523893\n",
      "Iteration 21420, Loss: 0.005093536805\n",
      "Iteration 21430, Loss: 0.005391712300\n",
      "Iteration 21440, Loss: 0.005382829811\n",
      "Iteration 21450, Loss: 0.005328988191\n",
      "Iteration 21460, Loss: 0.005939802621\n",
      "Iteration 21470, Loss: 0.005093938671\n",
      "Iteration 21480, Loss: 0.005506305955\n",
      "Iteration 21490, Loss: 0.005626526661\n",
      "Iteration 21500, Loss: 0.005096903536\n",
      "Iteration 21510, Loss: 0.005599648692\n",
      "Iteration 21520, Loss: 0.006390396971\n",
      "Iteration 21530, Loss: 0.005897570867\n",
      "Iteration 21540, Loss: 0.005481279455\n",
      "Iteration 21550, Loss: 0.005020922981\n",
      "Iteration 21560, Loss: 0.005575869698\n",
      "Iteration 21570, Loss: 0.005805492867\n",
      "Iteration 21580, Loss: 0.005685146432\n",
      "Iteration 21590, Loss: 0.005315796472\n",
      "Iteration 21600, Loss: 0.005058803130\n",
      "Iteration 21610, Loss: 0.005051632412\n",
      "Iteration 21620, Loss: 0.005656986032\n",
      "Iteration 21630, Loss: 0.005261477549\n",
      "Iteration 21640, Loss: 0.006203862373\n",
      "Iteration 21650, Loss: 0.005205155816\n",
      "Iteration 21660, Loss: 0.005073414184\n",
      "Iteration 21670, Loss: 0.005123426672\n",
      "Iteration 21680, Loss: 0.005292634480\n",
      "Iteration 21690, Loss: 0.005817407276\n",
      "Iteration 21700, Loss: 0.005008784588\n",
      "Iteration 21710, Loss: 0.005400830880\n",
      "Iteration 21720, Loss: 0.005833202042\n",
      "Iteration 21730, Loss: 0.004997199867\n",
      "Iteration 21740, Loss: 0.005488039460\n",
      "Iteration 21750, Loss: 0.006198122632\n",
      "Iteration 21760, Loss: 0.005956847686\n",
      "Iteration 21770, Loss: 0.005253829993\n",
      "Iteration 21780, Loss: 0.005357649177\n",
      "Iteration 21790, Loss: 0.005081675947\n",
      "Iteration 21800, Loss: 0.006869708188\n",
      "Iteration 21810, Loss: 0.005486504175\n",
      "Iteration 21820, Loss: 0.005302888807\n",
      "Iteration 21830, Loss: 0.005096163601\n",
      "Iteration 21840, Loss: 0.005106980912\n",
      "Iteration 21850, Loss: 0.005143212620\n",
      "Iteration 21860, Loss: 0.007488507777\n",
      "Iteration 21870, Loss: 0.006199403666\n",
      "Iteration 21880, Loss: 0.005426033866\n",
      "Iteration 21890, Loss: 0.005152946338\n",
      "Iteration 21900, Loss: 0.005094149616\n",
      "Iteration 21910, Loss: 0.005456158426\n",
      "Iteration 21920, Loss: 0.006840569898\n",
      "Iteration 21930, Loss: 0.005862262100\n",
      "Iteration 21940, Loss: 0.005263692234\n",
      "Iteration 21950, Loss: 0.005072817206\n",
      "Iteration 21960, Loss: 0.005034309812\n",
      "Iteration 21970, Loss: 0.006898162887\n",
      "Iteration 21980, Loss: 0.005956269801\n",
      "Iteration 21990, Loss: 0.005586311221\n",
      "Iteration 22000, Loss: 0.005300164223\n",
      "Iteration 22010, Loss: 0.005075291730\n",
      "Iteration 22020, Loss: 0.005518847145\n",
      "Iteration 22030, Loss: 0.005618046969\n",
      "Iteration 22040, Loss: 0.005167577416\n",
      "Iteration 22050, Loss: 0.005607089493\n",
      "Iteration 22060, Loss: 0.005470519885\n",
      "Iteration 22070, Loss: 0.004990977701\n",
      "Iteration 22080, Loss: 0.005072141998\n",
      "Iteration 22090, Loss: 0.008997599594\n",
      "Iteration 22100, Loss: 0.006514759734\n",
      "Iteration 22110, Loss: 0.005065995269\n",
      "Iteration 22120, Loss: 0.005161126610\n",
      "Iteration 22130, Loss: 0.005085550249\n",
      "Iteration 22140, Loss: 0.005272406153\n",
      "Iteration 22150, Loss: 0.005677856505\n",
      "Iteration 22160, Loss: 0.005136062857\n",
      "Iteration 22170, Loss: 0.005107000470\n",
      "Iteration 22180, Loss: 0.006208223756\n",
      "Iteration 22190, Loss: 0.004993265495\n",
      "Iteration 22200, Loss: 0.005323191173\n",
      "Iteration 22210, Loss: 0.005362005439\n",
      "Iteration 22220, Loss: 0.005255566910\n",
      "Iteration 22230, Loss: 0.005112168379\n",
      "Iteration 22240, Loss: 0.006508977152\n",
      "Iteration 22250, Loss: 0.005281651393\n",
      "Iteration 22260, Loss: 0.005137462635\n",
      "Iteration 22270, Loss: 0.005073909648\n",
      "Iteration 22280, Loss: 0.005305072293\n",
      "Iteration 22290, Loss: 0.005805222783\n",
      "Iteration 22300, Loss: 0.005234729964\n",
      "Iteration 22310, Loss: 0.005295942072\n",
      "Iteration 22320, Loss: 0.005524178036\n",
      "Iteration 22330, Loss: 0.005288464949\n",
      "Iteration 22340, Loss: 0.005413413048\n",
      "Iteration 22350, Loss: 0.006750643253\n",
      "Iteration 22360, Loss: 0.005839536898\n",
      "Iteration 22370, Loss: 0.005111409817\n",
      "Iteration 22380, Loss: 0.005347777624\n",
      "Iteration 22390, Loss: 0.005089882761\n",
      "Iteration 22400, Loss: 0.006014349870\n",
      "Iteration 22410, Loss: 0.005100265611\n",
      "Iteration 22420, Loss: 0.005007191561\n",
      "Iteration 22430, Loss: 0.004997686949\n",
      "Iteration 22440, Loss: 0.005251401104\n",
      "Iteration 22450, Loss: 0.005447937176\n",
      "Iteration 22460, Loss: 0.006034133025\n",
      "Iteration 22470, Loss: 0.005142244045\n",
      "Iteration 22480, Loss: 0.005410802085\n",
      "Iteration 22490, Loss: 0.005356381647\n",
      "Iteration 22500, Loss: 0.005948196165\n",
      "Iteration 22510, Loss: 0.005065275822\n",
      "Iteration 22520, Loss: 0.005721461959\n",
      "Iteration 22530, Loss: 0.005011801608\n",
      "Iteration 22540, Loss: 0.005357644055\n",
      "Iteration 22550, Loss: 0.007090139203\n",
      "Iteration 22560, Loss: 0.005675708875\n",
      "Iteration 22570, Loss: 0.005204213783\n",
      "Iteration 22580, Loss: 0.005122089759\n",
      "Iteration 22590, Loss: 0.005511787254\n",
      "Iteration 22600, Loss: 0.006064129062\n",
      "Iteration 22610, Loss: 0.005793595687\n",
      "Iteration 22620, Loss: 0.005379182287\n",
      "Iteration 22630, Loss: 0.005012172274\n",
      "Iteration 22640, Loss: 0.005148263182\n",
      "Iteration 22650, Loss: 0.007557337172\n",
      "Iteration 22660, Loss: 0.006204670295\n",
      "Iteration 22670, Loss: 0.005434564315\n",
      "Iteration 22680, Loss: 0.005144849420\n",
      "Iteration 22690, Loss: 0.005148361437\n",
      "Iteration 22700, Loss: 0.006225494668\n",
      "Iteration 22710, Loss: 0.005009836052\n",
      "Iteration 22720, Loss: 0.005275448784\n",
      "Iteration 22730, Loss: 0.005336911418\n",
      "Iteration 22740, Loss: 0.005187744740\n",
      "Iteration 22750, Loss: 0.005275824107\n",
      "Iteration 22760, Loss: 0.007245163433\n",
      "Iteration 22770, Loss: 0.005783276632\n",
      "Iteration 22780, Loss: 0.005205687135\n",
      "Iteration 22790, Loss: 0.005043929443\n",
      "Iteration 22800, Loss: 0.005097983405\n",
      "Iteration 22810, Loss: 0.006292150356\n",
      "Iteration 22820, Loss: 0.005083998665\n",
      "Iteration 22830, Loss: 0.005105820484\n",
      "Iteration 22840, Loss: 0.005010484252\n",
      "Iteration 22850, Loss: 0.005207330454\n",
      "Iteration 22860, Loss: 0.006067353301\n",
      "Iteration 22870, Loss: 0.005000100471\n",
      "Iteration 22880, Loss: 0.005513091106\n",
      "Iteration 22890, Loss: 0.004985527601\n",
      "Iteration 22900, Loss: 0.005227167625\n",
      "Iteration 22910, Loss: 0.008347598836\n",
      "Iteration 22920, Loss: 0.005536063109\n",
      "Iteration 22930, Loss: 0.005069016013\n",
      "Iteration 22940, Loss: 0.005043064244\n",
      "Iteration 22950, Loss: 0.005244155880\n",
      "Iteration 22960, Loss: 0.005827327725\n",
      "Iteration 22970, Loss: 0.004978590179\n",
      "Iteration 22980, Loss: 0.005257225595\n",
      "Iteration 22990, Loss: 0.006203067489\n",
      "Iteration 23000, Loss: 0.005024163518\n",
      "Iteration 23010, Loss: 0.005631812848\n",
      "Iteration 23020, Loss: 0.005016154144\n",
      "Iteration 23030, Loss: 0.005640086718\n",
      "Iteration 23040, Loss: 0.006582668982\n",
      "Iteration 23050, Loss: 0.005985303316\n",
      "Iteration 23060, Loss: 0.005328755826\n",
      "Iteration 23070, Loss: 0.005008707289\n",
      "Iteration 23080, Loss: 0.005716020241\n",
      "Iteration 23090, Loss: 0.005343901925\n",
      "Iteration 23100, Loss: 0.005436216947\n",
      "Iteration 23110, Loss: 0.005360831041\n",
      "Iteration 23120, Loss: 0.004970288835\n",
      "Iteration 23130, Loss: 0.005478537176\n",
      "Iteration 23140, Loss: 0.006074564531\n",
      "Iteration 23150, Loss: 0.005486044101\n",
      "Iteration 23160, Loss: 0.005235370714\n",
      "Iteration 23170, Loss: 0.005099114031\n",
      "Iteration 23180, Loss: 0.005737166386\n",
      "Iteration 23190, Loss: 0.005379006267\n",
      "Iteration 23200, Loss: 0.005469517782\n",
      "Iteration 23210, Loss: 0.005015462171\n",
      "Iteration 23220, Loss: 0.005099753849\n",
      "Iteration 23230, Loss: 0.007271111012\n",
      "Iteration 23240, Loss: 0.006325867493\n",
      "Iteration 23250, Loss: 0.005525097251\n",
      "Iteration 23260, Loss: 0.005161937326\n",
      "Iteration 23270, Loss: 0.005146165844\n",
      "Iteration 23280, Loss: 0.005368241575\n",
      "Iteration 23290, Loss: 0.006097169127\n",
      "Iteration 23300, Loss: 0.005199427251\n",
      "Iteration 23310, Loss: 0.005184337497\n",
      "Iteration 23320, Loss: 0.005001236685\n",
      "Iteration 23330, Loss: 0.006359982304\n",
      "Iteration 23340, Loss: 0.005200097803\n",
      "Iteration 23350, Loss: 0.005329441745\n",
      "Iteration 23360, Loss: 0.005145772826\n",
      "Iteration 23370, Loss: 0.004964654800\n",
      "Iteration 23380, Loss: 0.005297532305\n",
      "Iteration 23390, Loss: 0.006836723536\n",
      "Iteration 23400, Loss: 0.005719263107\n",
      "Iteration 23410, Loss: 0.005171670113\n",
      "Iteration 23420, Loss: 0.005065673962\n",
      "Iteration 23430, Loss: 0.005020123906\n",
      "Iteration 23440, Loss: 0.006247587036\n",
      "Iteration 23450, Loss: 0.005219829734\n",
      "Iteration 23460, Loss: 0.005550632719\n",
      "Iteration 23470, Loss: 0.005306723528\n",
      "Iteration 23480, Loss: 0.005062455311\n",
      "Iteration 23490, Loss: 0.005304308608\n",
      "Iteration 23500, Loss: 0.005565098021\n",
      "Iteration 23510, Loss: 0.005074113142\n",
      "Iteration 23520, Loss: 0.005003548693\n",
      "Iteration 23530, Loss: 0.006161645055\n",
      "Iteration 23540, Loss: 0.005012017209\n",
      "Iteration 23550, Loss: 0.005120390095\n",
      "Iteration 23560, Loss: 0.005258052144\n",
      "Iteration 23570, Loss: 0.005121875554\n",
      "Iteration 23580, Loss: 0.005035135895\n",
      "Iteration 23590, Loss: 0.005580093712\n",
      "Iteration 23600, Loss: 0.005278089084\n",
      "Iteration 23610, Loss: 0.005479805171\n",
      "Iteration 23620, Loss: 0.005311457440\n",
      "Iteration 23630, Loss: 0.005125388503\n",
      "Iteration 23640, Loss: 0.004981954582\n",
      "Iteration 23650, Loss: 0.004965764470\n",
      "Iteration 23660, Loss: 0.005531146657\n",
      "Iteration 23670, Loss: 0.005184900016\n",
      "Iteration 23680, Loss: 0.005772145465\n",
      "Iteration 23690, Loss: 0.005297663622\n",
      "Iteration 23700, Loss: 0.005085484125\n",
      "Iteration 23710, Loss: 0.005097730551\n",
      "Iteration 23720, Loss: 0.005293973256\n",
      "Iteration 23730, Loss: 0.005847011693\n",
      "Iteration 23740, Loss: 0.004972314928\n",
      "Iteration 23750, Loss: 0.005395980086\n",
      "Iteration 23760, Loss: 0.005580279045\n",
      "Iteration 23770, Loss: 0.004981539212\n",
      "Iteration 23780, Loss: 0.005003748927\n",
      "Iteration 23790, Loss: 0.007151865400\n",
      "Iteration 23800, Loss: 0.006353186909\n",
      "Iteration 23810, Loss: 0.005518237595\n",
      "Iteration 23820, Loss: 0.005318559706\n",
      "Iteration 23830, Loss: 0.005236728117\n",
      "Iteration 23840, Loss: 0.005039342679\n",
      "Iteration 23850, Loss: 0.006062881090\n",
      "Iteration 23860, Loss: 0.005013877526\n",
      "Iteration 23870, Loss: 0.005030059256\n",
      "Iteration 23880, Loss: 0.005167921074\n",
      "Iteration 23890, Loss: 0.005001142621\n",
      "Iteration 23900, Loss: 0.005912492983\n",
      "Iteration 23910, Loss: 0.005129641388\n",
      "Iteration 23920, Loss: 0.005272150971\n",
      "Iteration 23930, Loss: 0.005273220129\n",
      "Iteration 23940, Loss: 0.004953705706\n",
      "Iteration 23950, Loss: 0.005225348752\n",
      "Iteration 23960, Loss: 0.007764966693\n",
      "Iteration 23970, Loss: 0.005788811482\n",
      "Iteration 23980, Loss: 0.005060983822\n",
      "Iteration 23990, Loss: 0.005060759839\n",
      "Iteration 24000, Loss: 0.005055000074\n",
      "Iteration 24010, Loss: 0.005892046727\n",
      "Iteration 24020, Loss: 0.005038985983\n",
      "Iteration 24030, Loss: 0.005291750655\n",
      "Iteration 24040, Loss: 0.005309720989\n",
      "Iteration 24050, Loss: 0.004954729229\n",
      "Iteration 24060, Loss: 0.005331778899\n",
      "Iteration 24070, Loss: 0.007319799159\n",
      "Iteration 24080, Loss: 0.005425227806\n",
      "Iteration 24090, Loss: 0.005039163399\n",
      "Iteration 24100, Loss: 0.005072887987\n",
      "Iteration 24110, Loss: 0.004986634944\n",
      "Iteration 24120, Loss: 0.005066755693\n",
      "Iteration 24130, Loss: 0.005868577398\n",
      "Iteration 24140, Loss: 0.005491973832\n",
      "Iteration 24150, Loss: 0.006021080539\n",
      "Iteration 24160, Loss: 0.005014928523\n",
      "Iteration 24170, Loss: 0.005123470910\n",
      "Iteration 24180, Loss: 0.004962520674\n",
      "Iteration 24190, Loss: 0.005092167296\n",
      "Iteration 24200, Loss: 0.005545803346\n",
      "Iteration 24210, Loss: 0.005252162460\n",
      "Iteration 24220, Loss: 0.005194000434\n",
      "Iteration 24230, Loss: 0.005273146555\n",
      "Iteration 24240, Loss: 0.006672308780\n",
      "Iteration 24250, Loss: 0.005198556464\n",
      "Iteration 24260, Loss: 0.005057860166\n",
      "Iteration 24270, Loss: 0.005354819354\n",
      "Iteration 24280, Loss: 0.005656186957\n",
      "Iteration 24290, Loss: 0.005139807705\n",
      "Iteration 24300, Loss: 0.004955663346\n",
      "Iteration 24310, Loss: 0.005053312983\n",
      "Iteration 24320, Loss: 0.010229315609\n",
      "Iteration 24330, Loss: 0.005093845539\n",
      "Iteration 24340, Loss: 0.005718647502\n",
      "Iteration 24350, Loss: 0.005016318988\n",
      "Iteration 24360, Loss: 0.004975531716\n",
      "Iteration 24370, Loss: 0.005053070374\n",
      "Iteration 24380, Loss: 0.005057753995\n",
      "Iteration 24390, Loss: 0.005573763512\n",
      "Iteration 24400, Loss: 0.005386603996\n",
      "Iteration 24410, Loss: 0.005442629103\n",
      "Iteration 24420, Loss: 0.005302860867\n",
      "Iteration 24430, Loss: 0.004980339203\n",
      "Iteration 24440, Loss: 0.005588517990\n",
      "Iteration 24450, Loss: 0.005943252705\n",
      "Iteration 24460, Loss: 0.005429008976\n",
      "Iteration 24470, Loss: 0.005200827494\n",
      "Iteration 24480, Loss: 0.004951972049\n",
      "Iteration 24490, Loss: 0.005123899784\n",
      "Iteration 24500, Loss: 0.008203829639\n",
      "Iteration 24510, Loss: 0.006105940789\n",
      "Iteration 24520, Loss: 0.005080299452\n",
      "Iteration 24530, Loss: 0.005042773671\n",
      "Iteration 24540, Loss: 0.005338183139\n",
      "Iteration 24550, Loss: 0.005308046006\n",
      "Iteration 24560, Loss: 0.006678728387\n",
      "Iteration 24570, Loss: 0.005470326170\n",
      "Iteration 24580, Loss: 0.004990038928\n",
      "Iteration 24590, Loss: 0.005218223669\n",
      "Iteration 24600, Loss: 0.004950348753\n",
      "Iteration 24610, Loss: 0.004999489989\n",
      "Iteration 24620, Loss: 0.007635925896\n",
      "Iteration 24630, Loss: 0.007057841867\n",
      "Iteration 24640, Loss: 0.005075613037\n",
      "Iteration 24650, Loss: 0.005288511980\n",
      "Iteration 24660, Loss: 0.005148333963\n",
      "Iteration 24670, Loss: 0.004959744401\n",
      "Iteration 24680, Loss: 0.005039063282\n",
      "Iteration 24690, Loss: 0.006415203214\n",
      "Iteration 24700, Loss: 0.005082666408\n",
      "Iteration 24710, Loss: 0.004973154515\n",
      "Iteration 24720, Loss: 0.005110662896\n",
      "Iteration 24730, Loss: 0.004984728526\n",
      "Iteration 24740, Loss: 0.005546736997\n",
      "Iteration 24750, Loss: 0.005732093938\n",
      "Iteration 24760, Loss: 0.005064608995\n",
      "Iteration 24770, Loss: 0.005112965591\n",
      "Iteration 24780, Loss: 0.005096013658\n",
      "Iteration 24790, Loss: 0.004980245139\n",
      "Iteration 24800, Loss: 0.005350181833\n",
      "Iteration 24810, Loss: 0.006287679076\n",
      "Iteration 24820, Loss: 0.005075686146\n",
      "Iteration 24830, Loss: 0.005240752362\n",
      "Iteration 24840, Loss: 0.005052018911\n",
      "Iteration 24850, Loss: 0.004978334066\n",
      "Iteration 24860, Loss: 0.004960423335\n",
      "Iteration 24870, Loss: 0.005432683509\n",
      "Iteration 24880, Loss: 0.005743464921\n",
      "Iteration 24890, Loss: 0.005148770753\n",
      "Iteration 24900, Loss: 0.005470888689\n",
      "Iteration 24910, Loss: 0.005129239522\n",
      "Iteration 24920, Loss: 0.004944093060\n",
      "Iteration 24930, Loss: 0.005030584056\n",
      "Iteration 24940, Loss: 0.006702880841\n",
      "Iteration 24950, Loss: 0.005361345597\n",
      "Iteration 24960, Loss: 0.005229987204\n",
      "Iteration 24970, Loss: 0.005049750209\n",
      "Iteration 24980, Loss: 0.005005236249\n",
      "Iteration 24990, Loss: 0.004950715229\n",
      "Iteration 25000, Loss: 0.005008034408\n",
      "Iteration 25010, Loss: 0.008548473939\n",
      "Iteration 25020, Loss: 0.007014004048\n",
      "Iteration 25030, Loss: 0.005257652141\n",
      "Iteration 25040, Loss: 0.005190224852\n",
      "Iteration 25050, Loss: 0.005065908656\n",
      "Iteration 25060, Loss: 0.005041411147\n",
      "Iteration 25070, Loss: 0.005184725858\n",
      "Iteration 25080, Loss: 0.006803845055\n",
      "Iteration 25090, Loss: 0.005540771410\n",
      "Iteration 25100, Loss: 0.005004881416\n",
      "Iteration 25110, Loss: 0.005119571928\n",
      "Iteration 25120, Loss: 0.004935515579\n",
      "Iteration 25130, Loss: 0.004996862262\n",
      "Iteration 25140, Loss: 0.009328720160\n",
      "Iteration 25150, Loss: 0.006112232804\n",
      "Iteration 25160, Loss: 0.005388849415\n",
      "Iteration 25170, Loss: 0.005228976719\n",
      "Iteration 25180, Loss: 0.005075496156\n",
      "Iteration 25190, Loss: 0.004974001087\n",
      "Iteration 25200, Loss: 0.005113044288\n",
      "Iteration 25210, Loss: 0.006210215855\n",
      "Iteration 25220, Loss: 0.005015226547\n",
      "Iteration 25230, Loss: 0.005112913903\n",
      "Iteration 25240, Loss: 0.005151392426\n",
      "Iteration 25250, Loss: 0.005528879818\n",
      "Iteration 25260, Loss: 0.005474447738\n",
      "Iteration 25270, Loss: 0.004961573984\n",
      "Iteration 25280, Loss: 0.005592516158\n",
      "Iteration 25290, Loss: 0.005833039992\n",
      "Iteration 25300, Loss: 0.005687197670\n",
      "Iteration 25310, Loss: 0.005067282822\n",
      "Iteration 25320, Loss: 0.005461916327\n",
      "Iteration 25330, Loss: 0.005111901090\n",
      "Iteration 25340, Loss: 0.005334650632\n",
      "Iteration 25350, Loss: 0.007094554603\n",
      "Iteration 25360, Loss: 0.005946117453\n",
      "Iteration 25370, Loss: 0.005309134722\n",
      "Iteration 25380, Loss: 0.004981576931\n",
      "Iteration 25390, Loss: 0.005247624591\n",
      "Iteration 25400, Loss: 0.006975667085\n",
      "Iteration 25410, Loss: 0.005895968992\n",
      "Iteration 25420, Loss: 0.005248490721\n",
      "Iteration 25430, Loss: 0.004964458290\n",
      "Iteration 25440, Loss: 0.005275553558\n",
      "Iteration 25450, Loss: 0.006901142187\n",
      "Iteration 25460, Loss: 0.005656431429\n",
      "Iteration 25470, Loss: 0.005130506121\n",
      "Iteration 25480, Loss: 0.005056372378\n",
      "Iteration 25490, Loss: 0.004965302534\n",
      "Iteration 25500, Loss: 0.005028884858\n",
      "Iteration 25510, Loss: 0.007962223142\n",
      "Iteration 25520, Loss: 0.007528791204\n",
      "Iteration 25530, Loss: 0.005132222082\n",
      "Iteration 25540, Loss: 0.005320152268\n",
      "Iteration 25550, Loss: 0.005187483970\n",
      "Iteration 25560, Loss: 0.005002656020\n",
      "Iteration 25570, Loss: 0.005098335911\n",
      "Iteration 25580, Loss: 0.006006472744\n",
      "Iteration 25590, Loss: 0.004946123809\n",
      "Iteration 25600, Loss: 0.005171612371\n",
      "Iteration 25610, Loss: 0.005087883677\n",
      "Iteration 25620, Loss: 0.005476440769\n",
      "Iteration 25630, Loss: 0.005787330680\n",
      "Iteration 25640, Loss: 0.005111392587\n",
      "Iteration 25650, Loss: 0.005392447580\n",
      "Iteration 25660, Loss: 0.004983393010\n",
      "Iteration 25670, Loss: 0.005082700402\n",
      "Iteration 25680, Loss: 0.009354205802\n",
      "Iteration 25690, Loss: 0.006078919396\n",
      "Iteration 25700, Loss: 0.004992625676\n",
      "Iteration 25710, Loss: 0.005001299083\n",
      "Iteration 25720, Loss: 0.004969428759\n",
      "Iteration 25730, Loss: 0.005035630427\n",
      "Iteration 25740, Loss: 0.006097807549\n",
      "Iteration 25750, Loss: 0.005011351779\n",
      "Iteration 25760, Loss: 0.005021847319\n",
      "Iteration 25770, Loss: 0.005050879903\n",
      "Iteration 25780, Loss: 0.005040551070\n",
      "Iteration 25790, Loss: 0.005950042978\n",
      "Iteration 25800, Loss: 0.005057832692\n",
      "Iteration 25810, Loss: 0.005084787961\n",
      "Iteration 25820, Loss: 0.005092142615\n",
      "Iteration 25830, Loss: 0.005081827752\n",
      "Iteration 25840, Loss: 0.005510040093\n",
      "Iteration 25850, Loss: 0.005768343806\n",
      "Iteration 25860, Loss: 0.005254025105\n",
      "Iteration 25870, Loss: 0.005060553085\n",
      "Iteration 25880, Loss: 0.004988769069\n",
      "Iteration 25890, Loss: 0.005984638352\n",
      "Iteration 25900, Loss: 0.005080961622\n",
      "Iteration 25910, Loss: 0.005602640565\n",
      "Iteration 25920, Loss: 0.005288455170\n",
      "Iteration 25930, Loss: 0.005103875883\n",
      "Iteration 25940, Loss: 0.004982720595\n",
      "Iteration 25950, Loss: 0.005137925036\n",
      "Iteration 25960, Loss: 0.004971882328\n",
      "Iteration 25970, Loss: 0.005416431464\n",
      "Iteration 25980, Loss: 0.005433627870\n",
      "Iteration 25990, Loss: 0.006288508885\n",
      "Iteration 26000, Loss: 0.005083305296\n",
      "Iteration 26010, Loss: 0.005017673597\n",
      "Iteration 26020, Loss: 0.004950858653\n",
      "Iteration 26030, Loss: 0.005028263666\n",
      "Iteration 26040, Loss: 0.005566227715\n",
      "Iteration 26050, Loss: 0.005257964134\n",
      "Iteration 26060, Loss: 0.005347510800\n",
      "Iteration 26070, Loss: 0.004944811575\n",
      "Iteration 26080, Loss: 0.005164447706\n",
      "Iteration 26090, Loss: 0.007167421281\n",
      "Iteration 26100, Loss: 0.005926529411\n",
      "Iteration 26110, Loss: 0.005244956817\n",
      "Iteration 26120, Loss: 0.004948809277\n",
      "Iteration 26130, Loss: 0.005248866975\n",
      "Iteration 26140, Loss: 0.006163032725\n",
      "Iteration 26150, Loss: 0.004978378769\n",
      "Iteration 26160, Loss: 0.005304363091\n",
      "Iteration 26170, Loss: 0.004979947582\n",
      "Iteration 26180, Loss: 0.005118696950\n",
      "Iteration 26190, Loss: 0.008722374216\n",
      "Iteration 26200, Loss: 0.006315861829\n",
      "Iteration 26210, Loss: 0.004963065963\n",
      "Iteration 26220, Loss: 0.005006080028\n",
      "Iteration 26230, Loss: 0.004929500166\n",
      "Iteration 26240, Loss: 0.005116754211\n",
      "Iteration 26250, Loss: 0.006156727206\n",
      "Iteration 26260, Loss: 0.005026056431\n",
      "Iteration 26270, Loss: 0.005056161433\n",
      "Iteration 26280, Loss: 0.005073025357\n",
      "Iteration 26290, Loss: 0.005612091161\n",
      "Iteration 26300, Loss: 0.005728167947\n",
      "Iteration 26310, Loss: 0.005575871561\n",
      "Iteration 26320, Loss: 0.004972619470\n",
      "Iteration 26330, Loss: 0.005496219266\n",
      "Iteration 26340, Loss: 0.005406715907\n",
      "Iteration 26350, Loss: 0.005040736403\n",
      "Iteration 26360, Loss: 0.005191801582\n",
      "Iteration 26370, Loss: 0.007785931230\n",
      "Iteration 26380, Loss: 0.006064638030\n",
      "Iteration 26390, Loss: 0.005443696864\n",
      "Iteration 26400, Loss: 0.005132776219\n",
      "Iteration 26410, Loss: 0.005038226955\n",
      "Iteration 26420, Loss: 0.006037374027\n",
      "Iteration 26430, Loss: 0.005310482811\n",
      "Iteration 26440, Loss: 0.005409988109\n",
      "Iteration 26450, Loss: 0.004989729729\n",
      "Iteration 26460, Loss: 0.005477705970\n",
      "Iteration 26470, Loss: 0.006191887427\n",
      "Iteration 26480, Loss: 0.005764003843\n",
      "Iteration 26490, Loss: 0.005284545012\n",
      "Iteration 26500, Loss: 0.004970701877\n",
      "Iteration 26510, Loss: 0.005535474047\n",
      "Iteration 26520, Loss: 0.005760769360\n",
      "Iteration 26530, Loss: 0.005448264070\n",
      "Iteration 26540, Loss: 0.004948037211\n",
      "Iteration 26550, Loss: 0.005506467074\n",
      "Iteration 26560, Loss: 0.006067826413\n",
      "Iteration 26570, Loss: 0.004995707422\n",
      "Iteration 26580, Loss: 0.005464696325\n",
      "Iteration 26590, Loss: 0.004986679181\n",
      "Iteration 26600, Loss: 0.004989085719\n",
      "Iteration 26610, Loss: 0.007890762761\n",
      "Iteration 26620, Loss: 0.006965689361\n",
      "Iteration 26630, Loss: 0.005159691907\n",
      "Iteration 26640, Loss: 0.005063941237\n",
      "Iteration 26650, Loss: 0.004945599474\n",
      "Iteration 26660, Loss: 0.005042789970\n",
      "Iteration 26670, Loss: 0.006085446104\n",
      "Iteration 26680, Loss: 0.004941504449\n",
      "Iteration 26690, Loss: 0.005024181679\n",
      "Iteration 26700, Loss: 0.005087879952\n",
      "Iteration 26710, Loss: 0.004989391658\n",
      "Iteration 26720, Loss: 0.005818244535\n",
      "Iteration 26730, Loss: 0.005143261515\n",
      "Iteration 26740, Loss: 0.005233749747\n",
      "Iteration 26750, Loss: 0.005212064832\n",
      "Iteration 26760, Loss: 0.004943762440\n",
      "Iteration 26770, Loss: 0.005686726421\n",
      "Iteration 26780, Loss: 0.005972628482\n",
      "Iteration 26790, Loss: 0.005428179633\n",
      "Iteration 26800, Loss: 0.005200584419\n",
      "Iteration 26810, Loss: 0.004952319432\n",
      "Iteration 26820, Loss: 0.005520860199\n",
      "Iteration 26830, Loss: 0.005804420449\n",
      "Iteration 26840, Loss: 0.005571744405\n",
      "Iteration 26850, Loss: 0.005207386799\n",
      "Iteration 26860, Loss: 0.004936216865\n",
      "Iteration 26870, Loss: 0.005090156570\n",
      "Iteration 26880, Loss: 0.008215310052\n",
      "Iteration 26890, Loss: 0.006347135175\n",
      "Iteration 26900, Loss: 0.005276117940\n",
      "Iteration 26910, Loss: 0.005038155243\n",
      "Iteration 26920, Loss: 0.005052966997\n",
      "Iteration 26930, Loss: 0.005203129724\n",
      "Iteration 26940, Loss: 0.006238253787\n",
      "Iteration 26950, Loss: 0.005023393314\n",
      "Iteration 26960, Loss: 0.005080039147\n",
      "Iteration 26970, Loss: 0.005090512335\n",
      "Iteration 26980, Loss: 0.006201641634\n",
      "Iteration 26990, Loss: 0.004919063766\n",
      "Iteration 27000, Loss: 0.005350108258\n",
      "Iteration 27010, Loss: 0.005463560112\n",
      "Iteration 27020, Loss: 0.005548923742\n",
      "Iteration 27030, Loss: 0.005241416860\n",
      "Iteration 27040, Loss: 0.005146491341\n",
      "Iteration 27050, Loss: 0.005700051319\n",
      "Iteration 27060, Loss: 0.005444732960\n",
      "Iteration 27070, Loss: 0.005522912368\n",
      "Iteration 27080, Loss: 0.004913844168\n",
      "Iteration 27090, Loss: 0.005500435364\n",
      "Iteration 27100, Loss: 0.006121592596\n",
      "Iteration 27110, Loss: 0.005711378530\n",
      "Iteration 27120, Loss: 0.005148884375\n",
      "Iteration 27130, Loss: 0.005084723700\n",
      "Iteration 27140, Loss: 0.005054528825\n",
      "Iteration 27150, Loss: 0.005736777559\n",
      "Iteration 27160, Loss: 0.005222027656\n",
      "Iteration 27170, Loss: 0.005133840255\n",
      "Iteration 27180, Loss: 0.005072940141\n",
      "Iteration 27190, Loss: 0.004958059639\n",
      "Iteration 27200, Loss: 0.005297487602\n",
      "Iteration 27210, Loss: 0.006107934751\n",
      "Iteration 27220, Loss: 0.005082418211\n",
      "Iteration 27230, Loss: 0.005110526457\n",
      "Iteration 27240, Loss: 0.004933109507\n",
      "Iteration 27250, Loss: 0.005572844297\n",
      "Iteration 27260, Loss: 0.005397757981\n",
      "Iteration 27270, Loss: 0.005078519229\n",
      "Iteration 27280, Loss: 0.005192650016\n",
      "Iteration 27290, Loss: 0.005036142189\n",
      "Iteration 27300, Loss: 0.004997621290\n",
      "Iteration 27310, Loss: 0.005033836234\n",
      "Iteration 27320, Loss: 0.007103726268\n",
      "Iteration 27330, Loss: 0.005465028808\n",
      "Iteration 27340, Loss: 0.005037801806\n",
      "Iteration 27350, Loss: 0.004915811587\n",
      "Iteration 27360, Loss: 0.005143512972\n",
      "Iteration 27370, Loss: 0.006120148115\n",
      "Iteration 27380, Loss: 0.004914583173\n",
      "Iteration 27390, Loss: 0.005273623392\n",
      "Iteration 27400, Loss: 0.004926117603\n",
      "Iteration 27410, Loss: 0.005602618214\n",
      "Iteration 27420, Loss: 0.005469754338\n",
      "Iteration 27430, Loss: 0.005149214063\n",
      "Iteration 27440, Loss: 0.004990806803\n",
      "Iteration 27450, Loss: 0.005127016921\n",
      "Iteration 27460, Loss: 0.004963541403\n",
      "Iteration 27470, Loss: 0.005446051713\n",
      "Iteration 27480, Loss: 0.006103183609\n",
      "Iteration 27490, Loss: 0.005501007196\n",
      "Iteration 27500, Loss: 0.005138007458\n",
      "Iteration 27510, Loss: 0.005105513148\n",
      "Iteration 27520, Loss: 0.005331740715\n",
      "Iteration 27530, Loss: 0.005993296858\n",
      "Iteration 27540, Loss: 0.005011202767\n",
      "Iteration 27550, Loss: 0.005188707262\n",
      "Iteration 27560, Loss: 0.004906973336\n",
      "Iteration 27570, Loss: 0.004911191761\n",
      "Iteration 27580, Loss: 0.006302255671\n",
      "Iteration 27590, Loss: 0.007360492367\n",
      "Iteration 27600, Loss: 0.005104104988\n",
      "Iteration 27610, Loss: 0.005538836122\n",
      "Iteration 27620, Loss: 0.005082145799\n",
      "Iteration 27630, Loss: 0.004962436855\n",
      "Iteration 27640, Loss: 0.004957460798\n",
      "Iteration 27650, Loss: 0.005067791324\n",
      "Iteration 27660, Loss: 0.006039224565\n",
      "Iteration 27670, Loss: 0.004921794869\n",
      "Iteration 27680, Loss: 0.005081707146\n",
      "Iteration 27690, Loss: 0.005039436743\n",
      "Iteration 27700, Loss: 0.005567925051\n",
      "Iteration 27710, Loss: 0.005517641082\n",
      "Iteration 27720, Loss: 0.005209114868\n",
      "Iteration 27730, Loss: 0.005111368839\n",
      "Iteration 27740, Loss: 0.004905196372\n",
      "Iteration 27750, Loss: 0.004904279485\n",
      "Iteration 27760, Loss: 0.005755901337\n",
      "Iteration 27770, Loss: 0.006232396699\n",
      "Iteration 27780, Loss: 0.005150158424\n",
      "Iteration 27790, Loss: 0.005179893691\n",
      "Iteration 27800, Loss: 0.004963985179\n",
      "Iteration 27810, Loss: 0.004979732912\n",
      "Iteration 27820, Loss: 0.004912207369\n",
      "Iteration 27830, Loss: 0.004925722722\n",
      "Iteration 27840, Loss: 0.004980892409\n",
      "Iteration 27850, Loss: 0.006243303884\n",
      "Iteration 27860, Loss: 0.005121257156\n",
      "Iteration 27870, Loss: 0.004985477775\n",
      "Iteration 27880, Loss: 0.004932671320\n",
      "Iteration 27890, Loss: 0.005073559470\n",
      "Iteration 27900, Loss: 0.005451537669\n",
      "Iteration 27910, Loss: 0.005834285170\n",
      "Iteration 27920, Loss: 0.005163316615\n",
      "Iteration 27930, Loss: 0.005048601422\n",
      "Iteration 27940, Loss: 0.004916971549\n",
      "Iteration 27950, Loss: 0.005802652799\n",
      "Iteration 27960, Loss: 0.004982620012\n",
      "Iteration 27970, Loss: 0.005225047935\n",
      "Iteration 27980, Loss: 0.005335753784\n",
      "Iteration 27990, Loss: 0.005032139830\n",
      "Iteration 28000, Loss: 0.004988756496\n",
      "Iteration 28010, Loss: 0.004990691785\n",
      "Iteration 28020, Loss: 0.006673113909\n",
      "Iteration 28030, Loss: 0.005306110252\n",
      "Iteration 28040, Loss: 0.005243276246\n",
      "Iteration 28050, Loss: 0.005041073542\n",
      "Iteration 28060, Loss: 0.004911214579\n",
      "Iteration 28070, Loss: 0.005050936714\n",
      "Iteration 28080, Loss: 0.006975876167\n",
      "Iteration 28090, Loss: 0.005425712094\n",
      "Iteration 28100, Loss: 0.005061093718\n",
      "Iteration 28110, Loss: 0.004909305368\n",
      "Iteration 28120, Loss: 0.005164148286\n",
      "Iteration 28130, Loss: 0.005663824268\n",
      "Iteration 28140, Loss: 0.005204822868\n",
      "Iteration 28150, Loss: 0.005055369344\n",
      "Iteration 28160, Loss: 0.005488445051\n",
      "Iteration 28170, Loss: 0.005681982264\n",
      "Iteration 28180, Loss: 0.004915964790\n",
      "Iteration 28190, Loss: 0.005394421984\n",
      "Iteration 28200, Loss: 0.006067667622\n",
      "Iteration 28210, Loss: 0.005354794208\n",
      "Iteration 28220, Loss: 0.004987259395\n",
      "Iteration 28230, Loss: 0.005458291620\n",
      "Iteration 28240, Loss: 0.006321317982\n",
      "Iteration 28250, Loss: 0.005054939538\n",
      "Iteration 28260, Loss: 0.005262688734\n",
      "Iteration 28270, Loss: 0.005051021464\n",
      "Iteration 28280, Loss: 0.005888889544\n",
      "Iteration 28290, Loss: 0.005019435659\n",
      "Iteration 28300, Loss: 0.005379941314\n",
      "Iteration 28310, Loss: 0.005278283264\n",
      "Iteration 28320, Loss: 0.004939527716\n",
      "Iteration 28330, Loss: 0.004893933423\n",
      "Iteration 28340, Loss: 0.005080453586\n",
      "Iteration 28350, Loss: 0.009947720915\n",
      "Iteration 28360, Loss: 0.006050661206\n",
      "Iteration 28370, Loss: 0.005032397807\n",
      "Iteration 28380, Loss: 0.004994453397\n",
      "Iteration 28390, Loss: 0.005081094336\n",
      "Iteration 28400, Loss: 0.004901358858\n",
      "Iteration 28410, Loss: 0.004933084361\n",
      "Iteration 28420, Loss: 0.005359976552\n",
      "Iteration 28430, Loss: 0.005776203237\n",
      "Iteration 28440, Loss: 0.005175084341\n",
      "Iteration 28450, Loss: 0.004974937998\n",
      "Iteration 28460, Loss: 0.005028682761\n",
      "Iteration 28470, Loss: 0.004907440394\n",
      "Iteration 28480, Loss: 0.004901481792\n",
      "Iteration 28490, Loss: 0.005341308191\n",
      "Iteration 28500, Loss: 0.005663544871\n",
      "Iteration 28510, Loss: 0.006076126825\n",
      "Iteration 28520, Loss: 0.005083094817\n",
      "Iteration 28530, Loss: 0.004991431721\n",
      "Iteration 28540, Loss: 0.004913613200\n",
      "Iteration 28550, Loss: 0.004942200147\n",
      "Iteration 28560, Loss: 0.004933428485\n",
      "Iteration 28570, Loss: 0.005227424204\n",
      "Iteration 28580, Loss: 0.006768029649\n",
      "Iteration 28590, Loss: 0.005704227835\n",
      "Iteration 28600, Loss: 0.005212217569\n",
      "Iteration 28610, Loss: 0.005035888869\n",
      "Iteration 28620, Loss: 0.005039755255\n",
      "Iteration 28630, Loss: 0.005551867187\n",
      "Iteration 28640, Loss: 0.005700095091\n",
      "Iteration 28650, Loss: 0.005558953620\n",
      "Iteration 28660, Loss: 0.005055325571\n",
      "Iteration 28670, Loss: 0.005149000324\n",
      "Iteration 28680, Loss: 0.005109161139\n",
      "Iteration 28690, Loss: 0.006577466615\n",
      "Iteration 28700, Loss: 0.005078947637\n",
      "Iteration 28710, Loss: 0.005107719451\n",
      "Iteration 28720, Loss: 0.005201996770\n",
      "Iteration 28730, Loss: 0.004908950534\n",
      "Iteration 28740, Loss: 0.004927326459\n",
      "Iteration 28750, Loss: 0.005688622594\n",
      "Iteration 28760, Loss: 0.005141822621\n",
      "Iteration 28770, Loss: 0.006131643429\n",
      "Iteration 28780, Loss: 0.005028194282\n",
      "Iteration 28790, Loss: 0.005013777409\n",
      "Iteration 28800, Loss: 0.004893545527\n",
      "Iteration 28810, Loss: 0.004987295251\n",
      "Iteration 28820, Loss: 0.005243282765\n",
      "Iteration 28830, Loss: 0.005847346969\n",
      "Iteration 28840, Loss: 0.004992655478\n",
      "Iteration 28850, Loss: 0.005202384666\n",
      "Iteration 28860, Loss: 0.004942226689\n",
      "Iteration 28870, Loss: 0.005183626432\n",
      "Iteration 28880, Loss: 0.008035549894\n",
      "Iteration 28890, Loss: 0.005482712295\n",
      "Iteration 28900, Loss: 0.004936791956\n",
      "Iteration 28910, Loss: 0.004973839503\n",
      "Iteration 28920, Loss: 0.005086205900\n",
      "Iteration 28930, Loss: 0.005040883087\n",
      "Iteration 28940, Loss: 0.006157266907\n",
      "Iteration 28950, Loss: 0.004904739559\n",
      "Iteration 28960, Loss: 0.005049767438\n",
      "Iteration 28970, Loss: 0.005173590966\n",
      "Iteration 28980, Loss: 0.004896677565\n",
      "Iteration 28990, Loss: 0.004887935240\n",
      "Iteration 29000, Loss: 0.005265339278\n",
      "Iteration 29010, Loss: 0.006120760925\n",
      "Iteration 29020, Loss: 0.006253574044\n",
      "Iteration 29030, Loss: 0.005051469896\n",
      "Iteration 29040, Loss: 0.005153558683\n",
      "Iteration 29050, Loss: 0.004968995228\n",
      "Iteration 29060, Loss: 0.004913610406\n",
      "Iteration 29070, Loss: 0.004911245313\n",
      "Iteration 29080, Loss: 0.005120205693\n",
      "Iteration 29090, Loss: 0.007175581064\n",
      "Iteration 29100, Loss: 0.005692590028\n",
      "Iteration 29110, Loss: 0.005204669200\n",
      "Iteration 29120, Loss: 0.004934283905\n",
      "Iteration 29130, Loss: 0.005151600111\n",
      "Iteration 29140, Loss: 0.005836079363\n",
      "Iteration 29150, Loss: 0.004976231139\n",
      "Iteration 29160, Loss: 0.005310500041\n",
      "Iteration 29170, Loss: 0.005130170379\n",
      "Iteration 29180, Loss: 0.005086648278\n",
      "Iteration 29190, Loss: 0.006750063971\n",
      "Iteration 29200, Loss: 0.005118215457\n",
      "Iteration 29210, Loss: 0.004917527083\n",
      "Iteration 29220, Loss: 0.005161847919\n",
      "Iteration 29230, Loss: 0.004973100964\n",
      "Iteration 29240, Loss: 0.005728664808\n",
      "Iteration 29250, Loss: 0.005550724454\n",
      "Iteration 29260, Loss: 0.005101860967\n",
      "Iteration 29270, Loss: 0.004957751371\n",
      "Iteration 29280, Loss: 0.005064592231\n",
      "Iteration 29290, Loss: 0.005084678996\n",
      "Iteration 29300, Loss: 0.006363499444\n",
      "Iteration 29310, Loss: 0.004927785136\n",
      "Iteration 29320, Loss: 0.004980609287\n",
      "Iteration 29330, Loss: 0.005153539125\n",
      "Iteration 29340, Loss: 0.004891669378\n",
      "Iteration 29350, Loss: 0.005046444945\n",
      "Iteration 29360, Loss: 0.008734899573\n",
      "Iteration 29370, Loss: 0.006023624446\n",
      "Iteration 29380, Loss: 0.005040513352\n",
      "Iteration 29390, Loss: 0.005003882106\n",
      "Iteration 29400, Loss: 0.004897890612\n",
      "Iteration 29410, Loss: 0.004954475444\n",
      "Iteration 29420, Loss: 0.005722762085\n",
      "Iteration 29430, Loss: 0.005186047871\n",
      "Iteration 29440, Loss: 0.005387796555\n",
      "Iteration 29450, Loss: 0.005097595975\n",
      "Iteration 29460, Loss: 0.005031854380\n",
      "Iteration 29470, Loss: 0.005108451936\n",
      "Iteration 29480, Loss: 0.007431597449\n",
      "Iteration 29490, Loss: 0.005765860900\n",
      "Iteration 29500, Loss: 0.005177604500\n",
      "Iteration 29510, Loss: 0.004915318452\n",
      "Iteration 29520, Loss: 0.005233923439\n",
      "Iteration 29530, Loss: 0.005590450950\n",
      "Iteration 29540, Loss: 0.005251028109\n",
      "Iteration 29550, Loss: 0.004993401002\n",
      "Iteration 29560, Loss: 0.005507909227\n",
      "Iteration 29570, Loss: 0.005740927532\n",
      "Iteration 29580, Loss: 0.005034327973\n",
      "Iteration 29590, Loss: 0.005351253785\n",
      "Iteration 29600, Loss: 0.005065162666\n",
      "Iteration 29610, Loss: 0.005418536253\n",
      "Iteration 29620, Loss: 0.006109796464\n",
      "Iteration 29630, Loss: 0.005658676848\n",
      "Iteration 29640, Loss: 0.005047160201\n",
      "Iteration 29650, Loss: 0.005102371797\n",
      "Iteration 29660, Loss: 0.005310740788\n",
      "Iteration 29670, Loss: 0.006283111870\n",
      "Iteration 29680, Loss: 0.004977921490\n",
      "Iteration 29690, Loss: 0.005078787450\n",
      "Iteration 29700, Loss: 0.005145649426\n",
      "Iteration 29710, Loss: 0.005698696710\n",
      "Iteration 29720, Loss: 0.005067490973\n",
      "Iteration 29730, Loss: 0.005029191263\n",
      "Iteration 29740, Loss: 0.005956347100\n",
      "Iteration 29750, Loss: 0.005051750690\n",
      "Iteration 29760, Loss: 0.005483085755\n",
      "Iteration 29770, Loss: 0.004895430058\n",
      "Iteration 29780, Loss: 0.005210923962\n",
      "Iteration 29790, Loss: 0.007592672482\n",
      "Iteration 29800, Loss: 0.005847238936\n",
      "Iteration 29810, Loss: 0.005098390393\n",
      "Iteration 29820, Loss: 0.005028439220\n",
      "Iteration 29830, Loss: 0.004919691943\n",
      "Iteration 29840, Loss: 0.004907186609\n",
      "Iteration 29850, Loss: 0.005828993395\n",
      "Iteration 29860, Loss: 0.005039479118\n",
      "Iteration 29870, Loss: 0.005841516424\n",
      "Iteration 29880, Loss: 0.005165106151\n",
      "Iteration 29890, Loss: 0.004987893626\n",
      "Iteration 29900, Loss: 0.004960093647\n",
      "Iteration 29910, Loss: 0.004987880588\n",
      "Iteration 29920, Loss: 0.005471638404\n",
      "Iteration 29930, Loss: 0.005463708192\n",
      "Iteration 29940, Loss: 0.005344032776\n",
      "Iteration 29950, Loss: 0.004886843730\n",
      "Iteration 29960, Loss: 0.005262601189\n",
      "Iteration 29970, Loss: 0.006216585170\n",
      "Iteration 29980, Loss: 0.005189115647\n",
      "Iteration 29990, Loss: 0.005053583998\n",
      "Iteration 30000, Loss: 0.005008782260\n",
      "Iteration 30010, Loss: 0.006422657985\n",
      "Iteration 30020, Loss: 0.004993829411\n",
      "Iteration 30030, Loss: 0.004948892165\n",
      "Iteration 30040, Loss: 0.005097203422\n",
      "Iteration 30050, Loss: 0.004890768789\n",
      "Iteration 30060, Loss: 0.004992226139\n",
      "Iteration 30070, Loss: 0.009349196218\n",
      "Iteration 30080, Loss: 0.006156604737\n",
      "Iteration 30090, Loss: 0.005069518462\n",
      "Iteration 30100, Loss: 0.005012684036\n",
      "Iteration 30110, Loss: 0.004905624315\n",
      "Iteration 30120, Loss: 0.004921158776\n",
      "Iteration 30130, Loss: 0.005233539268\n",
      "Iteration 30140, Loss: 0.006227550097\n",
      "Iteration 30150, Loss: 0.005470606498\n",
      "Iteration 30160, Loss: 0.004955940880\n",
      "Iteration 30170, Loss: 0.005133166444\n",
      "Iteration 30180, Loss: 0.005035520066\n",
      "Iteration 30190, Loss: 0.006394205615\n",
      "Iteration 30200, Loss: 0.005004838109\n",
      "Iteration 30210, Loss: 0.004904787987\n",
      "Iteration 30220, Loss: 0.004952125251\n",
      "Iteration 30230, Loss: 0.005043907557\n",
      "Iteration 30240, Loss: 0.005871973932\n",
      "Iteration 30250, Loss: 0.005055198912\n",
      "Iteration 30260, Loss: 0.005324876867\n",
      "Iteration 30270, Loss: 0.004977454897\n",
      "Iteration 30280, Loss: 0.005086429417\n",
      "Iteration 30290, Loss: 0.008286816999\n",
      "Iteration 30300, Loss: 0.006296924781\n",
      "Iteration 30310, Loss: 0.005396939348\n",
      "Iteration 30320, Loss: 0.005111309234\n",
      "Iteration 30330, Loss: 0.004961163271\n",
      "Iteration 30340, Loss: 0.005248466507\n",
      "Iteration 30350, Loss: 0.006039190106\n",
      "Iteration 30360, Loss: 0.004956844728\n",
      "Iteration 30370, Loss: 0.005142550450\n",
      "Iteration 30380, Loss: 0.004894461017\n",
      "Iteration 30390, Loss: 0.004871632438\n",
      "Iteration 30400, Loss: 0.005518282298\n",
      "Iteration 30410, Loss: 0.005011736415\n",
      "Iteration 30420, Loss: 0.005895744078\n",
      "Iteration 30430, Loss: 0.005448409356\n",
      "Iteration 30440, Loss: 0.005077600013\n",
      "Iteration 30450, Loss: 0.004904639907\n",
      "Iteration 30460, Loss: 0.004897258710\n",
      "Iteration 30470, Loss: 0.004899201449\n",
      "Iteration 30480, Loss: 0.005004273262\n",
      "Iteration 30490, Loss: 0.006478711031\n",
      "Iteration 30500, Loss: 0.005123996176\n",
      "Iteration 30510, Loss: 0.004919323139\n",
      "Iteration 30520, Loss: 0.004969287664\n",
      "Iteration 30530, Loss: 0.004944374319\n",
      "Iteration 30540, Loss: 0.005594086833\n",
      "Iteration 30550, Loss: 0.005559564568\n",
      "Iteration 30560, Loss: 0.005473420955\n",
      "Iteration 30570, Loss: 0.005171660334\n",
      "Iteration 30580, Loss: 0.004930797964\n",
      "Iteration 30590, Loss: 0.004977150355\n",
      "Iteration 30600, Loss: 0.007097560912\n",
      "Iteration 30610, Loss: 0.005907360464\n",
      "Iteration 30620, Loss: 0.005391021259\n",
      "Iteration 30630, Loss: 0.005182956345\n",
      "Iteration 30640, Loss: 0.004916577134\n",
      "Iteration 30650, Loss: 0.005083058961\n",
      "Iteration 30660, Loss: 0.005687169265\n",
      "Iteration 30670, Loss: 0.005022618920\n",
      "Iteration 30680, Loss: 0.005158996210\n",
      "Iteration 30690, Loss: 0.005239487626\n",
      "Iteration 30700, Loss: 0.005276461132\n",
      "Iteration 30710, Loss: 0.005805185996\n",
      "Iteration 30720, Loss: 0.004903791938\n",
      "Iteration 30730, Loss: 0.005409379955\n",
      "Iteration 30740, Loss: 0.005927623250\n",
      "Iteration 30750, Loss: 0.004890640732\n",
      "Iteration 30760, Loss: 0.005164352711\n",
      "Iteration 30770, Loss: 0.005447843112\n",
      "Iteration 30780, Loss: 0.005510922521\n",
      "Iteration 30790, Loss: 0.004891575780\n",
      "Iteration 30800, Loss: 0.004899576772\n",
      "Iteration 30810, Loss: 0.006591900252\n",
      "Iteration 30820, Loss: 0.005747280084\n",
      "Iteration 30830, Loss: 0.005736793391\n",
      "Iteration 30840, Loss: 0.005045488477\n",
      "Iteration 30850, Loss: 0.005010405090\n",
      "Iteration 30860, Loss: 0.004920892417\n",
      "Iteration 30870, Loss: 0.004994924180\n",
      "Iteration 30880, Loss: 0.006116920151\n",
      "Iteration 30890, Loss: 0.004869160708\n",
      "Iteration 30900, Loss: 0.005051039625\n",
      "Iteration 30910, Loss: 0.005127256736\n",
      "Iteration 30920, Loss: 0.004971419461\n",
      "Iteration 30930, Loss: 0.005249435082\n",
      "Iteration 30940, Loss: 0.007070718799\n",
      "Iteration 30950, Loss: 0.005864743143\n",
      "Iteration 30960, Loss: 0.005296775140\n",
      "Iteration 30970, Loss: 0.004951075185\n",
      "Iteration 30980, Loss: 0.005228431895\n",
      "Iteration 30990, Loss: 0.005538860802\n",
      "Iteration 31000, Loss: 0.004997304175\n",
      "Iteration 31010, Loss: 0.004935650621\n",
      "Iteration 31020, Loss: 0.005500318948\n",
      "Iteration 31030, Loss: 0.006109643728\n",
      "Iteration 31040, Loss: 0.005588320084\n",
      "Iteration 31050, Loss: 0.005095780361\n",
      "Iteration 31060, Loss: 0.004975799005\n",
      "Iteration 31070, Loss: 0.004881216213\n",
      "Iteration 31080, Loss: 0.004949395545\n",
      "Iteration 31090, Loss: 0.007584871724\n",
      "Iteration 31100, Loss: 0.007593418937\n",
      "Iteration 31110, Loss: 0.005053233821\n",
      "Iteration 31120, Loss: 0.005117562599\n",
      "Iteration 31130, Loss: 0.005012240261\n",
      "Iteration 31140, Loss: 0.004892731085\n",
      "Iteration 31150, Loss: 0.004927559290\n",
      "Iteration 31160, Loss: 0.004977516830\n",
      "Iteration 31170, Loss: 0.005749001633\n",
      "Iteration 31180, Loss: 0.005001253914\n",
      "Iteration 31190, Loss: 0.005331471562\n",
      "Iteration 31200, Loss: 0.005010835826\n",
      "Iteration 31210, Loss: 0.005341460928\n",
      "Iteration 31220, Loss: 0.005522720050\n",
      "Iteration 31230, Loss: 0.005338311661\n",
      "Iteration 31240, Loss: 0.004884600639\n",
      "Iteration 31250, Loss: 0.005415164866\n",
      "Iteration 31260, Loss: 0.006189268548\n",
      "Iteration 31270, Loss: 0.005536653567\n",
      "Iteration 31280, Loss: 0.004931693431\n",
      "Iteration 31290, Loss: 0.005151784047\n",
      "Iteration 31300, Loss: 0.005101676099\n",
      "Iteration 31310, Loss: 0.006006608717\n",
      "Iteration 31320, Loss: 0.004902842455\n",
      "Iteration 31330, Loss: 0.005395175889\n",
      "Iteration 31340, Loss: 0.004876865540\n",
      "Iteration 31350, Loss: 0.005514214281\n",
      "Iteration 31360, Loss: 0.005986345932\n",
      "Iteration 31370, Loss: 0.005568808410\n",
      "Iteration 31380, Loss: 0.005152490456\n",
      "Iteration 31390, Loss: 0.004873604514\n",
      "Iteration 31400, Loss: 0.004976133350\n",
      "Iteration 31410, Loss: 0.006447372027\n",
      "Iteration 31420, Loss: 0.005199607462\n",
      "Iteration 31430, Loss: 0.005290057976\n",
      "Iteration 31440, Loss: 0.005088925362\n",
      "Iteration 31450, Loss: 0.004889294971\n",
      "Iteration 31460, Loss: 0.004978796002\n",
      "Iteration 31470, Loss: 0.005637600552\n",
      "Iteration 31480, Loss: 0.005146171898\n",
      "Iteration 31490, Loss: 0.005423761904\n",
      "Iteration 31500, Loss: 0.004886572249\n",
      "Iteration 31510, Loss: 0.005342158023\n",
      "Iteration 31520, Loss: 0.006070677657\n",
      "Iteration 31530, Loss: 0.005344231613\n",
      "Iteration 31540, Loss: 0.004929138348\n",
      "Iteration 31550, Loss: 0.005080226343\n",
      "Iteration 31560, Loss: 0.006079564802\n",
      "Iteration 31570, Loss: 0.004866773263\n",
      "Iteration 31580, Loss: 0.005321458448\n",
      "Iteration 31590, Loss: 0.004850847647\n",
      "Iteration 31600, Loss: 0.005286443979\n",
      "Iteration 31610, Loss: 0.006570434198\n",
      "Iteration 31620, Loss: 0.005736845080\n",
      "Iteration 31630, Loss: 0.005190953612\n",
      "Iteration 31640, Loss: 0.004990878515\n",
      "Iteration 31650, Loss: 0.005064746831\n",
      "Iteration 31660, Loss: 0.005556781776\n",
      "Iteration 31670, Loss: 0.005402110517\n",
      "Iteration 31680, Loss: 0.005337899551\n",
      "Iteration 31690, Loss: 0.004883130081\n",
      "Iteration 31700, Loss: 0.005242685787\n",
      "Iteration 31710, Loss: 0.006006990559\n",
      "Iteration 31720, Loss: 0.004871401004\n",
      "Iteration 31730, Loss: 0.005248227157\n",
      "Iteration 31740, Loss: 0.005055332091\n",
      "Iteration 31750, Loss: 0.005456659477\n",
      "Iteration 31760, Loss: 0.005697497632\n",
      "Iteration 31770, Loss: 0.005415442400\n",
      "Iteration 31780, Loss: 0.004888807889\n",
      "Iteration 31790, Loss: 0.005228129216\n",
      "Iteration 31800, Loss: 0.005964520387\n",
      "Iteration 31810, Loss: 0.004853811115\n",
      "Iteration 31820, Loss: 0.005268044304\n",
      "Iteration 31830, Loss: 0.005501231179\n",
      "Iteration 31840, Loss: 0.005153147504\n",
      "Iteration 31850, Loss: 0.004972577095\n",
      "Iteration 31860, Loss: 0.005543807521\n",
      "Iteration 31870, Loss: 0.005920168012\n",
      "Iteration 31880, Loss: 0.005214163102\n",
      "Iteration 31890, Loss: 0.005044506397\n",
      "Iteration 31900, Loss: 0.005003983155\n",
      "Iteration 31910, Loss: 0.005146331619\n",
      "Iteration 31920, Loss: 0.005944532808\n",
      "Iteration 31930, Loss: 0.004863681737\n",
      "Iteration 31940, Loss: 0.005222924985\n",
      "Iteration 31950, Loss: 0.004913457669\n",
      "Iteration 31960, Loss: 0.004844363779\n",
      "Iteration 31970, Loss: 0.004889939446\n",
      "Iteration 31980, Loss: 0.007433196530\n",
      "Iteration 31990, Loss: 0.008017912507\n",
      "Iteration 32000, Loss: 0.005042257253\n",
      "Iteration 32010, Loss: 0.005201761145\n",
      "Iteration 32020, Loss: 0.004883770831\n",
      "Iteration 32030, Loss: 0.004914188758\n",
      "Iteration 32040, Loss: 0.004882926121\n",
      "Iteration 32050, Loss: 0.004859327339\n",
      "Iteration 32060, Loss: 0.004885858856\n",
      "Iteration 32070, Loss: 0.005230861716\n",
      "Iteration 32080, Loss: 0.006699719466\n",
      "Iteration 32090, Loss: 0.005526878871\n",
      "Iteration 32100, Loss: 0.005163456779\n",
      "Iteration 32110, Loss: 0.004934060387\n",
      "Iteration 32120, Loss: 0.005037147086\n",
      "Iteration 32130, Loss: 0.005435067229\n",
      "Iteration 32140, Loss: 0.005490669049\n",
      "Iteration 32150, Loss: 0.005012917332\n",
      "Iteration 32160, Loss: 0.005301161669\n",
      "Iteration 32170, Loss: 0.005062854849\n",
      "Iteration 32180, Loss: 0.005378814414\n",
      "Iteration 32190, Loss: 0.005938874558\n",
      "Iteration 32200, Loss: 0.005021956284\n",
      "Iteration 32210, Loss: 0.005155618303\n",
      "Iteration 32220, Loss: 0.005111748818\n",
      "Iteration 32230, Loss: 0.004901272710\n",
      "Iteration 32240, Loss: 0.006906382740\n",
      "Iteration 32250, Loss: 0.006850617938\n",
      "Iteration 32260, Loss: 0.005219999701\n",
      "Iteration 32270, Loss: 0.005074944813\n",
      "Iteration 32280, Loss: 0.004941596184\n",
      "Iteration 32290, Loss: 0.004863989539\n",
      "Iteration 32300, Loss: 0.004877727944\n",
      "Iteration 32310, Loss: 0.004889552016\n",
      "Iteration 32320, Loss: 0.005307171959\n",
      "Iteration 32330, Loss: 0.006050334312\n",
      "Iteration 32340, Loss: 0.005526656751\n",
      "Iteration 32350, Loss: 0.005133128725\n",
      "Iteration 32360, Loss: 0.004894057754\n",
      "Iteration 32370, Loss: 0.005355306901\n",
      "Iteration 32380, Loss: 0.005835277494\n",
      "Iteration 32390, Loss: 0.004904497415\n",
      "Iteration 32400, Loss: 0.005163439084\n",
      "Iteration 32410, Loss: 0.005303210579\n",
      "Iteration 32420, Loss: 0.005211523268\n",
      "Iteration 32430, Loss: 0.005228368100\n",
      "Iteration 32440, Loss: 0.005317203701\n",
      "Iteration 32450, Loss: 0.005258081947\n",
      "Iteration 32460, Loss: 0.005069286097\n",
      "Iteration 32470, Loss: 0.005356207024\n",
      "Iteration 32480, Loss: 0.005935837515\n",
      "Iteration 32490, Loss: 0.005073591601\n",
      "Iteration 32500, Loss: 0.005146172829\n",
      "Iteration 32510, Loss: 0.004852343351\n",
      "Iteration 32520, Loss: 0.005007043481\n",
      "Iteration 32530, Loss: 0.008795255795\n",
      "Iteration 32540, Loss: 0.005991297308\n",
      "Iteration 32550, Loss: 0.004964242689\n",
      "Iteration 32560, Loss: 0.004965763073\n",
      "Iteration 32570, Loss: 0.004925573710\n",
      "Iteration 32580, Loss: 0.004901636392\n",
      "Iteration 32590, Loss: 0.004875421058\n",
      "Iteration 32600, Loss: 0.005369076505\n",
      "Iteration 32610, Loss: 0.006072654389\n",
      "Iteration 32620, Loss: 0.005225689150\n",
      "Iteration 32630, Loss: 0.005041656084\n",
      "Iteration 32640, Loss: 0.005095580593\n",
      "Iteration 32650, Loss: 0.005022414494\n",
      "Iteration 32660, Loss: 0.004857336171\n",
      "Iteration 32670, Loss: 0.004843806848\n",
      "Iteration 32680, Loss: 0.004879380576\n",
      "Iteration 32690, Loss: 0.010183801875\n",
      "Iteration 32700, Loss: 0.006652830169\n",
      "Iteration 32710, Loss: 0.005743643269\n",
      "Iteration 32720, Loss: 0.004996776581\n",
      "Iteration 32730, Loss: 0.005001914222\n",
      "Iteration 32740, Loss: 0.004881038330\n",
      "Iteration 32750, Loss: 0.004856324755\n",
      "Iteration 32760, Loss: 0.004842644092\n",
      "Iteration 32770, Loss: 0.004828414880\n",
      "Iteration 32780, Loss: 0.004829911049\n",
      "Iteration 32790, Loss: 0.004862460773\n",
      "Iteration 32800, Loss: 0.006476049311\n",
      "Iteration 32810, Loss: 0.005549081601\n",
      "Iteration 32820, Loss: 0.005772867706\n",
      "Iteration 32830, Loss: 0.004954812117\n",
      "Iteration 32840, Loss: 0.004854549654\n",
      "Iteration 32850, Loss: 0.004869068041\n",
      "Iteration 32860, Loss: 0.004827904515\n",
      "Iteration 32870, Loss: 0.004831247032\n",
      "Iteration 32880, Loss: 0.004974348471\n",
      "Iteration 32890, Loss: 0.008990829811\n",
      "Iteration 32900, Loss: 0.006295948755\n",
      "Iteration 32910, Loss: 0.005133426748\n",
      "Iteration 32920, Loss: 0.004906475078\n",
      "Iteration 32930, Loss: 0.004922564607\n",
      "Iteration 32940, Loss: 0.004839514382\n",
      "Iteration 32950, Loss: 0.004956572317\n",
      "Iteration 32960, Loss: 0.005931630731\n",
      "Iteration 32970, Loss: 0.004874538165\n",
      "Iteration 32980, Loss: 0.005184184760\n",
      "Iteration 32990, Loss: 0.004986179527\n",
      "Iteration 33000, Loss: 0.005109373014\n",
      "Iteration 33010, Loss: 0.005426517222\n",
      "Iteration 33020, Loss: 0.005378856324\n",
      "Iteration 33030, Loss: 0.004855017643\n",
      "Iteration 33040, Loss: 0.005326105282\n",
      "Iteration 33050, Loss: 0.006138958968\n",
      "Iteration 33060, Loss: 0.005037013441\n",
      "Iteration 33070, Loss: 0.005092701875\n",
      "Iteration 33080, Loss: 0.004872279707\n",
      "Iteration 33090, Loss: 0.005618174095\n",
      "Iteration 33100, Loss: 0.005448283628\n",
      "Iteration 33110, Loss: 0.005168121308\n",
      "Iteration 33120, Loss: 0.005200130865\n",
      "Iteration 33130, Loss: 0.005038958043\n",
      "Iteration 33140, Loss: 0.004940147977\n",
      "Iteration 33150, Loss: 0.005187258590\n",
      "Iteration 33160, Loss: 0.006439802703\n",
      "Iteration 33170, Loss: 0.005336615257\n",
      "Iteration 33180, Loss: 0.004866803531\n",
      "Iteration 33190, Loss: 0.005049448926\n",
      "Iteration 33200, Loss: 0.005204790272\n",
      "Iteration 33210, Loss: 0.005758228712\n",
      "Iteration 33220, Loss: 0.004910215270\n",
      "Iteration 33230, Loss: 0.005155810155\n",
      "Iteration 33240, Loss: 0.005066895857\n",
      "Iteration 33250, Loss: 0.005354397930\n",
      "Iteration 33260, Loss: 0.006229809485\n",
      "Iteration 33270, Loss: 0.005019863136\n",
      "Iteration 33280, Loss: 0.005003870465\n",
      "Iteration 33290, Loss: 0.004894528538\n",
      "Iteration 33300, Loss: 0.005813833326\n",
      "Iteration 33310, Loss: 0.005138557870\n",
      "Iteration 33320, Loss: 0.005116784945\n",
      "Iteration 33330, Loss: 0.005081002600\n",
      "Iteration 33340, Loss: 0.004896286409\n",
      "Iteration 33350, Loss: 0.005178933498\n",
      "Iteration 33360, Loss: 0.006066946313\n",
      "Iteration 33370, Loss: 0.005282478407\n",
      "Iteration 33380, Loss: 0.005127983633\n",
      "Iteration 33390, Loss: 0.004879409447\n",
      "Iteration 33400, Loss: 0.004839593079\n",
      "Iteration 33410, Loss: 0.004826588091\n",
      "Iteration 33420, Loss: 0.005634463392\n",
      "Iteration 33430, Loss: 0.006312977523\n",
      "Iteration 33440, Loss: 0.005196935963\n",
      "Iteration 33450, Loss: 0.005240942817\n",
      "Iteration 33460, Loss: 0.005013876129\n",
      "Iteration 33470, Loss: 0.004859658889\n",
      "Iteration 33480, Loss: 0.004828641657\n",
      "Iteration 33490, Loss: 0.004822517280\n",
      "Iteration 33500, Loss: 0.004821162205\n",
      "Iteration 33510, Loss: 0.004816365894\n",
      "Iteration 33520, Loss: 0.004831021652\n",
      "Iteration 33530, Loss: 0.005094180349\n",
      "Iteration 33540, Loss: 0.005584139377\n",
      "Iteration 33550, Loss: 0.004975929391\n",
      "Iteration 33560, Loss: 0.005959009752\n",
      "Iteration 33570, Loss: 0.004916331265\n",
      "Iteration 33580, Loss: 0.004905906972\n",
      "Iteration 33590, Loss: 0.004850672092\n",
      "Iteration 33600, Loss: 0.004820520990\n",
      "Iteration 33610, Loss: 0.004826953169\n",
      "Iteration 33620, Loss: 0.004813597072\n",
      "Iteration 33630, Loss: 0.004819020629\n",
      "Iteration 33640, Loss: 0.005052395165\n",
      "Iteration 33650, Loss: 0.008433982730\n",
      "Iteration 33660, Loss: 0.005169993732\n",
      "Iteration 33670, Loss: 0.004974775482\n",
      "Iteration 33680, Loss: 0.005010733381\n",
      "Iteration 33690, Loss: 0.004889310338\n",
      "Iteration 33700, Loss: 0.004816645756\n",
      "Iteration 33710, Loss: 0.004852368496\n",
      "Iteration 33720, Loss: 0.004881829023\n",
      "Iteration 33730, Loss: 0.005664650351\n",
      "Iteration 33740, Loss: 0.005322572775\n",
      "Iteration 33750, Loss: 0.005089250859\n",
      "Iteration 33760, Loss: 0.004976807162\n",
      "Iteration 33770, Loss: 0.004973677453\n",
      "Iteration 33780, Loss: 0.004908312578\n",
      "Iteration 33790, Loss: 0.005048563704\n",
      "Iteration 33800, Loss: 0.006756088231\n",
      "Iteration 33810, Loss: 0.005106280558\n",
      "Iteration 33820, Loss: 0.004842327908\n",
      "Iteration 33830, Loss: 0.005020437762\n",
      "Iteration 33840, Loss: 0.004881309811\n",
      "Iteration 33850, Loss: 0.005023750942\n",
      "Iteration 33860, Loss: 0.007670646533\n",
      "Iteration 33870, Loss: 0.005829734728\n",
      "Iteration 33880, Loss: 0.005255839787\n",
      "Iteration 33890, Loss: 0.005024721846\n",
      "Iteration 33900, Loss: 0.004838066176\n",
      "Iteration 33910, Loss: 0.005026130471\n",
      "Iteration 33920, Loss: 0.005294427276\n",
      "Iteration 33930, Loss: 0.005668072496\n",
      "Iteration 33940, Loss: 0.004825287499\n",
      "Iteration 33950, Loss: 0.005066557322\n",
      "Iteration 33960, Loss: 0.005932330154\n",
      "Iteration 33970, Loss: 0.004849483259\n",
      "Iteration 33980, Loss: 0.005242855288\n",
      "Iteration 33990, Loss: 0.004910146818\n",
      "Iteration 34000, Loss: 0.004836353473\n",
      "Iteration 34010, Loss: 0.005482227542\n",
      "Iteration 34020, Loss: 0.005714409519\n",
      "Iteration 34030, Loss: 0.004980133381\n",
      "Iteration 34040, Loss: 0.005194869358\n",
      "Iteration 34050, Loss: 0.004919795319\n",
      "Iteration 34060, Loss: 0.004863872658\n",
      "Iteration 34070, Loss: 0.004854558036\n",
      "Iteration 34080, Loss: 0.004887423944\n",
      "Iteration 34090, Loss: 0.005703009665\n",
      "Iteration 34100, Loss: 0.005081070587\n",
      "Iteration 34110, Loss: 0.005129483528\n",
      "Iteration 34120, Loss: 0.005061240401\n",
      "Iteration 34130, Loss: 0.004875081126\n",
      "Iteration 34140, Loss: 0.005500271451\n",
      "Iteration 34150, Loss: 0.005580157507\n",
      "Iteration 34160, Loss: 0.004921827465\n",
      "Iteration 34170, Loss: 0.004840075504\n",
      "Iteration 34180, Loss: 0.005218175240\n",
      "Iteration 34190, Loss: 0.007125549950\n",
      "Iteration 34200, Loss: 0.005515767261\n",
      "Iteration 34210, Loss: 0.004960890859\n",
      "Iteration 34220, Loss: 0.004908586852\n",
      "Iteration 34230, Loss: 0.004891867284\n",
      "Iteration 34240, Loss: 0.004909910262\n",
      "Iteration 34250, Loss: 0.005220363382\n",
      "Iteration 34260, Loss: 0.006276993081\n",
      "Iteration 34270, Loss: 0.005256826524\n",
      "Iteration 34280, Loss: 0.004853636492\n",
      "Iteration 34290, Loss: 0.005041518249\n",
      "Iteration 34300, Loss: 0.005081281532\n",
      "Iteration 34310, Loss: 0.006278420333\n",
      "Iteration 34320, Loss: 0.004862967879\n",
      "Iteration 34330, Loss: 0.005122768227\n",
      "Iteration 34340, Loss: 0.004908441100\n",
      "Iteration 34350, Loss: 0.005460143555\n",
      "Iteration 34360, Loss: 0.005396625027\n",
      "Iteration 34370, Loss: 0.004956840537\n",
      "Iteration 34380, Loss: 0.005270874128\n",
      "Iteration 34390, Loss: 0.005045406055\n",
      "Iteration 34400, Loss: 0.006017013453\n",
      "Iteration 34410, Loss: 0.004949387163\n",
      "Iteration 34420, Loss: 0.005236975849\n",
      "Iteration 34430, Loss: 0.004875428509\n",
      "Iteration 34440, Loss: 0.005299297627\n",
      "Iteration 34450, Loss: 0.005763332359\n",
      "Iteration 34460, Loss: 0.004834381863\n",
      "Iteration 34470, Loss: 0.005197795574\n",
      "Iteration 34480, Loss: 0.005260010716\n",
      "Iteration 34490, Loss: 0.005302828737\n",
      "Iteration 34500, Loss: 0.005110865925\n",
      "Iteration 34510, Loss: 0.005160093773\n",
      "Iteration 34520, Loss: 0.005513933022\n",
      "Iteration 34530, Loss: 0.005519491620\n",
      "Iteration 34540, Loss: 0.004966816865\n",
      "Iteration 34550, Loss: 0.005433902610\n",
      "Iteration 34560, Loss: 0.004992560949\n",
      "Iteration 34570, Loss: 0.004897756036\n",
      "Iteration 34580, Loss: 0.005879232194\n",
      "Iteration 34590, Loss: 0.005020343233\n",
      "Iteration 34600, Loss: 0.004944163375\n",
      "Iteration 34610, Loss: 0.004985103384\n",
      "Iteration 34620, Loss: 0.005113044754\n",
      "Iteration 34630, Loss: 0.004886028357\n",
      "Iteration 34640, Loss: 0.004861779045\n",
      "Iteration 34650, Loss: 0.006253430620\n",
      "Iteration 34660, Loss: 0.004984119441\n",
      "Iteration 34670, Loss: 0.005108221900\n",
      "Iteration 34680, Loss: 0.005022799596\n",
      "Iteration 34690, Loss: 0.004831099417\n",
      "Iteration 34700, Loss: 0.004848381970\n",
      "Iteration 34710, Loss: 0.004795433022\n",
      "Iteration 34720, Loss: 0.004803233780\n",
      "Iteration 34730, Loss: 0.004993567709\n",
      "Iteration 34740, Loss: 0.010332239792\n",
      "Iteration 34750, Loss: 0.006061634980\n",
      "Iteration 34760, Loss: 0.005311484449\n",
      "Iteration 34770, Loss: 0.004887954332\n",
      "Iteration 34780, Loss: 0.004806381185\n",
      "Iteration 34790, Loss: 0.004802649841\n",
      "Iteration 34800, Loss: 0.004794217646\n",
      "Iteration 34810, Loss: 0.004802454729\n",
      "Iteration 34820, Loss: 0.004790588748\n",
      "Iteration 34830, Loss: 0.004794986453\n",
      "Iteration 34840, Loss: 0.004888101947\n",
      "Iteration 34850, Loss: 0.006156919524\n",
      "Iteration 34860, Loss: 0.006250489037\n",
      "Iteration 34870, Loss: 0.005415941123\n",
      "Iteration 34880, Loss: 0.005164735019\n",
      "Iteration 34890, Loss: 0.004870467819\n",
      "Iteration 34900, Loss: 0.004798115697\n",
      "Iteration 34910, Loss: 0.004793136381\n",
      "Iteration 34920, Loss: 0.004793472122\n",
      "Iteration 34930, Loss: 0.004794527777\n",
      "Iteration 34940, Loss: 0.004789485130\n",
      "Iteration 34950, Loss: 0.004809136502\n",
      "Iteration 34960, Loss: 0.005757123698\n",
      "Iteration 34970, Loss: 0.004994754214\n",
      "Iteration 34980, Loss: 0.005277021788\n",
      "Iteration 34990, Loss: 0.005157339852\n",
      "Iteration 35000, Loss: 0.004896410275\n",
      "Iteration 35010, Loss: 0.004842967261\n",
      "Iteration 35020, Loss: 0.004803987220\n",
      "Iteration 35030, Loss: 0.004811922088\n",
      "Iteration 35040, Loss: 0.004850049969\n",
      "Iteration 35050, Loss: 0.006055916660\n",
      "Iteration 35060, Loss: 0.004830820486\n",
      "Iteration 35070, Loss: 0.004824390635\n",
      "Iteration 35080, Loss: 0.004812418018\n",
      "Iteration 35090, Loss: 0.004840213805\n",
      "Iteration 35100, Loss: 0.004866740201\n",
      "Iteration 35110, Loss: 0.004896571860\n",
      "Iteration 35120, Loss: 0.005502908491\n",
      "Iteration 35130, Loss: 0.005778332707\n",
      "Iteration 35140, Loss: 0.005349141080\n",
      "Iteration 35150, Loss: 0.005076768342\n",
      "Iteration 35160, Loss: 0.004902140237\n",
      "Iteration 35170, Loss: 0.004906586371\n",
      "Iteration 35180, Loss: 0.004964197055\n",
      "Iteration 35190, Loss: 0.006319096778\n",
      "Iteration 35200, Loss: 0.004854368046\n",
      "Iteration 35210, Loss: 0.004880305380\n",
      "Iteration 35220, Loss: 0.004999085795\n",
      "Iteration 35230, Loss: 0.004800144117\n",
      "Iteration 35240, Loss: 0.004781829193\n",
      "Iteration 35250, Loss: 0.004855379462\n",
      "Iteration 35260, Loss: 0.010684082285\n",
      "Iteration 35270, Loss: 0.006599802524\n",
      "Iteration 35280, Loss: 0.005046826322\n",
      "Iteration 35290, Loss: 0.005158421118\n",
      "Iteration 35300, Loss: 0.004917233251\n",
      "Iteration 35310, Loss: 0.004851504695\n",
      "Iteration 35320, Loss: 0.004794313572\n",
      "Iteration 35330, Loss: 0.004801317118\n",
      "Iteration 35340, Loss: 0.004827633500\n",
      "Iteration 35350, Loss: 0.005667173304\n",
      "Iteration 35360, Loss: 0.005043600220\n",
      "Iteration 35370, Loss: 0.004798086826\n",
      "Iteration 35380, Loss: 0.004796420224\n",
      "Iteration 35390, Loss: 0.004792275839\n",
      "Iteration 35400, Loss: 0.004865444265\n",
      "Iteration 35410, Loss: 0.004821277224\n",
      "Iteration 35420, Loss: 0.004950537812\n",
      "Iteration 35430, Loss: 0.007156295236\n",
      "Iteration 35440, Loss: 0.005503377412\n",
      "Iteration 35450, Loss: 0.005041741300\n",
      "Iteration 35460, Loss: 0.004839951172\n",
      "Iteration 35470, Loss: 0.004839212168\n",
      "Iteration 35480, Loss: 0.004785084166\n",
      "Iteration 35490, Loss: 0.004973852541\n",
      "Iteration 35500, Loss: 0.007708645426\n",
      "Iteration 35510, Loss: 0.005961287767\n",
      "Iteration 35520, Loss: 0.005226848181\n",
      "Iteration 35530, Loss: 0.004988116212\n",
      "Iteration 35540, Loss: 0.004797045607\n",
      "Iteration 35550, Loss: 0.004880785942\n",
      "Iteration 35560, Loss: 0.005194811616\n",
      "Iteration 35570, Loss: 0.006016442552\n",
      "Iteration 35580, Loss: 0.004987643100\n",
      "Iteration 35590, Loss: 0.004894909915\n",
      "Iteration 35600, Loss: 0.004863317590\n",
      "Iteration 35610, Loss: 0.005462047644\n",
      "Iteration 35620, Loss: 0.005860864185\n",
      "Iteration 35630, Loss: 0.005317548756\n",
      "Iteration 35640, Loss: 0.004984413274\n",
      "Iteration 35650, Loss: 0.004864600487\n",
      "Iteration 35660, Loss: 0.004833261017\n",
      "Iteration 35670, Loss: 0.006118234247\n",
      "Iteration 35680, Loss: 0.004828377627\n",
      "Iteration 35690, Loss: 0.005064916797\n",
      "Iteration 35700, Loss: 0.005017091520\n",
      "Iteration 35710, Loss: 0.004870280158\n",
      "Iteration 35720, Loss: 0.004806356505\n",
      "Iteration 35730, Loss: 0.004789900035\n",
      "Iteration 35740, Loss: 0.004881149158\n",
      "Iteration 35750, Loss: 0.007726131007\n",
      "Iteration 35760, Loss: 0.005854507908\n",
      "Iteration 35770, Loss: 0.005245917011\n",
      "Iteration 35780, Loss: 0.004951063544\n",
      "Iteration 35790, Loss: 0.004868327640\n",
      "Iteration 35800, Loss: 0.004811764229\n",
      "Iteration 35810, Loss: 0.004814878106\n",
      "Iteration 35820, Loss: 0.005055342335\n",
      "Iteration 35830, Loss: 0.008448733948\n",
      "Iteration 35840, Loss: 0.005814013537\n",
      "Iteration 35850, Loss: 0.004961878061\n",
      "Iteration 35860, Loss: 0.004825113341\n",
      "Iteration 35870, Loss: 0.004853953607\n",
      "Iteration 35880, Loss: 0.004781574942\n",
      "Iteration 35890, Loss: 0.004852087703\n",
      "Iteration 35900, Loss: 0.005684087984\n",
      "Iteration 35910, Loss: 0.005188201088\n",
      "Iteration 35920, Loss: 0.005023647100\n",
      "Iteration 35930, Loss: 0.004951904062\n",
      "Iteration 35940, Loss: 0.004841897637\n",
      "Iteration 35950, Loss: 0.004901056644\n",
      "Iteration 35960, Loss: 0.006250091828\n",
      "Iteration 35970, Loss: 0.004816826899\n",
      "Iteration 35980, Loss: 0.004942555446\n",
      "Iteration 35990, Loss: 0.004997056909\n",
      "Iteration 36000, Loss: 0.004781124648\n",
      "Iteration 36010, Loss: 0.004955209792\n",
      "Iteration 36020, Loss: 0.006735882256\n",
      "Iteration 36030, Loss: 0.005114073399\n",
      "Iteration 36040, Loss: 0.004878890235\n",
      "Iteration 36050, Loss: 0.004881431349\n",
      "Iteration 36060, Loss: 0.004933989141\n",
      "Iteration 36070, Loss: 0.005234349053\n",
      "Iteration 36080, Loss: 0.005680507515\n",
      "Iteration 36090, Loss: 0.004794425797\n",
      "Iteration 36100, Loss: 0.005110077560\n",
      "Iteration 36110, Loss: 0.005390367005\n",
      "Iteration 36120, Loss: 0.005166464020\n",
      "Iteration 36130, Loss: 0.004829655401\n",
      "Iteration 36140, Loss: 0.004859188106\n",
      "Iteration 36150, Loss: 0.004839900881\n",
      "Iteration 36160, Loss: 0.004768096376\n",
      "Iteration 36170, Loss: 0.005072732922\n",
      "Iteration 36180, Loss: 0.006395577453\n",
      "Iteration 36190, Loss: 0.005420194939\n",
      "Iteration 36200, Loss: 0.005281112157\n",
      "Iteration 36210, Loss: 0.004899435677\n",
      "Iteration 36220, Loss: 0.004883186892\n",
      "Iteration 36230, Loss: 0.004828688223\n",
      "Iteration 36240, Loss: 0.004910616204\n",
      "Iteration 36250, Loss: 0.004812275991\n",
      "Iteration 36260, Loss: 0.004775983747\n",
      "Iteration 36270, Loss: 0.004765139893\n",
      "Iteration 36280, Loss: 0.004762090743\n",
      "Iteration 36290, Loss: 0.004760259297\n",
      "Iteration 36300, Loss: 0.004758829251\n",
      "Iteration 36310, Loss: 0.004761874676\n",
      "Iteration 36320, Loss: 0.005460496992\n",
      "Iteration 36330, Loss: 0.004952582996\n",
      "Iteration 36340, Loss: 0.005755733699\n",
      "Iteration 36350, Loss: 0.005236195400\n",
      "Iteration 36360, Loss: 0.004776059650\n",
      "Iteration 36370, Loss: 0.004840832204\n",
      "Iteration 36380, Loss: 0.004792285617\n",
      "Iteration 36390, Loss: 0.004770400468\n",
      "Iteration 36400, Loss: 0.004765047692\n",
      "Iteration 36410, Loss: 0.004763019271\n",
      "Iteration 36420, Loss: 0.004801298957\n",
      "Iteration 36430, Loss: 0.004996375646\n",
      "Iteration 36440, Loss: 0.005931829568\n",
      "Iteration 36450, Loss: 0.004871803336\n",
      "Iteration 36460, Loss: 0.004860750865\n",
      "Iteration 36470, Loss: 0.004857535008\n",
      "Iteration 36480, Loss: 0.004888273310\n",
      "Iteration 36490, Loss: 0.004775800277\n",
      "Iteration 36500, Loss: 0.004758007359\n",
      "Iteration 36510, Loss: 0.004798422102\n",
      "Iteration 36520, Loss: 0.008236505091\n",
      "Iteration 36530, Loss: 0.007024134509\n",
      "Iteration 36540, Loss: 0.004916824866\n",
      "Iteration 36550, Loss: 0.005080651958\n",
      "Iteration 36560, Loss: 0.004858907312\n",
      "Iteration 36570, Loss: 0.004788264632\n",
      "Iteration 36580, Loss: 0.004782086238\n",
      "Iteration 36590, Loss: 0.004780272953\n",
      "Iteration 36600, Loss: 0.004823154304\n",
      "Iteration 36610, Loss: 0.004819544498\n",
      "Iteration 36620, Loss: 0.005731453653\n",
      "Iteration 36630, Loss: 0.004984529223\n",
      "Iteration 36640, Loss: 0.004855161998\n",
      "Iteration 36650, Loss: 0.004880476277\n",
      "Iteration 36660, Loss: 0.004804183263\n",
      "Iteration 36670, Loss: 0.004770303145\n",
      "Iteration 36680, Loss: 0.004789100960\n",
      "Iteration 36690, Loss: 0.004778943956\n",
      "Iteration 36700, Loss: 0.005118109286\n",
      "Iteration 36710, Loss: 0.008236771449\n",
      "Iteration 36720, Loss: 0.005132083315\n",
      "Iteration 36730, Loss: 0.004959435202\n",
      "Iteration 36740, Loss: 0.004902585410\n",
      "Iteration 36750, Loss: 0.004797792993\n",
      "Iteration 36760, Loss: 0.004758281168\n",
      "Iteration 36770, Loss: 0.004770856816\n",
      "Iteration 36780, Loss: 0.004824381322\n",
      "Iteration 36790, Loss: 0.006217262242\n",
      "Iteration 36800, Loss: 0.004812309984\n",
      "Iteration 36810, Loss: 0.004869350698\n",
      "Iteration 36820, Loss: 0.004780395888\n",
      "Iteration 36830, Loss: 0.004770562984\n",
      "Iteration 36840, Loss: 0.004816170316\n",
      "Iteration 36850, Loss: 0.004872414283\n",
      "Iteration 36860, Loss: 0.006083323155\n",
      "Iteration 36870, Loss: 0.004984743893\n",
      "Iteration 36880, Loss: 0.005013723858\n",
      "Iteration 36890, Loss: 0.004987182096\n",
      "Iteration 36900, Loss: 0.004776257090\n",
      "Iteration 36910, Loss: 0.004974807613\n",
      "Iteration 36920, Loss: 0.005930101033\n",
      "Iteration 36930, Loss: 0.004784306511\n",
      "Iteration 36940, Loss: 0.005080694333\n",
      "Iteration 36950, Loss: 0.004822639748\n",
      "Iteration 36960, Loss: 0.005187371280\n",
      "Iteration 36970, Loss: 0.006000064313\n",
      "Iteration 36980, Loss: 0.004826257005\n",
      "Iteration 36990, Loss: 0.005141419824\n",
      "Iteration 37000, Loss: 0.004795455374\n",
      "Iteration 37010, Loss: 0.005129854660\n",
      "Iteration 37020, Loss: 0.005985058378\n",
      "Iteration 37030, Loss: 0.004801663104\n",
      "Iteration 37040, Loss: 0.004999870900\n",
      "Iteration 37050, Loss: 0.004795314278\n",
      "Iteration 37060, Loss: 0.004873278551\n",
      "Iteration 37070, Loss: 0.006279059686\n",
      "Iteration 37080, Loss: 0.005025500432\n",
      "Iteration 37090, Loss: 0.005601527635\n",
      "Iteration 37100, Loss: 0.004935189150\n",
      "Iteration 37110, Loss: 0.004777832422\n",
      "Iteration 37120, Loss: 0.004793942906\n",
      "Iteration 37130, Loss: 0.004808356520\n",
      "Iteration 37140, Loss: 0.004749268293\n",
      "Iteration 37150, Loss: 0.004743393511\n",
      "Iteration 37160, Loss: 0.004805056378\n",
      "Iteration 37170, Loss: 0.009038824588\n",
      "Iteration 37180, Loss: 0.006886536721\n",
      "Iteration 37190, Loss: 0.004829783924\n",
      "Iteration 37200, Loss: 0.005046031438\n",
      "Iteration 37210, Loss: 0.004966341425\n",
      "Iteration 37220, Loss: 0.004778227303\n",
      "Iteration 37230, Loss: 0.004782841075\n",
      "Iteration 37240, Loss: 0.004754509311\n",
      "Iteration 37250, Loss: 0.004773789551\n",
      "Iteration 37260, Loss: 0.004959948361\n",
      "Iteration 37270, Loss: 0.007235410623\n",
      "Iteration 37280, Loss: 0.005566273350\n",
      "Iteration 37290, Loss: 0.005031858105\n",
      "Iteration 37300, Loss: 0.004833329469\n",
      "Iteration 37310, Loss: 0.004771950655\n",
      "Iteration 37320, Loss: 0.004763113335\n",
      "Iteration 37330, Loss: 0.004945971537\n",
      "Iteration 37340, Loss: 0.007849219255\n",
      "Iteration 37350, Loss: 0.005918538664\n",
      "Iteration 37360, Loss: 0.005115680397\n",
      "Iteration 37370, Loss: 0.004883657675\n",
      "Iteration 37380, Loss: 0.004833051469\n",
      "Iteration 37390, Loss: 0.004741845652\n",
      "Iteration 37400, Loss: 0.004761359654\n",
      "Iteration 37410, Loss: 0.004925203510\n",
      "Iteration 37420, Loss: 0.009061207995\n",
      "Iteration 37430, Loss: 0.005957547575\n",
      "Iteration 37440, Loss: 0.004812812898\n",
      "Iteration 37450, Loss: 0.004851179197\n",
      "Iteration 37460, Loss: 0.004779361654\n",
      "Iteration 37470, Loss: 0.004769901279\n",
      "Iteration 37480, Loss: 0.004746632650\n",
      "Iteration 37490, Loss: 0.004772054497\n",
      "Iteration 37500, Loss: 0.006210019812\n",
      "Iteration 37510, Loss: 0.004910848103\n",
      "Iteration 37520, Loss: 0.005332184490\n",
      "Iteration 37530, Loss: 0.004913994577\n",
      "Iteration 37540, Loss: 0.004794740584\n",
      "Iteration 37550, Loss: 0.004771375097\n",
      "Iteration 37560, Loss: 0.004741854966\n",
      "Iteration 37570, Loss: 0.004741009325\n",
      "Iteration 37580, Loss: 0.004822941963\n",
      "Iteration 37590, Loss: 0.007613102905\n",
      "Iteration 37600, Loss: 0.006007702556\n",
      "Iteration 37610, Loss: 0.005143139511\n",
      "Iteration 37620, Loss: 0.004796511494\n",
      "Iteration 37630, Loss: 0.004775337875\n",
      "Iteration 37640, Loss: 0.004752003122\n",
      "Iteration 37650, Loss: 0.004739419091\n",
      "Iteration 37660, Loss: 0.004762051161\n",
      "Iteration 37670, Loss: 0.005716957152\n",
      "Iteration 37680, Loss: 0.004968362860\n",
      "Iteration 37690, Loss: 0.005070695188\n",
      "Iteration 37700, Loss: 0.005043936428\n",
      "Iteration 37710, Loss: 0.004847191274\n",
      "Iteration 37720, Loss: 0.004771518055\n",
      "Iteration 37730, Loss: 0.004742641002\n",
      "Iteration 37740, Loss: 0.004735767841\n",
      "Iteration 37750, Loss: 0.004801202100\n",
      "Iteration 37760, Loss: 0.007869376801\n",
      "Iteration 37770, Loss: 0.006172269583\n",
      "Iteration 37780, Loss: 0.005176739767\n",
      "Iteration 37790, Loss: 0.004763920791\n",
      "Iteration 37800, Loss: 0.004751649220\n",
      "Iteration 37810, Loss: 0.004741691984\n",
      "Iteration 37820, Loss: 0.004744715057\n",
      "Iteration 37830, Loss: 0.004751240835\n",
      "Iteration 37840, Loss: 0.004932703450\n",
      "Iteration 37850, Loss: 0.008380929008\n",
      "Iteration 37860, Loss: 0.005909082014\n",
      "Iteration 37870, Loss: 0.005005677696\n",
      "Iteration 37880, Loss: 0.004783411976\n",
      "Iteration 37890, Loss: 0.004773770459\n",
      "Iteration 37900, Loss: 0.004749999847\n",
      "Iteration 37910, Loss: 0.004763040692\n",
      "Iteration 37920, Loss: 0.005100935232\n",
      "Iteration 37930, Loss: 0.007239370607\n",
      "Iteration 37940, Loss: 0.005496802740\n",
      "Iteration 37950, Loss: 0.004948030692\n",
      "Iteration 37960, Loss: 0.004831945989\n",
      "Iteration 37970, Loss: 0.004787087906\n",
      "Iteration 37980, Loss: 0.004767558072\n",
      "Iteration 37990, Loss: 0.004942806903\n",
      "Iteration 38000, Loss: 0.006684779190\n",
      "Iteration 38010, Loss: 0.004986680113\n",
      "Iteration 38020, Loss: 0.004809338599\n",
      "Iteration 38030, Loss: 0.004753333982\n",
      "Iteration 38040, Loss: 0.004788641818\n",
      "Iteration 38050, Loss: 0.004722606856\n",
      "Iteration 38060, Loss: 0.004739001859\n",
      "Iteration 38070, Loss: 0.005469613709\n",
      "Iteration 38080, Loss: 0.005223841872\n",
      "Iteration 38090, Loss: 0.005428926088\n",
      "Iteration 38100, Loss: 0.005022477359\n",
      "Iteration 38110, Loss: 0.004745055456\n",
      "Iteration 38120, Loss: 0.004733527079\n",
      "Iteration 38130, Loss: 0.004746140912\n",
      "Iteration 38140, Loss: 0.004816497210\n",
      "Iteration 38150, Loss: 0.004728585482\n",
      "Iteration 38160, Loss: 0.004781060386\n",
      "Iteration 38170, Loss: 0.006713405252\n",
      "Iteration 38180, Loss: 0.005648068152\n",
      "Iteration 38190, Loss: 0.005385064986\n",
      "Iteration 38200, Loss: 0.004768843763\n",
      "Iteration 38210, Loss: 0.004745038226\n",
      "Iteration 38220, Loss: 0.004738575313\n",
      "Iteration 38230, Loss: 0.004728101660\n",
      "Iteration 38240, Loss: 0.004731513094\n",
      "Iteration 38250, Loss: 0.004886370618\n",
      "Iteration 38260, Loss: 0.008071527816\n",
      "Iteration 38270, Loss: 0.005989832338\n",
      "Iteration 38280, Loss: 0.004979562946\n",
      "Iteration 38290, Loss: 0.004792001564\n",
      "Iteration 38300, Loss: 0.004781370051\n",
      "Iteration 38310, Loss: 0.004728760570\n",
      "Iteration 38320, Loss: 0.004738119897\n",
      "Iteration 38330, Loss: 0.005012424197\n",
      "Iteration 38340, Loss: 0.007851513103\n",
      "Iteration 38350, Loss: 0.005559610203\n",
      "Iteration 38360, Loss: 0.004917748272\n",
      "Iteration 38370, Loss: 0.004784872755\n",
      "Iteration 38380, Loss: 0.004760210402\n",
      "Iteration 38390, Loss: 0.004732274450\n",
      "Iteration 38400, Loss: 0.004750896245\n",
      "Iteration 38410, Loss: 0.005773928948\n",
      "Iteration 38420, Loss: 0.004885518458\n",
      "Iteration 38430, Loss: 0.005097670015\n",
      "Iteration 38440, Loss: 0.004987105262\n",
      "Iteration 38450, Loss: 0.004821228329\n",
      "Iteration 38460, Loss: 0.004738268908\n",
      "Iteration 38470, Loss: 0.004718832206\n",
      "Iteration 38480, Loss: 0.004712387454\n",
      "Iteration 38490, Loss: 0.004738990683\n",
      "Iteration 38500, Loss: 0.006735049654\n",
      "Iteration 38510, Loss: 0.005842636339\n",
      "Iteration 38520, Loss: 0.005230055191\n",
      "Iteration 38530, Loss: 0.004890557844\n",
      "Iteration 38540, Loss: 0.004842695780\n",
      "Iteration 38550, Loss: 0.004752606619\n",
      "Iteration 38560, Loss: 0.004721296951\n",
      "Iteration 38570, Loss: 0.004714452662\n",
      "Iteration 38580, Loss: 0.004712077323\n",
      "Iteration 38590, Loss: 0.004719261546\n",
      "Iteration 38600, Loss: 0.005157358479\n",
      "Iteration 38610, Loss: 0.006813280284\n",
      "Iteration 38620, Loss: 0.005050903186\n",
      "Iteration 38630, Loss: 0.005042371806\n",
      "Iteration 38640, Loss: 0.004766818602\n",
      "Iteration 38650, Loss: 0.004732294008\n",
      "Iteration 38660, Loss: 0.004728761967\n",
      "Iteration 38670, Loss: 0.004716055002\n",
      "Iteration 38680, Loss: 0.004713094328\n",
      "Iteration 38690, Loss: 0.004765718710\n",
      "Iteration 38700, Loss: 0.006985335611\n",
      "Iteration 38710, Loss: 0.005612804089\n",
      "Iteration 38720, Loss: 0.005448998883\n",
      "Iteration 38730, Loss: 0.004796165507\n",
      "Iteration 38740, Loss: 0.004730316810\n",
      "Iteration 38750, Loss: 0.004715393763\n",
      "Iteration 38760, Loss: 0.004729314242\n",
      "Iteration 38770, Loss: 0.004703865387\n",
      "Iteration 38780, Loss: 0.004710535984\n",
      "Iteration 38790, Loss: 0.005058554001\n",
      "Iteration 38800, Loss: 0.007610958070\n",
      "Iteration 38810, Loss: 0.004862068221\n",
      "Iteration 38820, Loss: 0.005157710984\n",
      "Iteration 38830, Loss: 0.004777092487\n",
      "Iteration 38840, Loss: 0.004718321841\n",
      "Iteration 38850, Loss: 0.004711050075\n",
      "Iteration 38860, Loss: 0.004722889047\n",
      "Iteration 38870, Loss: 0.004746010993\n",
      "Iteration 38880, Loss: 0.005021476652\n",
      "Iteration 38890, Loss: 0.007366531063\n",
      "Iteration 38900, Loss: 0.005454838276\n",
      "Iteration 38910, Loss: 0.005048080347\n",
      "Iteration 38920, Loss: 0.004804513883\n",
      "Iteration 38930, Loss: 0.004722458310\n",
      "Iteration 38940, Loss: 0.004727197811\n",
      "Iteration 38950, Loss: 0.004833976272\n",
      "Iteration 38960, Loss: 0.007425168063\n",
      "Iteration 38970, Loss: 0.005444270093\n",
      "Iteration 38980, Loss: 0.005099259317\n",
      "Iteration 38990, Loss: 0.004834750667\n",
      "Iteration 39000, Loss: 0.004736054689\n",
      "Iteration 39010, Loss: 0.004713143688\n",
      "Iteration 39020, Loss: 0.004703799263\n",
      "Iteration 39030, Loss: 0.004708698485\n",
      "Iteration 39040, Loss: 0.005710280966\n",
      "Iteration 39050, Loss: 0.004928579088\n",
      "Iteration 39060, Loss: 0.005688551813\n",
      "Iteration 39070, Loss: 0.004823381081\n",
      "Iteration 39080, Loss: 0.004851875827\n",
      "Iteration 39090, Loss: 0.004731284454\n",
      "Iteration 39100, Loss: 0.004701852798\n",
      "Iteration 39110, Loss: 0.004703122657\n",
      "Iteration 39120, Loss: 0.004700755700\n",
      "Iteration 39130, Loss: 0.004697162192\n",
      "Iteration 39140, Loss: 0.004721771460\n",
      "Iteration 39150, Loss: 0.006298314314\n",
      "Iteration 39160, Loss: 0.005158821121\n",
      "Iteration 39170, Loss: 0.005453803111\n",
      "Iteration 39180, Loss: 0.004723185673\n",
      "Iteration 39190, Loss: 0.004817460664\n",
      "Iteration 39200, Loss: 0.004743617959\n",
      "Iteration 39210, Loss: 0.004716231953\n",
      "Iteration 39220, Loss: 0.004696214106\n",
      "Iteration 39230, Loss: 0.004699221347\n",
      "Iteration 39240, Loss: 0.004739005119\n",
      "Iteration 39250, Loss: 0.006296082400\n",
      "Iteration 39260, Loss: 0.004974691197\n",
      "Iteration 39270, Loss: 0.005266087595\n",
      "Iteration 39280, Loss: 0.004898767918\n",
      "Iteration 39290, Loss: 0.004736466799\n",
      "Iteration 39300, Loss: 0.004728332162\n",
      "Iteration 39310, Loss: 0.004711119458\n",
      "Iteration 39320, Loss: 0.004699894227\n",
      "Iteration 39330, Loss: 0.004706853069\n",
      "Iteration 39340, Loss: 0.005083067808\n",
      "Iteration 39350, Loss: 0.009265618399\n",
      "Iteration 39360, Loss: 0.005408741999\n",
      "Iteration 39370, Loss: 0.005038375035\n",
      "Iteration 39380, Loss: 0.004892675206\n",
      "Iteration 39390, Loss: 0.004769904073\n",
      "Iteration 39400, Loss: 0.004710558336\n",
      "Iteration 39410, Loss: 0.004699444398\n",
      "Iteration 39420, Loss: 0.004688629415\n",
      "Iteration 39430, Loss: 0.004693915602\n",
      "Iteration 39440, Loss: 0.005400577560\n",
      "Iteration 39450, Loss: 0.004916618578\n",
      "Iteration 39460, Loss: 0.006029338576\n",
      "Iteration 39470, Loss: 0.004875153303\n",
      "Iteration 39480, Loss: 0.004821333103\n",
      "Iteration 39490, Loss: 0.004698263481\n",
      "Iteration 39500, Loss: 0.004706444219\n",
      "Iteration 39510, Loss: 0.004694027361\n",
      "Iteration 39520, Loss: 0.004687737674\n",
      "Iteration 39530, Loss: 0.004688518587\n",
      "Iteration 39540, Loss: 0.004686335567\n",
      "Iteration 39550, Loss: 0.004711613990\n",
      "Iteration 39560, Loss: 0.006696255412\n",
      "Iteration 39570, Loss: 0.005821252242\n",
      "Iteration 39580, Loss: 0.005149860401\n",
      "Iteration 39590, Loss: 0.004870902281\n",
      "Iteration 39600, Loss: 0.004760117736\n",
      "Iteration 39610, Loss: 0.004718485288\n",
      "Iteration 39620, Loss: 0.004696909804\n",
      "Iteration 39630, Loss: 0.004687295295\n",
      "Iteration 39640, Loss: 0.004685357679\n",
      "Iteration 39650, Loss: 0.004690708593\n",
      "Iteration 39660, Loss: 0.005021784455\n",
      "Iteration 39670, Loss: 0.008166288957\n",
      "Iteration 39680, Loss: 0.004938912112\n",
      "Iteration 39690, Loss: 0.005056701135\n",
      "Iteration 39700, Loss: 0.004720572848\n",
      "Iteration 39710, Loss: 0.004717539065\n",
      "Iteration 39720, Loss: 0.004694324452\n",
      "Iteration 39730, Loss: 0.004686237779\n",
      "Iteration 39740, Loss: 0.004681556020\n",
      "Iteration 39750, Loss: 0.004679806065\n",
      "Iteration 39760, Loss: 0.004702842329\n",
      "Iteration 39770, Loss: 0.007711251732\n",
      "Iteration 39780, Loss: 0.007497815415\n",
      "Iteration 39790, Loss: 0.004970256705\n",
      "Iteration 39800, Loss: 0.004872525111\n",
      "Iteration 39810, Loss: 0.004801034927\n",
      "Iteration 39820, Loss: 0.004712096881\n",
      "Iteration 39830, Loss: 0.004691828974\n",
      "Iteration 39840, Loss: 0.004683364183\n",
      "Iteration 39850, Loss: 0.004679892678\n",
      "Iteration 39860, Loss: 0.004678318277\n",
      "Iteration 39870, Loss: 0.004679031204\n",
      "Iteration 39880, Loss: 0.004738001153\n",
      "Iteration 39890, Loss: 0.007935158908\n",
      "Iteration 39900, Loss: 0.006268330850\n",
      "Iteration 39910, Loss: 0.004803937394\n",
      "Iteration 39920, Loss: 0.004779740237\n",
      "Iteration 39930, Loss: 0.004760914482\n",
      "Iteration 39940, Loss: 0.004699064884\n",
      "Iteration 39950, Loss: 0.004684160929\n",
      "Iteration 39960, Loss: 0.004677947611\n",
      "Iteration 39970, Loss: 0.004685224034\n",
      "Iteration 39980, Loss: 0.005262244027\n",
      "Iteration 39990, Loss: 0.006132240873\n",
      "Iteration 40000, Loss: 0.005419110414\n",
      "Iteration 40010, Loss: 0.004842036869\n",
      "Iteration 40020, Loss: 0.004759778269\n",
      "Iteration 40030, Loss: 0.004723755177\n",
      "Iteration 40040, Loss: 0.004697954282\n",
      "Iteration 40050, Loss: 0.004674566444\n",
      "Iteration 40060, Loss: 0.004677351564\n",
      "Iteration 40070, Loss: 0.004677904770\n",
      "Iteration 40080, Loss: 0.004823767580\n",
      "Iteration 40090, Loss: 0.009686399251\n",
      "Iteration 40100, Loss: 0.005435872357\n",
      "Iteration 40110, Loss: 0.004940591753\n",
      "Iteration 40120, Loss: 0.004787953570\n",
      "Iteration 40130, Loss: 0.004716694821\n",
      "Iteration 40140, Loss: 0.004691385198\n",
      "Iteration 40150, Loss: 0.004683028441\n",
      "Iteration 40160, Loss: 0.004673585296\n",
      "Iteration 40170, Loss: 0.004671022296\n",
      "Iteration 40180, Loss: 0.004748226143\n",
      "Iteration 40190, Loss: 0.009806582704\n",
      "Iteration 40200, Loss: 0.005970050581\n",
      "Iteration 40210, Loss: 0.005179087166\n",
      "Iteration 40220, Loss: 0.004746874794\n",
      "Iteration 40230, Loss: 0.004748912994\n",
      "Iteration 40240, Loss: 0.004698423669\n",
      "Iteration 40250, Loss: 0.004684520885\n",
      "Iteration 40260, Loss: 0.004670258146\n",
      "Iteration 40270, Loss: 0.004670541268\n",
      "Iteration 40280, Loss: 0.004670204129\n",
      "Iteration 40290, Loss: 0.004802503157\n",
      "Iteration 40300, Loss: 0.010093666613\n",
      "Iteration 40310, Loss: 0.005275662057\n",
      "Iteration 40320, Loss: 0.005030953791\n",
      "Iteration 40330, Loss: 0.004748532083\n",
      "Iteration 40340, Loss: 0.004713750444\n",
      "Iteration 40350, Loss: 0.004675358534\n",
      "Iteration 40360, Loss: 0.004676443990\n",
      "Iteration 40370, Loss: 0.004666321911\n",
      "Iteration 40380, Loss: 0.004667123780\n",
      "Iteration 40390, Loss: 0.004768207669\n",
      "Iteration 40400, Loss: 0.009956068359\n",
      "Iteration 40410, Loss: 0.005888534710\n",
      "Iteration 40420, Loss: 0.005037452560\n",
      "Iteration 40430, Loss: 0.004740529694\n",
      "Iteration 40440, Loss: 0.004731907509\n",
      "Iteration 40450, Loss: 0.004685495980\n",
      "Iteration 40460, Loss: 0.004667084664\n",
      "Iteration 40470, Loss: 0.004662170075\n",
      "Iteration 40480, Loss: 0.004663212691\n",
      "Iteration 40490, Loss: 0.004716033116\n",
      "Iteration 40500, Loss: 0.008981328458\n",
      "Iteration 40510, Loss: 0.006509262137\n",
      "Iteration 40520, Loss: 0.004986677784\n",
      "Iteration 40530, Loss: 0.004774993286\n",
      "Iteration 40540, Loss: 0.004725092091\n",
      "Iteration 40550, Loss: 0.004686321598\n",
      "Iteration 40560, Loss: 0.004668606911\n",
      "Iteration 40570, Loss: 0.004662794992\n",
      "Iteration 40580, Loss: 0.004659570288\n",
      "Iteration 40590, Loss: 0.004660540260\n",
      "Iteration 40600, Loss: 0.004851583391\n",
      "Iteration 40610, Loss: 0.010502982885\n",
      "Iteration 40620, Loss: 0.005053951871\n",
      "Iteration 40630, Loss: 0.004970832262\n",
      "Iteration 40640, Loss: 0.004811418708\n",
      "Iteration 40650, Loss: 0.004733536858\n",
      "Iteration 40660, Loss: 0.004683900625\n",
      "Iteration 40670, Loss: 0.004664089531\n",
      "Iteration 40680, Loss: 0.004659200087\n",
      "Iteration 40690, Loss: 0.004657210317\n",
      "Iteration 40700, Loss: 0.004656631034\n",
      "Iteration 40710, Loss: 0.004741881974\n",
      "Iteration 40720, Loss: 0.010004192591\n",
      "Iteration 40730, Loss: 0.005592530593\n",
      "Iteration 40740, Loss: 0.005137772765\n",
      "Iteration 40750, Loss: 0.004775245208\n",
      "Iteration 40760, Loss: 0.004680639599\n",
      "Iteration 40770, Loss: 0.004671281669\n",
      "Iteration 40780, Loss: 0.004662975203\n",
      "Iteration 40790, Loss: 0.004656267352\n",
      "Iteration 40800, Loss: 0.004654735792\n",
      "Iteration 40810, Loss: 0.004659128375\n",
      "Iteration 40820, Loss: 0.004935423378\n",
      "Iteration 40830, Loss: 0.008693752810\n",
      "Iteration 40840, Loss: 0.004887250252\n",
      "Iteration 40850, Loss: 0.004965582862\n",
      "Iteration 40860, Loss: 0.004772020504\n",
      "Iteration 40870, Loss: 0.004660618491\n",
      "Iteration 40880, Loss: 0.004659011494\n",
      "Iteration 40890, Loss: 0.004661684856\n",
      "Iteration 40900, Loss: 0.004650975578\n",
      "Iteration 40910, Loss: 0.004651373252\n",
      "Iteration 40920, Loss: 0.004755742848\n",
      "Iteration 40930, Loss: 0.010908648372\n",
      "Iteration 40940, Loss: 0.005302973092\n",
      "Iteration 40950, Loss: 0.005139357410\n",
      "Iteration 40960, Loss: 0.004708415363\n",
      "Iteration 40970, Loss: 0.004690370522\n",
      "Iteration 40980, Loss: 0.004681193270\n",
      "Iteration 40990, Loss: 0.004659137689\n",
      "Iteration 41000, Loss: 0.004649932496\n",
      "Iteration 41010, Loss: 0.004649822600\n",
      "Iteration 41020, Loss: 0.004653313197\n",
      "Iteration 41030, Loss: 0.004953312688\n",
      "Iteration 41040, Loss: 0.007937035523\n",
      "Iteration 41050, Loss: 0.005008771084\n",
      "Iteration 41060, Loss: 0.004952589516\n",
      "Iteration 41070, Loss: 0.004759448115\n",
      "Iteration 41080, Loss: 0.004661311395\n",
      "Iteration 41090, Loss: 0.004656161647\n",
      "Iteration 41100, Loss: 0.004651999101\n",
      "Iteration 41110, Loss: 0.004646607209\n",
      "Iteration 41120, Loss: 0.004651479889\n",
      "Iteration 41130, Loss: 0.004888616037\n",
      "Iteration 41140, Loss: 0.008729658090\n",
      "Iteration 41150, Loss: 0.004932417534\n",
      "Iteration 41160, Loss: 0.004988135770\n",
      "Iteration 41170, Loss: 0.004725199658\n",
      "Iteration 41180, Loss: 0.004660911858\n",
      "Iteration 41190, Loss: 0.004661045969\n",
      "Iteration 41200, Loss: 0.004648616537\n",
      "Iteration 41210, Loss: 0.004642418120\n",
      "Iteration 41220, Loss: 0.004640689120\n",
      "Iteration 41230, Loss: 0.004638710059\n",
      "Iteration 41240, Loss: 0.004693175200\n",
      "Iteration 41250, Loss: 0.016040962189\n",
      "Iteration 41260, Loss: 0.006157223135\n",
      "Iteration 41270, Loss: 0.005000543315\n",
      "Iteration 41280, Loss: 0.004873776808\n",
      "Iteration 41290, Loss: 0.004671386909\n",
      "Iteration 41300, Loss: 0.004682605620\n",
      "Iteration 41310, Loss: 0.004660186823\n",
      "Iteration 41320, Loss: 0.004650135525\n",
      "Iteration 41330, Loss: 0.004644376691\n",
      "Iteration 41340, Loss: 0.004642095417\n",
      "Iteration 41350, Loss: 0.004639914725\n",
      "Iteration 41360, Loss: 0.004638067912\n",
      "Iteration 41370, Loss: 0.004636554047\n",
      "Iteration 41380, Loss: 0.004635328427\n",
      "Iteration 41390, Loss: 0.004655939061\n",
      "Iteration 41400, Loss: 0.010066552088\n",
      "Iteration 41410, Loss: 0.006019458640\n",
      "Iteration 41420, Loss: 0.005491378251\n",
      "Iteration 41430, Loss: 0.004984722938\n",
      "Iteration 41440, Loss: 0.004775420297\n",
      "Iteration 41450, Loss: 0.004653383512\n",
      "Iteration 41460, Loss: 0.004664821550\n",
      "Iteration 41470, Loss: 0.004642506130\n",
      "Iteration 41480, Loss: 0.004639343824\n",
      "Iteration 41490, Loss: 0.004637460690\n",
      "Iteration 41500, Loss: 0.004635631107\n",
      "Iteration 41510, Loss: 0.004633827601\n",
      "Iteration 41520, Loss: 0.004632486962\n",
      "Iteration 41530, Loss: 0.004636024591\n",
      "Iteration 41540, Loss: 0.005198406987\n",
      "Iteration 41550, Loss: 0.005318740848\n",
      "Iteration 41560, Loss: 0.005607696716\n",
      "Iteration 41570, Loss: 0.005135607440\n",
      "Iteration 41580, Loss: 0.004781180061\n",
      "Iteration 41590, Loss: 0.004692852497\n",
      "Iteration 41600, Loss: 0.004657213110\n",
      "Iteration 41610, Loss: 0.004638649523\n",
      "Iteration 41620, Loss: 0.004635061603\n",
      "Iteration 41630, Loss: 0.004632340744\n",
      "Iteration 41640, Loss: 0.004630120471\n",
      "Iteration 41650, Loss: 0.004629458766\n",
      "Iteration 41660, Loss: 0.004685229622\n",
      "Iteration 41670, Loss: 0.010578741319\n",
      "Iteration 41680, Loss: 0.005811838433\n",
      "Iteration 41690, Loss: 0.005130801816\n",
      "Iteration 41700, Loss: 0.004839221947\n",
      "Iteration 41710, Loss: 0.004730390385\n",
      "Iteration 41720, Loss: 0.004659977742\n",
      "Iteration 41730, Loss: 0.004642276559\n",
      "Iteration 41740, Loss: 0.004633408040\n",
      "Iteration 41750, Loss: 0.004629593808\n",
      "Iteration 41760, Loss: 0.004626964219\n",
      "Iteration 41770, Loss: 0.004625973292\n",
      "Iteration 41780, Loss: 0.004666444845\n",
      "Iteration 41790, Loss: 0.009916520678\n",
      "Iteration 41800, Loss: 0.006316995248\n",
      "Iteration 41810, Loss: 0.005168321542\n",
      "Iteration 41820, Loss: 0.004825285170\n",
      "Iteration 41830, Loss: 0.004735718016\n",
      "Iteration 41840, Loss: 0.004660138395\n",
      "Iteration 41850, Loss: 0.004639811814\n",
      "Iteration 41860, Loss: 0.004630530719\n",
      "Iteration 41870, Loss: 0.004626800772\n",
      "Iteration 41880, Loss: 0.004624288063\n",
      "Iteration 41890, Loss: 0.004624109250\n",
      "Iteration 41900, Loss: 0.004679094069\n",
      "Iteration 41910, Loss: 0.008839616552\n",
      "Iteration 41920, Loss: 0.006474550348\n",
      "Iteration 41930, Loss: 0.004947899841\n",
      "Iteration 41940, Loss: 0.004777946975\n",
      "Iteration 41950, Loss: 0.004641196690\n",
      "Iteration 41960, Loss: 0.004638582468\n",
      "Iteration 41970, Loss: 0.004630818032\n",
      "Iteration 41980, Loss: 0.004624934867\n",
      "Iteration 41990, Loss: 0.004624163266\n",
      "Iteration 42000, Loss: 0.004625184461\n",
      "Iteration 42010, Loss: 0.004770210944\n",
      "Iteration 42020, Loss: 0.009985582903\n",
      "Iteration 42030, Loss: 0.005215851590\n",
      "Iteration 42040, Loss: 0.004928888753\n",
      "Iteration 42050, Loss: 0.004740607925\n",
      "Iteration 42060, Loss: 0.004630082753\n",
      "Iteration 42070, Loss: 0.004636501893\n",
      "Iteration 42080, Loss: 0.004626964219\n",
      "Iteration 42090, Loss: 0.004621205386\n",
      "Iteration 42100, Loss: 0.004627128597\n",
      "Iteration 42110, Loss: 0.004826118704\n",
      "Iteration 42120, Loss: 0.009233024903\n",
      "Iteration 42130, Loss: 0.005097850226\n",
      "Iteration 42140, Loss: 0.004879895598\n",
      "Iteration 42150, Loss: 0.004706700798\n",
      "Iteration 42160, Loss: 0.004675463308\n",
      "Iteration 42170, Loss: 0.004628778435\n",
      "Iteration 42180, Loss: 0.004623031709\n",
      "Iteration 42190, Loss: 0.004622139037\n",
      "Iteration 42200, Loss: 0.004674745724\n",
      "Iteration 42210, Loss: 0.007138641551\n",
      "Iteration 42220, Loss: 0.006007692777\n",
      "Iteration 42230, Loss: 0.004924001638\n",
      "Iteration 42240, Loss: 0.004739543889\n",
      "Iteration 42250, Loss: 0.004671702161\n",
      "Iteration 42260, Loss: 0.004634671845\n",
      "Iteration 42270, Loss: 0.004620722495\n",
      "Iteration 42280, Loss: 0.004616660997\n",
      "Iteration 42290, Loss: 0.004636055790\n",
      "Iteration 42300, Loss: 0.005620348267\n",
      "Iteration 42310, Loss: 0.004738571588\n",
      "Iteration 42320, Loss: 0.005328434985\n",
      "Iteration 42330, Loss: 0.004713507835\n",
      "Iteration 42340, Loss: 0.004704316612\n",
      "Iteration 42350, Loss: 0.004633772653\n",
      "Iteration 42360, Loss: 0.004621654283\n",
      "Iteration 42370, Loss: 0.004617228638\n",
      "Iteration 42380, Loss: 0.004610521253\n",
      "Iteration 42390, Loss: 0.004627087153\n",
      "Iteration 42400, Loss: 0.006407518405\n",
      "Iteration 42410, Loss: 0.006280620117\n",
      "Iteration 42420, Loss: 0.004869764205\n",
      "Iteration 42430, Loss: 0.004942974076\n",
      "Iteration 42440, Loss: 0.004671769682\n",
      "Iteration 42450, Loss: 0.004652929492\n",
      "Iteration 42460, Loss: 0.004626517184\n",
      "Iteration 42470, Loss: 0.004617004655\n",
      "Iteration 42480, Loss: 0.004608589225\n",
      "Iteration 42490, Loss: 0.004609258845\n",
      "Iteration 42500, Loss: 0.004634564277\n",
      "Iteration 42510, Loss: 0.006208996754\n",
      "Iteration 42520, Loss: 0.005403723568\n",
      "Iteration 42530, Loss: 0.005103301723\n",
      "Iteration 42540, Loss: 0.004743192811\n",
      "Iteration 42550, Loss: 0.004679093137\n",
      "Iteration 42560, Loss: 0.004638539162\n",
      "Iteration 42570, Loss: 0.004615149926\n",
      "Iteration 42580, Loss: 0.004607244860\n",
      "Iteration 42590, Loss: 0.004606721457\n",
      "Iteration 42600, Loss: 0.004672696814\n",
      "Iteration 42610, Loss: 0.008225666359\n",
      "Iteration 42620, Loss: 0.006393984891\n",
      "Iteration 42630, Loss: 0.004844175186\n",
      "Iteration 42640, Loss: 0.004747090861\n",
      "Iteration 42650, Loss: 0.004679214209\n",
      "Iteration 42660, Loss: 0.004616738763\n",
      "Iteration 42670, Loss: 0.004609397613\n",
      "Iteration 42680, Loss: 0.004604965448\n",
      "Iteration 42690, Loss: 0.004600485787\n",
      "Iteration 42700, Loss: 0.004622603767\n",
      "Iteration 42710, Loss: 0.007581260987\n",
      "Iteration 42720, Loss: 0.007308451459\n",
      "Iteration 42730, Loss: 0.005151844118\n",
      "Iteration 42740, Loss: 0.004647910129\n",
      "Iteration 42750, Loss: 0.004645746667\n",
      "Iteration 42760, Loss: 0.004616127349\n",
      "Iteration 42770, Loss: 0.004606983624\n",
      "Iteration 42780, Loss: 0.004605619702\n",
      "Iteration 42790, Loss: 0.004601101857\n",
      "Iteration 42800, Loss: 0.004597675055\n",
      "Iteration 42810, Loss: 0.004596594721\n",
      "Iteration 42820, Loss: 0.004627121612\n",
      "Iteration 42830, Loss: 0.009186042473\n",
      "Iteration 42840, Loss: 0.006611770019\n",
      "Iteration 42850, Loss: 0.005308242980\n",
      "Iteration 42860, Loss: 0.004820696078\n",
      "Iteration 42870, Loss: 0.004678335972\n",
      "Iteration 42880, Loss: 0.004627300426\n",
      "Iteration 42890, Loss: 0.004609425087\n",
      "Iteration 42900, Loss: 0.004600259475\n",
      "Iteration 42910, Loss: 0.004596098326\n",
      "Iteration 42920, Loss: 0.004595076665\n",
      "Iteration 42930, Loss: 0.004595720209\n",
      "Iteration 42940, Loss: 0.004701223690\n",
      "Iteration 42950, Loss: 0.009865662083\n",
      "Iteration 42960, Loss: 0.005154046696\n",
      "Iteration 42970, Loss: 0.004985214677\n",
      "Iteration 42980, Loss: 0.004742632154\n",
      "Iteration 42990, Loss: 0.004656020086\n",
      "Iteration 43000, Loss: 0.004615903366\n",
      "Iteration 43010, Loss: 0.004598043393\n",
      "Iteration 43020, Loss: 0.004592590500\n",
      "Iteration 43030, Loss: 0.004592187703\n",
      "Iteration 43040, Loss: 0.004605651367\n",
      "Iteration 43050, Loss: 0.005608780775\n",
      "Iteration 43060, Loss: 0.004689300898\n",
      "Iteration 43070, Loss: 0.005358571652\n",
      "Iteration 43080, Loss: 0.004770145286\n",
      "Iteration 43090, Loss: 0.004675662611\n",
      "Iteration 43100, Loss: 0.004609579686\n",
      "Iteration 43110, Loss: 0.004603557289\n",
      "Iteration 43120, Loss: 0.004596549552\n",
      "Iteration 43130, Loss: 0.004590078257\n",
      "Iteration 43140, Loss: 0.004588755313\n",
      "Iteration 43150, Loss: 0.004678560421\n",
      "Iteration 43160, Loss: 0.011092594825\n",
      "Iteration 43170, Loss: 0.005081470124\n",
      "Iteration 43180, Loss: 0.005027671810\n",
      "Iteration 43190, Loss: 0.004749087617\n",
      "Iteration 43200, Loss: 0.004619750660\n",
      "Iteration 43210, Loss: 0.004598368425\n",
      "Iteration 43220, Loss: 0.004597500898\n",
      "Iteration 43230, Loss: 0.004589823075\n",
      "Iteration 43240, Loss: 0.004585566930\n",
      "Iteration 43250, Loss: 0.004586432595\n",
      "Iteration 43260, Loss: 0.004656935111\n",
      "Iteration 43270, Loss: 0.009664053097\n",
      "Iteration 43280, Loss: 0.005848926492\n",
      "Iteration 43290, Loss: 0.005027194042\n",
      "Iteration 43300, Loss: 0.004647310358\n",
      "Iteration 43310, Loss: 0.004641785286\n",
      "Iteration 43320, Loss: 0.004617046099\n",
      "Iteration 43330, Loss: 0.004588464275\n",
      "Iteration 43340, Loss: 0.004584536888\n",
      "Iteration 43350, Loss: 0.004583264701\n",
      "Iteration 43360, Loss: 0.004591490142\n",
      "Iteration 43370, Loss: 0.005324639846\n",
      "Iteration 43380, Loss: 0.004780480638\n",
      "Iteration 43390, Loss: 0.005404864438\n",
      "Iteration 43400, Loss: 0.004778278060\n",
      "Iteration 43410, Loss: 0.004638263024\n",
      "Iteration 43420, Loss: 0.004626689944\n",
      "Iteration 43430, Loss: 0.004591595381\n",
      "Iteration 43440, Loss: 0.004582343157\n",
      "Iteration 43450, Loss: 0.004580905195\n",
      "Iteration 43460, Loss: 0.004579815548\n",
      "Iteration 43470, Loss: 0.004656603094\n",
      "Iteration 43480, Loss: 0.010146530345\n",
      "Iteration 43490, Loss: 0.005569197703\n",
      "Iteration 43500, Loss: 0.005051001906\n",
      "Iteration 43510, Loss: 0.004641825333\n",
      "Iteration 43520, Loss: 0.004627548158\n",
      "Iteration 43530, Loss: 0.004612902179\n",
      "Iteration 43540, Loss: 0.004585069604\n",
      "Iteration 43550, Loss: 0.004578321707\n",
      "Iteration 43560, Loss: 0.004577656277\n",
      "Iteration 43570, Loss: 0.004582049325\n",
      "Iteration 43580, Loss: 0.005046010949\n",
      "Iteration 43590, Loss: 0.005876020994\n",
      "Iteration 43600, Loss: 0.005427123047\n",
      "Iteration 43610, Loss: 0.004716204945\n",
      "Iteration 43620, Loss: 0.004676261917\n",
      "Iteration 43630, Loss: 0.004629617091\n",
      "Iteration 43640, Loss: 0.004584230017\n",
      "Iteration 43650, Loss: 0.004577233456\n",
      "Iteration 43660, Loss: 0.004574995954\n",
      "Iteration 43670, Loss: 0.004573552404\n",
      "Iteration 43680, Loss: 0.004639507737\n",
      "Iteration 43690, Loss: 0.009690628387\n",
      "Iteration 43700, Loss: 0.005890392233\n",
      "Iteration 43710, Loss: 0.005039052106\n",
      "Iteration 43720, Loss: 0.004637381528\n",
      "Iteration 43730, Loss: 0.004616163671\n",
      "Iteration 43740, Loss: 0.004606594797\n",
      "Iteration 43750, Loss: 0.004581155721\n",
      "Iteration 43760, Loss: 0.004571013153\n",
      "Iteration 43770, Loss: 0.004571450409\n",
      "Iteration 43780, Loss: 0.004582249559\n",
      "Iteration 43790, Loss: 0.005435840227\n",
      "Iteration 43800, Loss: 0.004672460258\n",
      "Iteration 43810, Loss: 0.005274758209\n",
      "Iteration 43820, Loss: 0.004758832976\n",
      "Iteration 43830, Loss: 0.004615996033\n",
      "Iteration 43840, Loss: 0.004605429247\n",
      "Iteration 43850, Loss: 0.004583999980\n",
      "Iteration 43860, Loss: 0.004569772631\n",
      "Iteration 43870, Loss: 0.004567832686\n",
      "Iteration 43880, Loss: 0.004568929318\n",
      "Iteration 43890, Loss: 0.004795955028\n",
      "Iteration 43900, Loss: 0.008755288087\n",
      "Iteration 43910, Loss: 0.005232769996\n",
      "Iteration 43920, Loss: 0.004755242262\n",
      "Iteration 43930, Loss: 0.004726417828\n",
      "Iteration 43940, Loss: 0.004624645691\n",
      "Iteration 43950, Loss: 0.004574833438\n",
      "Iteration 43960, Loss: 0.004567901138\n",
      "Iteration 43970, Loss: 0.004567530472\n",
      "Iteration 43980, Loss: 0.004562969320\n",
      "Iteration 43990, Loss: 0.004564001691\n",
      "Iteration 44000, Loss: 0.004742385354\n",
      "Iteration 44010, Loss: 0.009530286305\n",
      "Iteration 44020, Loss: 0.005172484554\n",
      "Iteration 44030, Loss: 0.004750197288\n",
      "Iteration 44040, Loss: 0.004743615165\n",
      "Iteration 44050, Loss: 0.004622315057\n",
      "Iteration 44060, Loss: 0.004580771551\n",
      "Iteration 44070, Loss: 0.004565853626\n",
      "Iteration 44080, Loss: 0.004563860130\n",
      "Iteration 44090, Loss: 0.004561573267\n",
      "Iteration 44100, Loss: 0.004563691095\n",
      "Iteration 44110, Loss: 0.004759352654\n",
      "Iteration 44120, Loss: 0.008967993781\n",
      "Iteration 44130, Loss: 0.004846497905\n",
      "Iteration 44140, Loss: 0.004851864185\n",
      "Iteration 44150, Loss: 0.004705538973\n",
      "Iteration 44160, Loss: 0.004588797223\n",
      "Iteration 44170, Loss: 0.004569320939\n",
      "Iteration 44180, Loss: 0.004566112999\n",
      "Iteration 44190, Loss: 0.004559496418\n",
      "Iteration 44200, Loss: 0.004556322936\n",
      "Iteration 44210, Loss: 0.004592638928\n",
      "Iteration 44220, Loss: 0.008446956985\n",
      "Iteration 44230, Loss: 0.006755352486\n",
      "Iteration 44240, Loss: 0.005178800784\n",
      "Iteration 44250, Loss: 0.004654118791\n",
      "Iteration 44260, Loss: 0.004597163759\n",
      "Iteration 44270, Loss: 0.004584075883\n",
      "Iteration 44280, Loss: 0.004569025245\n",
      "Iteration 44290, Loss: 0.004557679407\n",
      "Iteration 44300, Loss: 0.004554477986\n",
      "Iteration 44310, Loss: 0.004553940147\n",
      "Iteration 44320, Loss: 0.004579366650\n",
      "Iteration 44330, Loss: 0.006955496967\n",
      "Iteration 44340, Loss: 0.006664169952\n",
      "Iteration 44350, Loss: 0.004893173929\n",
      "Iteration 44360, Loss: 0.004705065396\n",
      "Iteration 44370, Loss: 0.004647843074\n",
      "Iteration 44380, Loss: 0.004581272136\n",
      "Iteration 44390, Loss: 0.004556067754\n",
      "Iteration 44400, Loss: 0.004553535022\n",
      "Iteration 44410, Loss: 0.004552097991\n",
      "Iteration 44420, Loss: 0.004552549217\n",
      "Iteration 44430, Loss: 0.004759137053\n",
      "Iteration 44440, Loss: 0.009579000995\n",
      "Iteration 44450, Loss: 0.004957201891\n",
      "Iteration 44460, Loss: 0.004757394549\n",
      "Iteration 44470, Loss: 0.004698726349\n",
      "Iteration 44480, Loss: 0.004607775714\n",
      "Iteration 44490, Loss: 0.004556089174\n",
      "Iteration 44500, Loss: 0.004552063532\n",
      "Iteration 44510, Loss: 0.004550539423\n",
      "Iteration 44520, Loss: 0.004548157565\n",
      "Iteration 44530, Loss: 0.004574323539\n",
      "Iteration 44540, Loss: 0.006756419316\n",
      "Iteration 44550, Loss: 0.006406039000\n",
      "Iteration 44560, Loss: 0.004838139284\n",
      "Iteration 44570, Loss: 0.004727333784\n",
      "Iteration 44580, Loss: 0.004626692738\n",
      "Iteration 44590, Loss: 0.004556826316\n",
      "Iteration 44600, Loss: 0.004551469814\n",
      "Iteration 44610, Loss: 0.004549715668\n",
      "Iteration 44620, Loss: 0.004543689545\n",
      "Iteration 44630, Loss: 0.004545256030\n",
      "Iteration 44640, Loss: 0.004787722137\n",
      "Iteration 44650, Loss: 0.008812079206\n",
      "Iteration 44660, Loss: 0.005517316051\n",
      "Iteration 44670, Loss: 0.004734887742\n",
      "Iteration 44680, Loss: 0.004659537226\n",
      "Iteration 44690, Loss: 0.004613075405\n",
      "Iteration 44700, Loss: 0.004564428702\n",
      "Iteration 44710, Loss: 0.004547735676\n",
      "Iteration 44720, Loss: 0.004545000382\n",
      "Iteration 44730, Loss: 0.004541290458\n",
      "Iteration 44740, Loss: 0.004541399423\n",
      "Iteration 44750, Loss: 0.004647899419\n",
      "Iteration 44760, Loss: 0.010774562135\n",
      "Iteration 44770, Loss: 0.004873317666\n",
      "Iteration 44780, Loss: 0.004944065120\n",
      "Iteration 44790, Loss: 0.004701814149\n",
      "Iteration 44800, Loss: 0.004563575611\n",
      "Iteration 44810, Loss: 0.004551713821\n",
      "Iteration 44820, Loss: 0.004549052101\n",
      "Iteration 44830, Loss: 0.004540937021\n",
      "Iteration 44840, Loss: 0.004537533503\n",
      "Iteration 44850, Loss: 0.004535204265\n",
      "Iteration 44860, Loss: 0.004544310272\n",
      "Iteration 44870, Loss: 0.006874646060\n",
      "Iteration 44880, Loss: 0.007727272809\n",
      "Iteration 44890, Loss: 0.005391501822\n",
      "Iteration 44900, Loss: 0.004735180177\n",
      "Iteration 44910, Loss: 0.004588571843\n",
      "Iteration 44920, Loss: 0.004585481249\n",
      "Iteration 44930, Loss: 0.004546592943\n",
      "Iteration 44940, Loss: 0.004541159142\n",
      "Iteration 44950, Loss: 0.004538063426\n",
      "Iteration 44960, Loss: 0.004535182379\n",
      "Iteration 44970, Loss: 0.004533388186\n",
      "Iteration 44980, Loss: 0.004534195643\n",
      "Iteration 44990, Loss: 0.004657547455\n",
      "Iteration 45000, Loss: 0.009674135596\n",
      "Iteration 45010, Loss: 0.004806303419\n",
      "Iteration 45020, Loss: 0.004917491227\n",
      "Iteration 45030, Loss: 0.004712772090\n",
      "Iteration 45040, Loss: 0.004566119052\n",
      "Iteration 45050, Loss: 0.004541508853\n",
      "Iteration 45060, Loss: 0.004542174283\n",
      "Iteration 45070, Loss: 0.004535029642\n",
      "Iteration 45080, Loss: 0.004531800747\n",
      "Iteration 45090, Loss: 0.004547689110\n",
      "Iteration 45100, Loss: 0.005946994759\n",
      "Iteration 45110, Loss: 0.005138840526\n",
      "Iteration 45120, Loss: 0.004917727318\n",
      "Iteration 45130, Loss: 0.004802734591\n",
      "Iteration 45140, Loss: 0.004607290961\n",
      "Iteration 45150, Loss: 0.004545507487\n",
      "Iteration 45160, Loss: 0.004532385152\n",
      "Iteration 45170, Loss: 0.004530664533\n",
      "Iteration 45180, Loss: 0.004528176039\n",
      "Iteration 45190, Loss: 0.004524479154\n",
      "Iteration 45200, Loss: 0.004532788415\n",
      "Iteration 45210, Loss: 0.006405720022\n",
      "Iteration 45220, Loss: 0.007691744249\n",
      "Iteration 45230, Loss: 0.005306102335\n",
      "Iteration 45240, Loss: 0.004888479598\n",
      "Iteration 45250, Loss: 0.004592260346\n",
      "Iteration 45260, Loss: 0.004570028745\n",
      "Iteration 45270, Loss: 0.004537039436\n",
      "Iteration 45280, Loss: 0.004532040562\n",
      "Iteration 45290, Loss: 0.004528203979\n",
      "Iteration 45300, Loss: 0.004524715245\n",
      "Iteration 45310, Loss: 0.004523472860\n",
      "Iteration 45320, Loss: 0.004524043296\n",
      "Iteration 45330, Loss: 0.004639700986\n",
      "Iteration 45340, Loss: 0.010498012416\n",
      "Iteration 45350, Loss: 0.004813975655\n",
      "Iteration 45360, Loss: 0.004832150415\n",
      "Iteration 45370, Loss: 0.004630902316\n",
      "Iteration 45380, Loss: 0.004590675700\n",
      "Iteration 45390, Loss: 0.004537758417\n",
      "Iteration 45400, Loss: 0.004532371182\n",
      "Iteration 45410, Loss: 0.004522581119\n",
      "Iteration 45420, Loss: 0.004522512667\n",
      "Iteration 45430, Loss: 0.004549466074\n",
      "Iteration 45440, Loss: 0.006444678642\n",
      "Iteration 45450, Loss: 0.005885503255\n",
      "Iteration 45460, Loss: 0.004840389825\n",
      "Iteration 45470, Loss: 0.004665343091\n",
      "Iteration 45480, Loss: 0.004599898122\n",
      "Iteration 45490, Loss: 0.004541780334\n",
      "Iteration 45500, Loss: 0.004530108534\n",
      "Iteration 45510, Loss: 0.004522124771\n",
      "Iteration 45520, Loss: 0.004524612334\n",
      "Iteration 45530, Loss: 0.004651310854\n",
      "Iteration 45540, Loss: 0.008635585196\n",
      "Iteration 45550, Loss: 0.005413032137\n",
      "Iteration 45560, Loss: 0.004754853901\n",
      "Iteration 45570, Loss: 0.004630872514\n",
      "Iteration 45580, Loss: 0.004562051967\n",
      "Iteration 45590, Loss: 0.004533159547\n",
      "Iteration 45600, Loss: 0.004519901704\n",
      "Iteration 45610, Loss: 0.004523783922\n",
      "Iteration 45620, Loss: 0.004617768340\n",
      "Iteration 45630, Loss: 0.008602000773\n",
      "Iteration 45640, Loss: 0.005887049250\n",
      "Iteration 45650, Loss: 0.004720442928\n",
      "Iteration 45660, Loss: 0.004632427823\n",
      "Iteration 45670, Loss: 0.004547836725\n",
      "Iteration 45680, Loss: 0.004539215472\n",
      "Iteration 45690, Loss: 0.004515117034\n",
      "Iteration 45700, Loss: 0.004516579211\n",
      "Iteration 45710, Loss: 0.004544029012\n",
      "Iteration 45720, Loss: 0.006950629409\n",
      "Iteration 45730, Loss: 0.006545110140\n",
      "Iteration 45740, Loss: 0.004621624481\n",
      "Iteration 45750, Loss: 0.004729142878\n",
      "Iteration 45760, Loss: 0.004548479337\n",
      "Iteration 45770, Loss: 0.004558339715\n",
      "Iteration 45780, Loss: 0.004525523633\n",
      "Iteration 45790, Loss: 0.004510213621\n",
      "Iteration 45800, Loss: 0.004517060705\n",
      "Iteration 45810, Loss: 0.004705927335\n",
      "Iteration 45820, Loss: 0.006124904379\n",
      "Iteration 45830, Loss: 0.004711357877\n",
      "Iteration 45840, Loss: 0.004607398994\n",
      "Iteration 45850, Loss: 0.004540730268\n",
      "Iteration 45860, Loss: 0.004616357386\n",
      "Iteration 45870, Loss: 0.004755942617\n",
      "Iteration 45880, Loss: 0.006357089151\n",
      "Iteration 45890, Loss: 0.004908922128\n",
      "Iteration 45900, Loss: 0.004622558132\n",
      "Iteration 45910, Loss: 0.004510869272\n",
      "Iteration 45920, Loss: 0.004626288544\n",
      "Iteration 45930, Loss: 0.005021315068\n",
      "Iteration 45940, Loss: 0.005826555658\n",
      "Iteration 45950, Loss: 0.004994674120\n",
      "Iteration 45960, Loss: 0.004647210240\n",
      "Iteration 45970, Loss: 0.004542271607\n",
      "Iteration 45980, Loss: 0.004532536957\n",
      "Iteration 45990, Loss: 0.005323666148\n",
      "Iteration 46000, Loss: 0.004720502999\n",
      "Iteration 46010, Loss: 0.005316332448\n",
      "Iteration 46020, Loss: 0.004533546511\n",
      "Iteration 46030, Loss: 0.004605197348\n",
      "Iteration 46040, Loss: 0.004561959766\n",
      "Iteration 46050, Loss: 0.004519467242\n",
      "Iteration 46060, Loss: 0.004506533500\n",
      "Iteration 46070, Loss: 0.004506475758\n",
      "Iteration 46080, Loss: 0.004700292367\n",
      "Iteration 46090, Loss: 0.009974809363\n",
      "Iteration 46100, Loss: 0.004830530845\n",
      "Iteration 46110, Loss: 0.004706352483\n",
      "Iteration 46120, Loss: 0.004693749826\n",
      "Iteration 46130, Loss: 0.004517873283\n",
      "Iteration 46140, Loss: 0.004513446707\n",
      "Iteration 46150, Loss: 0.004500083160\n",
      "Iteration 46160, Loss: 0.004501812626\n",
      "Iteration 46170, Loss: 0.004495800473\n",
      "Iteration 46180, Loss: 0.004497670103\n",
      "Iteration 46190, Loss: 0.004809150472\n",
      "Iteration 46200, Loss: 0.007144434843\n",
      "Iteration 46210, Loss: 0.005616461858\n",
      "Iteration 46220, Loss: 0.004727425985\n",
      "Iteration 46230, Loss: 0.004578096326\n",
      "Iteration 46240, Loss: 0.004571653903\n",
      "Iteration 46250, Loss: 0.004524292890\n",
      "Iteration 46260, Loss: 0.004504084121\n",
      "Iteration 46270, Loss: 0.004499108065\n",
      "Iteration 46280, Loss: 0.004494462162\n",
      "Iteration 46290, Loss: 0.004497309681\n",
      "Iteration 46300, Loss: 0.004668380599\n",
      "Iteration 46310, Loss: 0.011093040928\n",
      "Iteration 46320, Loss: 0.004773429595\n",
      "Iteration 46330, Loss: 0.004599418957\n",
      "Iteration 46340, Loss: 0.004669936839\n",
      "Iteration 46350, Loss: 0.004538130481\n",
      "Iteration 46360, Loss: 0.004532369319\n",
      "Iteration 46370, Loss: 0.004503396805\n",
      "Iteration 46380, Loss: 0.004497127607\n",
      "Iteration 46390, Loss: 0.004489602521\n",
      "Iteration 46400, Loss: 0.004490055144\n",
      "Iteration 46410, Loss: 0.004567245487\n",
      "Iteration 46420, Loss: 0.009018618613\n",
      "Iteration 46430, Loss: 0.005306210835\n",
      "Iteration 46440, Loss: 0.004972770810\n",
      "Iteration 46450, Loss: 0.004515391309\n",
      "Iteration 46460, Loss: 0.004577569198\n",
      "Iteration 46470, Loss: 0.004516897723\n",
      "Iteration 46480, Loss: 0.004503386095\n",
      "Iteration 46490, Loss: 0.004491243511\n",
      "Iteration 46500, Loss: 0.004492422566\n",
      "Iteration 46510, Loss: 0.004591696896\n",
      "Iteration 46520, Loss: 0.008752420545\n",
      "Iteration 46530, Loss: 0.005903747398\n",
      "Iteration 46540, Loss: 0.004678211641\n",
      "Iteration 46550, Loss: 0.004673237912\n",
      "Iteration 46560, Loss: 0.004507022444\n",
      "Iteration 46570, Loss: 0.004501138348\n",
      "Iteration 46580, Loss: 0.004498997703\n",
      "Iteration 46590, Loss: 0.004493276589\n",
      "Iteration 46600, Loss: 0.004574024118\n",
      "Iteration 46610, Loss: 0.008252955973\n",
      "Iteration 46620, Loss: 0.005836668424\n",
      "Iteration 46630, Loss: 0.004743031692\n",
      "Iteration 46640, Loss: 0.004647031892\n",
      "Iteration 46650, Loss: 0.004493753426\n",
      "Iteration 46660, Loss: 0.004503078759\n",
      "Iteration 46670, Loss: 0.004491837230\n",
      "Iteration 46680, Loss: 0.004481490701\n",
      "Iteration 46690, Loss: 0.004497266840\n",
      "Iteration 46700, Loss: 0.005002190825\n",
      "Iteration 46710, Loss: 0.008993146941\n",
      "Iteration 46720, Loss: 0.004807783756\n",
      "Iteration 46730, Loss: 0.005009925924\n",
      "Iteration 46740, Loss: 0.004502342083\n",
      "Iteration 46750, Loss: 0.004513934255\n",
      "Iteration 46760, Loss: 0.004495088011\n",
      "Iteration 46770, Loss: 0.004482517950\n",
      "Iteration 46780, Loss: 0.004484205507\n",
      "Iteration 46790, Loss: 0.004594851285\n",
      "Iteration 46800, Loss: 0.008145078085\n",
      "Iteration 46810, Loss: 0.005632748827\n",
      "Iteration 46820, Loss: 0.004594873171\n",
      "Iteration 46830, Loss: 0.004630748648\n",
      "Iteration 46840, Loss: 0.004528985824\n",
      "Iteration 46850, Loss: 0.004503723700\n",
      "Iteration 46860, Loss: 0.004485141486\n",
      "Iteration 46870, Loss: 0.004507255740\n",
      "Iteration 46880, Loss: 0.005067431834\n",
      "Iteration 46890, Loss: 0.005469582975\n",
      "Iteration 46900, Loss: 0.004566131625\n",
      "Iteration 46910, Loss: 0.004725938663\n",
      "Iteration 46920, Loss: 0.004578197375\n",
      "Iteration 46930, Loss: 0.004521962721\n",
      "Iteration 46940, Loss: 0.004477635492\n",
      "Iteration 46950, Loss: 0.004504031036\n",
      "Iteration 46960, Loss: 0.005355045199\n",
      "Iteration 46970, Loss: 0.004718380049\n",
      "Iteration 46980, Loss: 0.005001295358\n",
      "Iteration 46990, Loss: 0.004670226946\n",
      "Iteration 47000, Loss: 0.004493025597\n",
      "Iteration 47010, Loss: 0.004491185769\n",
      "Iteration 47020, Loss: 0.004503137432\n",
      "Iteration 47030, Loss: 0.004498565570\n",
      "Iteration 47040, Loss: 0.004906965885\n",
      "Iteration 47050, Loss: 0.006547791418\n",
      "Iteration 47060, Loss: 0.004978092387\n",
      "Iteration 47070, Loss: 0.004856701940\n",
      "Iteration 47080, Loss: 0.004649527837\n",
      "Iteration 47090, Loss: 0.004530829843\n",
      "Iteration 47100, Loss: 0.004518571775\n",
      "Iteration 47110, Loss: 0.004573822021\n",
      "Iteration 47120, Loss: 0.007084331475\n",
      "Iteration 47130, Loss: 0.005519648548\n",
      "Iteration 47140, Loss: 0.004820610862\n",
      "Iteration 47150, Loss: 0.004481227137\n",
      "Iteration 47160, Loss: 0.004473422654\n",
      "Iteration 47170, Loss: 0.004502296448\n",
      "Iteration 47180, Loss: 0.004467638675\n",
      "Iteration 47190, Loss: 0.004472152330\n",
      "Iteration 47200, Loss: 0.005399894901\n",
      "Iteration 47210, Loss: 0.004966698121\n",
      "Iteration 47220, Loss: 0.004869546741\n",
      "Iteration 47230, Loss: 0.004817061126\n",
      "Iteration 47240, Loss: 0.004573297687\n",
      "Iteration 47250, Loss: 0.004500468727\n",
      "Iteration 47260, Loss: 0.004478143994\n",
      "Iteration 47270, Loss: 0.004465748556\n",
      "Iteration 47280, Loss: 0.004466696177\n",
      "Iteration 47290, Loss: 0.004463953432\n",
      "Iteration 47300, Loss: 0.004518344067\n",
      "Iteration 47310, Loss: 0.007635893300\n",
      "Iteration 47320, Loss: 0.006384580396\n",
      "Iteration 47330, Loss: 0.004660128616\n",
      "Iteration 47340, Loss: 0.004687222652\n",
      "Iteration 47350, Loss: 0.004470815882\n",
      "Iteration 47360, Loss: 0.004478763323\n",
      "Iteration 47370, Loss: 0.004460384604\n",
      "Iteration 47380, Loss: 0.004468317144\n",
      "Iteration 47390, Loss: 0.004473542795\n",
      "Iteration 47400, Loss: 0.004894875921\n",
      "Iteration 47410, Loss: 0.006124940701\n",
      "Iteration 47420, Loss: 0.004761377815\n",
      "Iteration 47430, Loss: 0.004735372029\n",
      "Iteration 47440, Loss: 0.004479377996\n",
      "Iteration 47450, Loss: 0.004477242474\n",
      "Iteration 47460, Loss: 0.004488425795\n",
      "Iteration 47470, Loss: 0.004566662945\n",
      "Iteration 47480, Loss: 0.004637951963\n",
      "Iteration 47490, Loss: 0.006675323937\n",
      "Iteration 47500, Loss: 0.005251336843\n",
      "Iteration 47510, Loss: 0.004768045153\n",
      "Iteration 47520, Loss: 0.004578634631\n",
      "Iteration 47530, Loss: 0.004525158089\n",
      "Iteration 47540, Loss: 0.004532770254\n",
      "Iteration 47550, Loss: 0.004657955840\n",
      "Iteration 47560, Loss: 0.007295861375\n",
      "Iteration 47570, Loss: 0.005858605728\n",
      "Iteration 47580, Loss: 0.004662738182\n",
      "Iteration 47590, Loss: 0.004595792852\n",
      "Iteration 47600, Loss: 0.004537011497\n",
      "Iteration 47610, Loss: 0.004490421154\n",
      "Iteration 47620, Loss: 0.004451907706\n",
      "Iteration 47630, Loss: 0.004465505481\n",
      "Iteration 47640, Loss: 0.004745180253\n",
      "Iteration 47650, Loss: 0.007597535849\n",
      "Iteration 47660, Loss: 0.004710800480\n",
      "Iteration 47670, Loss: 0.004679507576\n",
      "Iteration 47680, Loss: 0.004589179065\n",
      "Iteration 47690, Loss: 0.004493649118\n",
      "Iteration 47700, Loss: 0.004453127738\n",
      "Iteration 47710, Loss: 0.004456445109\n",
      "Iteration 47720, Loss: 0.004462459125\n",
      "Iteration 47730, Loss: 0.004917265847\n",
      "Iteration 47740, Loss: 0.005779699888\n",
      "Iteration 47750, Loss: 0.004920909647\n",
      "Iteration 47760, Loss: 0.004625693429\n",
      "Iteration 47770, Loss: 0.004524205811\n",
      "Iteration 47780, Loss: 0.004467551131\n",
      "Iteration 47790, Loss: 0.004447929095\n",
      "Iteration 47800, Loss: 0.004449869972\n",
      "Iteration 47810, Loss: 0.004456946161\n",
      "Iteration 47820, Loss: 0.004813155159\n",
      "Iteration 47830, Loss: 0.006343764719\n",
      "Iteration 47840, Loss: 0.005071896594\n",
      "Iteration 47850, Loss: 0.004611162003\n",
      "Iteration 47860, Loss: 0.004567761440\n",
      "Iteration 47870, Loss: 0.004464925732\n",
      "Iteration 47880, Loss: 0.004465240519\n",
      "Iteration 47890, Loss: 0.004442182370\n",
      "Iteration 47900, Loss: 0.004446811974\n",
      "Iteration 47910, Loss: 0.004591151606\n",
      "Iteration 47920, Loss: 0.009047600441\n",
      "Iteration 47930, Loss: 0.004670164548\n",
      "Iteration 47940, Loss: 0.004856710788\n",
      "Iteration 47950, Loss: 0.004493597895\n",
      "Iteration 47960, Loss: 0.004518092610\n",
      "Iteration 47970, Loss: 0.004456961527\n",
      "Iteration 47980, Loss: 0.004448527005\n",
      "Iteration 47990, Loss: 0.004443821963\n",
      "Iteration 48000, Loss: 0.004510306288\n",
      "Iteration 48010, Loss: 0.007058611605\n",
      "Iteration 48020, Loss: 0.005929986015\n",
      "Iteration 48030, Loss: 0.004674643744\n",
      "Iteration 48040, Loss: 0.004585720133\n",
      "Iteration 48050, Loss: 0.004482586868\n",
      "Iteration 48060, Loss: 0.004459591582\n",
      "Iteration 48070, Loss: 0.004436621908\n",
      "Iteration 48080, Loss: 0.004436070099\n",
      "Iteration 48090, Loss: 0.004493133165\n",
      "Iteration 48100, Loss: 0.010292859748\n",
      "Iteration 48110, Loss: 0.005128736608\n",
      "Iteration 48120, Loss: 0.005012425594\n",
      "Iteration 48130, Loss: 0.004633737728\n",
      "Iteration 48140, Loss: 0.004488098901\n",
      "Iteration 48150, Loss: 0.004447291140\n",
      "Iteration 48160, Loss: 0.004440301098\n",
      "Iteration 48170, Loss: 0.004436071496\n",
      "Iteration 48180, Loss: 0.004434882198\n",
      "Iteration 48190, Loss: 0.004457538482\n",
      "Iteration 48200, Loss: 0.006056437269\n",
      "Iteration 48210, Loss: 0.005343548954\n",
      "Iteration 48220, Loss: 0.004733124282\n",
      "Iteration 48230, Loss: 0.004658573307\n",
      "Iteration 48240, Loss: 0.004482199438\n",
      "Iteration 48250, Loss: 0.004451158457\n",
      "Iteration 48260, Loss: 0.004440918565\n",
      "Iteration 48270, Loss: 0.004434503615\n",
      "Iteration 48280, Loss: 0.004451239947\n",
      "Iteration 48290, Loss: 0.005085028708\n",
      "Iteration 48300, Loss: 0.005029764958\n",
      "Iteration 48310, Loss: 0.005034688395\n",
      "Iteration 48320, Loss: 0.004576938692\n",
      "Iteration 48330, Loss: 0.004453454167\n",
      "Iteration 48340, Loss: 0.004461514298\n",
      "Iteration 48350, Loss: 0.004433711991\n",
      "Iteration 48360, Loss: 0.004443622660\n",
      "Iteration 48370, Loss: 0.004533489235\n",
      "Iteration 48380, Loss: 0.006969300099\n",
      "Iteration 48390, Loss: 0.005568101536\n",
      "Iteration 48400, Loss: 0.004571537487\n",
      "Iteration 48410, Loss: 0.004534367006\n",
      "Iteration 48420, Loss: 0.004484863486\n",
      "Iteration 48430, Loss: 0.004432668909\n",
      "Iteration 48440, Loss: 0.004455076996\n",
      "Iteration 48450, Loss: 0.006134540774\n",
      "Iteration 48460, Loss: 0.005221494008\n",
      "Iteration 48470, Loss: 0.004887711722\n",
      "Iteration 48480, Loss: 0.004591143224\n",
      "Iteration 48490, Loss: 0.004510309082\n",
      "Iteration 48500, Loss: 0.004456003662\n",
      "Iteration 48510, Loss: 0.004429849330\n",
      "Iteration 48520, Loss: 0.004434459843\n",
      "Iteration 48530, Loss: 0.004550478887\n",
      "Iteration 48540, Loss: 0.008004753850\n",
      "Iteration 48550, Loss: 0.005439505447\n",
      "Iteration 48560, Loss: 0.004622969776\n",
      "Iteration 48570, Loss: 0.004498491529\n",
      "Iteration 48580, Loss: 0.004470581654\n",
      "Iteration 48590, Loss: 0.004432759248\n",
      "Iteration 48600, Loss: 0.004427873064\n",
      "Iteration 48610, Loss: 0.004449555185\n",
      "Iteration 48620, Loss: 0.006053558085\n",
      "Iteration 48630, Loss: 0.005353813525\n",
      "Iteration 48640, Loss: 0.004729109816\n",
      "Iteration 48650, Loss: 0.004646334331\n",
      "Iteration 48660, Loss: 0.004452688619\n",
      "Iteration 48670, Loss: 0.004439843353\n",
      "Iteration 48680, Loss: 0.004420832265\n",
      "Iteration 48690, Loss: 0.004423203878\n",
      "Iteration 48700, Loss: 0.004417058546\n",
      "Iteration 48710, Loss: 0.004538684152\n",
      "Iteration 48720, Loss: 0.011211058125\n",
      "Iteration 48730, Loss: 0.004595246166\n",
      "Iteration 48740, Loss: 0.004741866142\n",
      "Iteration 48750, Loss: 0.004561384209\n",
      "Iteration 48760, Loss: 0.004444566555\n",
      "Iteration 48770, Loss: 0.004448505584\n",
      "Iteration 48780, Loss: 0.004426760599\n",
      "Iteration 48790, Loss: 0.004415207077\n",
      "Iteration 48800, Loss: 0.004416768439\n",
      "Iteration 48810, Loss: 0.004450339824\n",
      "Iteration 48820, Loss: 0.006545379758\n",
      "Iteration 48830, Loss: 0.005701782648\n",
      "Iteration 48840, Loss: 0.004547725432\n",
      "Iteration 48850, Loss: 0.004645499866\n",
      "Iteration 48860, Loss: 0.004471714143\n",
      "Iteration 48870, Loss: 0.004426704254\n",
      "Iteration 48880, Loss: 0.004427382257\n",
      "Iteration 48890, Loss: 0.004413332324\n",
      "Iteration 48900, Loss: 0.004423370585\n",
      "Iteration 48910, Loss: 0.005067765247\n",
      "Iteration 48920, Loss: 0.004980208818\n",
      "Iteration 48930, Loss: 0.005487347022\n",
      "Iteration 48940, Loss: 0.004587232135\n",
      "Iteration 48950, Loss: 0.004480032250\n",
      "Iteration 48960, Loss: 0.004431567620\n",
      "Iteration 48970, Loss: 0.004416691605\n",
      "Iteration 48980, Loss: 0.004418226890\n",
      "Iteration 48990, Loss: 0.004407609347\n",
      "Iteration 49000, Loss: 0.004430073779\n",
      "Iteration 49010, Loss: 0.006534255110\n",
      "Iteration 49020, Loss: 0.006152224727\n",
      "Iteration 49030, Loss: 0.004530934617\n",
      "Iteration 49040, Loss: 0.004611885641\n",
      "Iteration 49050, Loss: 0.004457728006\n",
      "Iteration 49060, Loss: 0.004445812665\n",
      "Iteration 49070, Loss: 0.004419615492\n",
      "Iteration 49080, Loss: 0.004408187233\n",
      "Iteration 49090, Loss: 0.004412693437\n",
      "Iteration 49100, Loss: 0.004533880856\n",
      "Iteration 49110, Loss: 0.005057658069\n",
      "Iteration 49120, Loss: 0.005132196937\n",
      "Iteration 49130, Loss: 0.004660660867\n",
      "Iteration 49140, Loss: 0.004707735498\n",
      "Iteration 49150, Loss: 0.004479730036\n",
      "Iteration 49160, Loss: 0.004455817398\n",
      "Iteration 49170, Loss: 0.004415270407\n",
      "Iteration 49180, Loss: 0.004506743979\n",
      "Iteration 49190, Loss: 0.006235984154\n",
      "Iteration 49200, Loss: 0.004931060597\n",
      "Iteration 49210, Loss: 0.004757937975\n",
      "Iteration 49220, Loss: 0.004499286879\n",
      "Iteration 49230, Loss: 0.004444788676\n",
      "Iteration 49240, Loss: 0.004403480794\n",
      "Iteration 49250, Loss: 0.004400697537\n",
      "Iteration 49260, Loss: 0.004418072291\n",
      "Iteration 49270, Loss: 0.008603717200\n",
      "Iteration 49280, Loss: 0.006238646805\n",
      "Iteration 49290, Loss: 0.005083239637\n",
      "Iteration 49300, Loss: 0.004746663384\n",
      "Iteration 49310, Loss: 0.004549848847\n",
      "Iteration 49320, Loss: 0.004431045614\n",
      "Iteration 49330, Loss: 0.004414677154\n",
      "Iteration 49340, Loss: 0.004402217921\n",
      "Iteration 49350, Loss: 0.004396949895\n",
      "Iteration 49360, Loss: 0.004397009965\n",
      "Iteration 49370, Loss: 0.004404463340\n",
      "Iteration 49380, Loss: 0.004710932262\n",
      "Iteration 49390, Loss: 0.007318694144\n",
      "Iteration 49400, Loss: 0.004731737077\n",
      "Iteration 49410, Loss: 0.004516313784\n",
      "Iteration 49420, Loss: 0.004513306543\n",
      "Iteration 49430, Loss: 0.004435090814\n",
      "Iteration 49440, Loss: 0.004406524822\n",
      "Iteration 49450, Loss: 0.004401552957\n",
      "Iteration 49460, Loss: 0.004403392784\n",
      "Iteration 49470, Loss: 0.004829760641\n",
      "Iteration 49480, Loss: 0.005969901104\n",
      "Iteration 49490, Loss: 0.004915092140\n",
      "Iteration 49500, Loss: 0.004598383792\n",
      "Iteration 49510, Loss: 0.004433279857\n",
      "Iteration 49520, Loss: 0.004434228409\n",
      "Iteration 49530, Loss: 0.004404354375\n",
      "Iteration 49540, Loss: 0.004398706369\n",
      "Iteration 49550, Loss: 0.004402405582\n",
      "Iteration 49560, Loss: 0.004837936256\n",
      "Iteration 49570, Loss: 0.005660205148\n",
      "Iteration 49580, Loss: 0.004957381170\n",
      "Iteration 49590, Loss: 0.004525215365\n",
      "Iteration 49600, Loss: 0.004490627442\n",
      "Iteration 49610, Loss: 0.004408819601\n",
      "Iteration 49620, Loss: 0.004390448797\n",
      "Iteration 49630, Loss: 0.004390773829\n",
      "Iteration 49640, Loss: 0.004397986457\n",
      "Iteration 49650, Loss: 0.005016778596\n",
      "Iteration 49660, Loss: 0.004749205895\n",
      "Iteration 49670, Loss: 0.005278932862\n",
      "Iteration 49680, Loss: 0.004562983755\n",
      "Iteration 49690, Loss: 0.004472565837\n",
      "Iteration 49700, Loss: 0.004404969048\n",
      "Iteration 49710, Loss: 0.004391781054\n",
      "Iteration 49720, Loss: 0.004386022221\n",
      "Iteration 49730, Loss: 0.004388299305\n",
      "Iteration 49740, Loss: 0.004479738884\n",
      "Iteration 49750, Loss: 0.008911655284\n",
      "Iteration 49760, Loss: 0.005682123825\n",
      "Iteration 49770, Loss: 0.004731125198\n",
      "Iteration 49780, Loss: 0.004408367444\n",
      "Iteration 49790, Loss: 0.004427054897\n",
      "Iteration 49800, Loss: 0.004408531822\n",
      "Iteration 49810, Loss: 0.004388315603\n",
      "Iteration 49820, Loss: 0.004383896478\n",
      "Iteration 49830, Loss: 0.004400600679\n",
      "Iteration 49840, Loss: 0.005557585508\n",
      "Iteration 49850, Loss: 0.004685293883\n",
      "Iteration 49860, Loss: 0.004983290099\n",
      "Iteration 49870, Loss: 0.004548640922\n",
      "Iteration 49880, Loss: 0.004466490820\n",
      "Iteration 49890, Loss: 0.004406694788\n",
      "Iteration 49900, Loss: 0.004395068157\n",
      "Iteration 49910, Loss: 0.004382214043\n",
      "Iteration 49920, Loss: 0.004380577710\n",
      "Iteration 49930, Loss: 0.004644949920\n",
      "Iteration 49940, Loss: 0.008051880635\n",
      "Iteration 49950, Loss: 0.005163967609\n",
      "Iteration 49960, Loss: 0.004500273149\n",
      "Iteration 49970, Loss: 0.004560115747\n",
      "Iteration 49980, Loss: 0.004397334531\n",
      "Iteration 49990, Loss: 0.004387164023\n",
      "Iteration 50000, Loss: 0.004378995858\n",
      "Iteration 50010, Loss: 0.004378290381\n",
      "Iteration 50020, Loss: 0.004388150759\n",
      "Iteration 50030, Loss: 0.005187338684\n",
      "Iteration 50040, Loss: 0.004516543821\n",
      "Iteration 50050, Loss: 0.005172534380\n",
      "Iteration 50060, Loss: 0.004499706440\n",
      "Iteration 50070, Loss: 0.004493476823\n",
      "Iteration 50080, Loss: 0.004399031401\n",
      "Iteration 50090, Loss: 0.004387941677\n",
      "Iteration 50100, Loss: 0.004374847282\n",
      "Iteration 50110, Loss: 0.004391286056\n",
      "Iteration 50120, Loss: 0.005091164727\n",
      "Iteration 50130, Loss: 0.004793937784\n",
      "Iteration 50140, Loss: 0.005195622332\n",
      "Iteration 50150, Loss: 0.004449284635\n",
      "Iteration 50160, Loss: 0.004469391424\n",
      "Iteration 50170, Loss: 0.004414759111\n",
      "Iteration 50180, Loss: 0.004383966792\n",
      "Iteration 50190, Loss: 0.004385074135\n",
      "Iteration 50200, Loss: 0.004494306631\n",
      "Iteration 50210, Loss: 0.005505203735\n",
      "Iteration 50220, Loss: 0.004523266107\n",
      "Iteration 50230, Loss: 0.004961473867\n",
      "Iteration 50240, Loss: 0.004445665982\n",
      "Iteration 50250, Loss: 0.004424734507\n",
      "Iteration 50260, Loss: 0.004395145457\n",
      "Iteration 50270, Loss: 0.004367909394\n",
      "Iteration 50280, Loss: 0.004422420636\n",
      "Iteration 50290, Loss: 0.007130591199\n",
      "Iteration 50300, Loss: 0.006124837324\n",
      "Iteration 50310, Loss: 0.004487251397\n",
      "Iteration 50320, Loss: 0.004563163500\n",
      "Iteration 50330, Loss: 0.004401725251\n",
      "Iteration 50340, Loss: 0.004388866946\n",
      "Iteration 50350, Loss: 0.004376154393\n",
      "Iteration 50360, Loss: 0.004386817571\n",
      "Iteration 50370, Loss: 0.005053940695\n",
      "Iteration 50380, Loss: 0.004741669632\n",
      "Iteration 50390, Loss: 0.004933496937\n",
      "Iteration 50400, Loss: 0.004446336534\n",
      "Iteration 50410, Loss: 0.004446295556\n",
      "Iteration 50420, Loss: 0.004384127446\n",
      "Iteration 50430, Loss: 0.004380249418\n",
      "Iteration 50440, Loss: 0.004367321730\n",
      "Iteration 50450, Loss: 0.004523023497\n",
      "Iteration 50460, Loss: 0.009512913413\n",
      "Iteration 50470, Loss: 0.004532415420\n",
      "Iteration 50480, Loss: 0.004666306544\n",
      "Iteration 50490, Loss: 0.004468900152\n",
      "Iteration 50500, Loss: 0.004417636897\n",
      "Iteration 50510, Loss: 0.004382588435\n",
      "Iteration 50520, Loss: 0.004365265835\n",
      "Iteration 50530, Loss: 0.004362547304\n",
      "Iteration 50540, Loss: 0.004509525839\n",
      "Iteration 50550, Loss: 0.008880886249\n",
      "Iteration 50560, Loss: 0.004866260570\n",
      "Iteration 50570, Loss: 0.004684993997\n",
      "Iteration 50580, Loss: 0.004424487706\n",
      "Iteration 50590, Loss: 0.004415784962\n",
      "Iteration 50600, Loss: 0.004365876317\n",
      "Iteration 50610, Loss: 0.004365451634\n",
      "Iteration 50620, Loss: 0.004373762757\n",
      "Iteration 50630, Loss: 0.005158751272\n",
      "Iteration 50640, Loss: 0.004440175835\n",
      "Iteration 50650, Loss: 0.005026755854\n",
      "Iteration 50660, Loss: 0.004412618931\n",
      "Iteration 50670, Loss: 0.004471256398\n",
      "Iteration 50680, Loss: 0.004383004736\n",
      "Iteration 50690, Loss: 0.004365329631\n",
      "Iteration 50700, Loss: 0.004365738016\n",
      "Iteration 50710, Loss: 0.004533990286\n",
      "Iteration 50720, Loss: 0.008399400860\n",
      "Iteration 50730, Loss: 0.004862337839\n",
      "Iteration 50740, Loss: 0.004635627382\n",
      "Iteration 50750, Loss: 0.004466908053\n",
      "Iteration 50760, Loss: 0.004370545037\n",
      "Iteration 50770, Loss: 0.004372241907\n",
      "Iteration 50780, Loss: 0.004365759436\n",
      "Iteration 50790, Loss: 0.004498220980\n",
      "Iteration 50800, Loss: 0.008783267811\n",
      "Iteration 50810, Loss: 0.005150895100\n",
      "Iteration 50820, Loss: 0.004666787107\n",
      "Iteration 50830, Loss: 0.004455716815\n",
      "Iteration 50840, Loss: 0.004358624108\n",
      "Iteration 50850, Loss: 0.004363822285\n",
      "Iteration 50860, Loss: 0.004351441283\n",
      "Iteration 50870, Loss: 0.004379958380\n",
      "Iteration 50880, Loss: 0.006122517399\n",
      "Iteration 50890, Loss: 0.005559916608\n",
      "Iteration 50900, Loss: 0.004548787139\n",
      "Iteration 50910, Loss: 0.004492571577\n",
      "Iteration 50920, Loss: 0.004426708911\n",
      "Iteration 50930, Loss: 0.004359591752\n",
      "Iteration 50940, Loss: 0.004359633196\n",
      "Iteration 50950, Loss: 0.004357114434\n",
      "Iteration 50960, Loss: 0.004584480543\n",
      "Iteration 50970, Loss: 0.008063073270\n",
      "Iteration 50980, Loss: 0.004648280330\n",
      "Iteration 50990, Loss: 0.004632031545\n",
      "Iteration 51000, Loss: 0.004427654669\n",
      "Iteration 51010, Loss: 0.004359633662\n",
      "Iteration 51020, Loss: 0.004359452054\n",
      "Iteration 51030, Loss: 0.004352539312\n",
      "Iteration 51040, Loss: 0.004469626583\n",
      "Iteration 51050, Loss: 0.009060549550\n",
      "Iteration 51060, Loss: 0.004830291960\n",
      "Iteration 51070, Loss: 0.004569490906\n",
      "Iteration 51080, Loss: 0.004397361074\n",
      "Iteration 51090, Loss: 0.004372217227\n",
      "Iteration 51100, Loss: 0.004357232712\n",
      "Iteration 51110, Loss: 0.004341705702\n",
      "Iteration 51120, Loss: 0.004362295847\n",
      "Iteration 51130, Loss: 0.005547434092\n",
      "Iteration 51140, Loss: 0.004745099228\n",
      "Iteration 51150, Loss: 0.004875462502\n",
      "Iteration 51160, Loss: 0.004473168869\n",
      "Iteration 51170, Loss: 0.004445915110\n",
      "Iteration 51180, Loss: 0.004382819869\n",
      "Iteration 51190, Loss: 0.004356964957\n",
      "Iteration 51200, Loss: 0.004355199635\n",
      "Iteration 51210, Loss: 0.004497598391\n",
      "Iteration 51220, Loss: 0.008029134944\n",
      "Iteration 51230, Loss: 0.005315582734\n",
      "Iteration 51240, Loss: 0.004439687356\n",
      "Iteration 51250, Loss: 0.004474315792\n",
      "Iteration 51260, Loss: 0.004403939936\n",
      "Iteration 51270, Loss: 0.004337919876\n",
      "Iteration 51280, Loss: 0.004379412625\n",
      "Iteration 51290, Loss: 0.005111353938\n",
      "Iteration 51300, Loss: 0.004699828569\n",
      "Iteration 51310, Loss: 0.004589928314\n",
      "Iteration 51320, Loss: 0.004594345577\n",
      "Iteration 51330, Loss: 0.004443990998\n",
      "Iteration 51340, Loss: 0.004371558316\n",
      "Iteration 51350, Loss: 0.004427813459\n",
      "Iteration 51360, Loss: 0.005404600408\n",
      "Iteration 51370, Loss: 0.004432546906\n",
      "Iteration 51380, Loss: 0.004379873164\n",
      "Iteration 51390, Loss: 0.004404619336\n",
      "Iteration 51400, Loss: 0.004465576261\n",
      "Iteration 51410, Loss: 0.004738901742\n",
      "Iteration 51420, Loss: 0.005366016179\n",
      "Iteration 51430, Loss: 0.004553169943\n",
      "Iteration 51440, Loss: 0.004371591844\n",
      "Iteration 51450, Loss: 0.004442965146\n",
      "Iteration 51460, Loss: 0.006116852164\n",
      "Iteration 51470, Loss: 0.004631015472\n",
      "Iteration 51480, Loss: 0.004740621429\n",
      "Iteration 51490, Loss: 0.004527873360\n",
      "Iteration 51500, Loss: 0.004404038191\n",
      "Iteration 51510, Loss: 0.004462987650\n",
      "Iteration 51520, Loss: 0.005211634561\n",
      "Iteration 51530, Loss: 0.004653034266\n",
      "Iteration 51540, Loss: 0.004591234960\n",
      "Iteration 51550, Loss: 0.004446939565\n",
      "Iteration 51560, Loss: 0.004699205980\n",
      "Iteration 51570, Loss: 0.005484097637\n",
      "Iteration 51580, Loss: 0.004449655768\n",
      "Iteration 51590, Loss: 0.004395479802\n",
      "Iteration 51600, Loss: 0.004401789512\n",
      "Iteration 51610, Loss: 0.005994331557\n",
      "Iteration 51620, Loss: 0.004597023129\n",
      "Iteration 51630, Loss: 0.004760359880\n",
      "Iteration 51640, Loss: 0.004495275207\n",
      "Iteration 51650, Loss: 0.004428393673\n",
      "Iteration 51660, Loss: 0.004354326054\n",
      "Iteration 51670, Loss: 0.004466805607\n",
      "Iteration 51680, Loss: 0.004732730798\n",
      "Iteration 51690, Loss: 0.008606906980\n",
      "Iteration 51700, Loss: 0.004637484904\n",
      "Iteration 51710, Loss: 0.004505808000\n",
      "Iteration 51720, Loss: 0.004444110207\n",
      "Iteration 51730, Loss: 0.004375402350\n",
      "Iteration 51740, Loss: 0.004342255648\n",
      "Iteration 51750, Loss: 0.004333604593\n",
      "Iteration 51760, Loss: 0.004352561664\n",
      "Iteration 51770, Loss: 0.005747050513\n",
      "Iteration 51780, Loss: 0.004884209950\n",
      "Iteration 51790, Loss: 0.004783955403\n",
      "Iteration 51800, Loss: 0.004418823868\n",
      "Iteration 51810, Loss: 0.004386742599\n",
      "Iteration 51820, Loss: 0.004328146577\n",
      "Iteration 51830, Loss: 0.004328663927\n",
      "Iteration 51840, Loss: 0.004336227197\n",
      "Iteration 51850, Loss: 0.005153711885\n",
      "Iteration 51860, Loss: 0.004385708366\n",
      "Iteration 51870, Loss: 0.005034446716\n",
      "Iteration 51880, Loss: 0.004360728897\n",
      "Iteration 51890, Loss: 0.004405491054\n",
      "Iteration 51900, Loss: 0.004354835022\n",
      "Iteration 51910, Loss: 0.004320050590\n",
      "Iteration 51920, Loss: 0.004310802557\n",
      "Iteration 51930, Loss: 0.004316947889\n",
      "Iteration 51940, Loss: 0.005697356537\n",
      "Iteration 51950, Loss: 0.005741751287\n",
      "Iteration 51960, Loss: 0.004446879029\n",
      "Iteration 51970, Loss: 0.004470367916\n",
      "Iteration 51980, Loss: 0.004484404344\n",
      "Iteration 51990, Loss: 0.004332682583\n",
      "Iteration 52000, Loss: 0.004314716440\n",
      "Iteration 52010, Loss: 0.004317657556\n",
      "Iteration 52020, Loss: 0.004309230950\n",
      "Iteration 52030, Loss: 0.004335294478\n",
      "Iteration 52040, Loss: 0.005663156509\n",
      "Iteration 52050, Loss: 0.004762399010\n",
      "Iteration 52060, Loss: 0.004788172897\n",
      "Iteration 52070, Loss: 0.004341198131\n",
      "Iteration 52080, Loss: 0.004387035966\n",
      "Iteration 52090, Loss: 0.004325186834\n",
      "Iteration 52100, Loss: 0.004311719444\n",
      "Iteration 52110, Loss: 0.004315891769\n",
      "Iteration 52120, Loss: 0.004352279473\n",
      "Iteration 52130, Loss: 0.007163055707\n",
      "Iteration 52140, Loss: 0.006990327965\n",
      "Iteration 52150, Loss: 0.004841827322\n",
      "Iteration 52160, Loss: 0.004356768914\n",
      "Iteration 52170, Loss: 0.004396380391\n",
      "Iteration 52180, Loss: 0.004316704348\n",
      "Iteration 52190, Loss: 0.004313213285\n",
      "Iteration 52200, Loss: 0.004309989978\n",
      "Iteration 52210, Loss: 0.004300786648\n",
      "Iteration 52220, Loss: 0.004302533343\n",
      "Iteration 52230, Loss: 0.004640732892\n",
      "Iteration 52240, Loss: 0.005895187613\n",
      "Iteration 52250, Loss: 0.005504475906\n",
      "Iteration 52260, Loss: 0.004465143196\n",
      "Iteration 52270, Loss: 0.004339434206\n",
      "Iteration 52280, Loss: 0.004329527263\n",
      "Iteration 52290, Loss: 0.004325239453\n",
      "Iteration 52300, Loss: 0.004302509129\n",
      "Iteration 52310, Loss: 0.004303322174\n",
      "Iteration 52320, Loss: 0.004308895674\n",
      "Iteration 52330, Loss: 0.004914896563\n",
      "Iteration 52340, Loss: 0.004933355376\n",
      "Iteration 52350, Loss: 0.005097960588\n",
      "Iteration 52360, Loss: 0.004420886282\n",
      "Iteration 52370, Loss: 0.004376853816\n",
      "Iteration 52380, Loss: 0.004343834240\n",
      "Iteration 52390, Loss: 0.004305097740\n",
      "Iteration 52400, Loss: 0.004320427310\n",
      "Iteration 52410, Loss: 0.004433503374\n",
      "Iteration 52420, Loss: 0.007222580723\n",
      "Iteration 52430, Loss: 0.005296542775\n",
      "Iteration 52440, Loss: 0.004445211962\n",
      "Iteration 52450, Loss: 0.004305002745\n",
      "Iteration 52460, Loss: 0.004309760407\n",
      "Iteration 52470, Loss: 0.004331202246\n",
      "Iteration 52480, Loss: 0.004428728949\n",
      "Iteration 52490, Loss: 0.006662367377\n",
      "Iteration 52500, Loss: 0.005276612006\n",
      "Iteration 52510, Loss: 0.004634606186\n",
      "Iteration 52520, Loss: 0.004379296675\n",
      "Iteration 52530, Loss: 0.004363355227\n",
      "Iteration 52540, Loss: 0.004305033479\n",
      "Iteration 52550, Loss: 0.004329060204\n",
      "Iteration 52560, Loss: 0.005754205864\n",
      "Iteration 52570, Loss: 0.005169864744\n",
      "Iteration 52580, Loss: 0.004751684610\n",
      "Iteration 52590, Loss: 0.004558228888\n",
      "Iteration 52600, Loss: 0.004334880970\n",
      "Iteration 52610, Loss: 0.004303836729\n",
      "Iteration 52620, Loss: 0.004308232572\n",
      "Iteration 52630, Loss: 0.004294888116\n",
      "Iteration 52640, Loss: 0.004308002070\n",
      "Iteration 52650, Loss: 0.004580854438\n",
      "Iteration 52660, Loss: 0.008428189903\n",
      "Iteration 52670, Loss: 0.005428863689\n",
      "Iteration 52680, Loss: 0.004406032152\n",
      "Iteration 52690, Loss: 0.004349606112\n",
      "Iteration 52700, Loss: 0.004326689988\n",
      "Iteration 52710, Loss: 0.004307655152\n",
      "Iteration 52720, Loss: 0.004296242725\n",
      "Iteration 52730, Loss: 0.004289136268\n",
      "Iteration 52740, Loss: 0.004340586253\n",
      "Iteration 52750, Loss: 0.007826992311\n",
      "Iteration 52760, Loss: 0.005919314455\n",
      "Iteration 52770, Loss: 0.004405127838\n",
      "Iteration 52780, Loss: 0.004439360462\n",
      "Iteration 52790, Loss: 0.004368618596\n",
      "Iteration 52800, Loss: 0.004307623021\n",
      "Iteration 52810, Loss: 0.004282692447\n",
      "Iteration 52820, Loss: 0.004305968992\n",
      "Iteration 52830, Loss: 0.005062969867\n",
      "Iteration 52840, Loss: 0.004414278548\n",
      "Iteration 52850, Loss: 0.004758099094\n",
      "Iteration 52860, Loss: 0.004406686872\n",
      "Iteration 52870, Loss: 0.004377465695\n",
      "Iteration 52880, Loss: 0.004303604364\n",
      "Iteration 52890, Loss: 0.004291122779\n",
      "Iteration 52900, Loss: 0.004282547627\n",
      "Iteration 52910, Loss: 0.004424283747\n",
      "Iteration 52920, Loss: 0.010442713276\n",
      "Iteration 52930, Loss: 0.005264551379\n",
      "Iteration 52940, Loss: 0.004516799469\n",
      "Iteration 52950, Loss: 0.004340107087\n",
      "Iteration 52960, Loss: 0.004376458470\n",
      "Iteration 52970, Loss: 0.004293743055\n",
      "Iteration 52980, Loss: 0.004285586067\n",
      "Iteration 52990, Loss: 0.004278922454\n",
      "Iteration 53000, Loss: 0.004273452330\n",
      "Iteration 53010, Loss: 0.004276735242\n",
      "Iteration 53020, Loss: 0.004471649416\n",
      "Iteration 53030, Loss: 0.006278176326\n",
      "Iteration 53040, Loss: 0.006514656357\n",
      "Iteration 53050, Loss: 0.004816131666\n",
      "Iteration 53060, Loss: 0.004356893245\n",
      "Iteration 53070, Loss: 0.004343216773\n",
      "Iteration 53080, Loss: 0.004309523385\n",
      "Iteration 53090, Loss: 0.004278334323\n",
      "Iteration 53100, Loss: 0.004277952015\n",
      "Iteration 53110, Loss: 0.004272950813\n",
      "Iteration 53120, Loss: 0.004288277123\n",
      "Iteration 53130, Loss: 0.005634320900\n",
      "Iteration 53140, Loss: 0.004939157050\n",
      "Iteration 53150, Loss: 0.004624225199\n",
      "Iteration 53160, Loss: 0.004484413657\n",
      "Iteration 53170, Loss: 0.004363974556\n",
      "Iteration 53180, Loss: 0.004287121817\n",
      "Iteration 53190, Loss: 0.004279909655\n",
      "Iteration 53200, Loss: 0.004273147788\n",
      "Iteration 53210, Loss: 0.004276079126\n",
      "Iteration 53220, Loss: 0.004916453268\n",
      "Iteration 53230, Loss: 0.004409742076\n",
      "Iteration 53240, Loss: 0.005071036518\n",
      "Iteration 53250, Loss: 0.004540149588\n",
      "Iteration 53260, Loss: 0.004368121270\n",
      "Iteration 53270, Loss: 0.004279737361\n",
      "Iteration 53280, Loss: 0.004274984822\n",
      "Iteration 53290, Loss: 0.004272880964\n",
      "Iteration 53300, Loss: 0.004277938046\n",
      "Iteration 53310, Loss: 0.004673465155\n",
      "Iteration 53320, Loss: 0.005666960962\n",
      "Iteration 53330, Loss: 0.004601069726\n",
      "Iteration 53340, Loss: 0.004489962012\n",
      "Iteration 53350, Loss: 0.004341050517\n",
      "Iteration 53360, Loss: 0.004273043945\n",
      "Iteration 53370, Loss: 0.004278861918\n",
      "Iteration 53380, Loss: 0.004267798271\n",
      "Iteration 53390, Loss: 0.004333797377\n",
      "Iteration 53400, Loss: 0.007849091664\n",
      "Iteration 53410, Loss: 0.005785548594\n",
      "Iteration 53420, Loss: 0.004581140820\n",
      "Iteration 53430, Loss: 0.004329534248\n",
      "Iteration 53440, Loss: 0.004339615349\n",
      "Iteration 53450, Loss: 0.004288506228\n",
      "Iteration 53460, Loss: 0.004273953382\n",
      "Iteration 53470, Loss: 0.004263271112\n",
      "Iteration 53480, Loss: 0.004497230519\n",
      "Iteration 53490, Loss: 0.008024127223\n",
      "Iteration 53500, Loss: 0.004682275932\n",
      "Iteration 53510, Loss: 0.004390973598\n",
      "Iteration 53520, Loss: 0.004420156591\n",
      "Iteration 53530, Loss: 0.004307178315\n",
      "Iteration 53540, Loss: 0.004279297777\n",
      "Iteration 53550, Loss: 0.004270938225\n",
      "Iteration 53560, Loss: 0.004261016846\n",
      "Iteration 53570, Loss: 0.004356321413\n",
      "Iteration 53580, Loss: 0.010064899921\n",
      "Iteration 53590, Loss: 0.004488791805\n",
      "Iteration 53600, Loss: 0.004605077673\n",
      "Iteration 53610, Loss: 0.004443882965\n",
      "Iteration 53620, Loss: 0.004310419317\n",
      "Iteration 53630, Loss: 0.004272086546\n",
      "Iteration 53640, Loss: 0.004276183434\n",
      "Iteration 53650, Loss: 0.004272798542\n",
      "Iteration 53660, Loss: 0.005008723121\n",
      "Iteration 53670, Loss: 0.005894161761\n",
      "Iteration 53680, Loss: 0.004587139003\n",
      "Iteration 53690, Loss: 0.004352008924\n",
      "Iteration 53700, Loss: 0.004282382783\n",
      "Iteration 53710, Loss: 0.004369613249\n",
      "Iteration 53720, Loss: 0.005670772865\n",
      "Iteration 53730, Loss: 0.004371623509\n",
      "Iteration 53740, Loss: 0.004401315004\n",
      "Iteration 53750, Loss: 0.004292846192\n",
      "Iteration 53760, Loss: 0.004285540897\n",
      "Iteration 53770, Loss: 0.004251413047\n",
      "Iteration 53780, Loss: 0.004323826637\n",
      "Iteration 53790, Loss: 0.010132769123\n",
      "Iteration 53800, Loss: 0.004462842364\n",
      "Iteration 53810, Loss: 0.004599142820\n",
      "Iteration 53820, Loss: 0.004537345376\n",
      "Iteration 53830, Loss: 0.004271440674\n",
      "Iteration 53840, Loss: 0.004267130513\n",
      "Iteration 53850, Loss: 0.004258272238\n",
      "Iteration 53860, Loss: 0.004259104840\n",
      "Iteration 53870, Loss: 0.004287854303\n",
      "Iteration 53880, Loss: 0.005604934879\n",
      "Iteration 53890, Loss: 0.004487296566\n",
      "Iteration 53900, Loss: 0.004742203746\n",
      "Iteration 53910, Loss: 0.004327198025\n",
      "Iteration 53920, Loss: 0.004322555382\n",
      "Iteration 53930, Loss: 0.004269578028\n",
      "Iteration 53940, Loss: 0.004254459869\n",
      "Iteration 53950, Loss: 0.004257910885\n",
      "Iteration 53960, Loss: 0.005696675740\n",
      "Iteration 53970, Loss: 0.005741387606\n",
      "Iteration 53980, Loss: 0.004376133904\n",
      "Iteration 53990, Loss: 0.004440007266\n",
      "Iteration 54000, Loss: 0.004344381392\n",
      "Iteration 54010, Loss: 0.004288305528\n",
      "Iteration 54020, Loss: 0.004255876876\n",
      "Iteration 54030, Loss: 0.004248179030\n",
      "Iteration 54040, Loss: 0.004239306320\n",
      "Iteration 54050, Loss: 0.004266555887\n",
      "Iteration 54060, Loss: 0.007202611305\n",
      "Iteration 54070, Loss: 0.006423987448\n",
      "Iteration 54080, Loss: 0.004634305835\n",
      "Iteration 54090, Loss: 0.004368433729\n",
      "Iteration 54100, Loss: 0.004313675687\n",
      "Iteration 54110, Loss: 0.004269840661\n",
      "Iteration 54120, Loss: 0.004254606552\n",
      "Iteration 54130, Loss: 0.004239569884\n",
      "Iteration 54140, Loss: 0.004237018526\n",
      "Iteration 54150, Loss: 0.004302493762\n",
      "Iteration 54160, Loss: 0.010543240234\n",
      "Iteration 54170, Loss: 0.004519540351\n",
      "Iteration 54180, Loss: 0.004678813741\n",
      "Iteration 54190, Loss: 0.004478189629\n",
      "Iteration 54200, Loss: 0.004273699597\n",
      "Iteration 54210, Loss: 0.004251677077\n",
      "Iteration 54220, Loss: 0.004242984578\n",
      "Iteration 54230, Loss: 0.004244343843\n",
      "Iteration 54240, Loss: 0.004252084065\n",
      "Iteration 54250, Loss: 0.004771012813\n",
      "Iteration 54260, Loss: 0.004991161637\n",
      "Iteration 54270, Loss: 0.004689176567\n",
      "Iteration 54280, Loss: 0.004360171501\n",
      "Iteration 54290, Loss: 0.004297636915\n",
      "Iteration 54300, Loss: 0.004264973104\n",
      "Iteration 54310, Loss: 0.004248755053\n",
      "Iteration 54320, Loss: 0.004259640351\n",
      "Iteration 54330, Loss: 0.005263125990\n",
      "Iteration 54340, Loss: 0.004355591722\n",
      "Iteration 54350, Loss: 0.004899070133\n",
      "Iteration 54360, Loss: 0.004359634593\n",
      "Iteration 54370, Loss: 0.004314570222\n",
      "Iteration 54380, Loss: 0.004242975265\n",
      "Iteration 54390, Loss: 0.004244927783\n",
      "Iteration 54400, Loss: 0.004254424479\n",
      "Iteration 54410, Loss: 0.005107241683\n",
      "Iteration 54420, Loss: 0.004301688168\n",
      "Iteration 54430, Loss: 0.004950261675\n",
      "Iteration 54440, Loss: 0.004332657438\n",
      "Iteration 54450, Loss: 0.004346765112\n",
      "Iteration 54460, Loss: 0.004261001479\n",
      "Iteration 54470, Loss: 0.004248872399\n",
      "Iteration 54480, Loss: 0.004225423094\n",
      "Iteration 54490, Loss: 0.004235667177\n",
      "Iteration 54500, Loss: 0.004518899601\n",
      "Iteration 54510, Loss: 0.009722440504\n",
      "Iteration 54520, Loss: 0.005864479113\n",
      "Iteration 54530, Loss: 0.004823663272\n",
      "Iteration 54540, Loss: 0.004396708217\n",
      "Iteration 54550, Loss: 0.004306145478\n",
      "Iteration 54560, Loss: 0.004257122055\n",
      "Iteration 54570, Loss: 0.004232434090\n",
      "Iteration 54580, Loss: 0.004228273872\n",
      "Iteration 54590, Loss: 0.004235255998\n",
      "Iteration 54600, Loss: 0.004553508013\n",
      "Iteration 54610, Loss: 0.006070965901\n",
      "Iteration 54620, Loss: 0.004321838263\n",
      "Iteration 54630, Loss: 0.004529531114\n",
      "Iteration 54640, Loss: 0.004290649202\n",
      "Iteration 54650, Loss: 0.004244119860\n",
      "Iteration 54660, Loss: 0.004252055194\n",
      "Iteration 54670, Loss: 0.004252702463\n",
      "Iteration 54680, Loss: 0.004638141487\n",
      "Iteration 54690, Loss: 0.005779374857\n",
      "Iteration 54700, Loss: 0.004300550092\n",
      "Iteration 54710, Loss: 0.004357557744\n",
      "Iteration 54720, Loss: 0.004268076736\n",
      "Iteration 54730, Loss: 0.004221774172\n",
      "Iteration 54740, Loss: 0.004254630301\n",
      "Iteration 54750, Loss: 0.004802458454\n",
      "Iteration 54760, Loss: 0.005177633371\n",
      "Iteration 54770, Loss: 0.004280293360\n",
      "Iteration 54780, Loss: 0.004342425149\n",
      "Iteration 54790, Loss: 0.004267377779\n",
      "Iteration 54800, Loss: 0.004273286555\n",
      "Iteration 54810, Loss: 0.004227135796\n",
      "Iteration 54820, Loss: 0.004258748144\n",
      "Iteration 54830, Loss: 0.008674797602\n",
      "Iteration 54840, Loss: 0.005958793219\n",
      "Iteration 54850, Loss: 0.004982876591\n",
      "Iteration 54860, Loss: 0.004342882894\n",
      "Iteration 54870, Loss: 0.004305772949\n",
      "Iteration 54880, Loss: 0.004232502077\n",
      "Iteration 54890, Loss: 0.004226987716\n",
      "Iteration 54900, Loss: 0.004227848258\n",
      "Iteration 54910, Loss: 0.004304083530\n",
      "Iteration 54920, Loss: 0.004556082655\n",
      "Iteration 54930, Loss: 0.005122090690\n",
      "Iteration 54940, Loss: 0.004373999778\n",
      "Iteration 54950, Loss: 0.004348725080\n",
      "Iteration 54960, Loss: 0.004419041332\n",
      "Iteration 54970, Loss: 0.004304362927\n",
      "Iteration 54980, Loss: 0.004224233329\n",
      "Iteration 54990, Loss: 0.004223234486\n",
      "Iteration 55000, Loss: 0.004929158371\n",
      "Iteration 55010, Loss: 0.004373694770\n",
      "Iteration 55020, Loss: 0.005053737201\n",
      "Iteration 55030, Loss: 0.004262666218\n",
      "Iteration 55040, Loss: 0.004346803296\n",
      "Iteration 55050, Loss: 0.004252223764\n",
      "Iteration 55060, Loss: 0.004214057233\n",
      "Iteration 55070, Loss: 0.004232407082\n",
      "Iteration 55080, Loss: 0.004868125543\n",
      "Iteration 55090, Loss: 0.004684341606\n",
      "Iteration 55100, Loss: 0.004618910141\n",
      "Iteration 55110, Loss: 0.004377437290\n",
      "Iteration 55120, Loss: 0.004252760671\n",
      "Iteration 55130, Loss: 0.004246397875\n",
      "Iteration 55140, Loss: 0.004208077211\n",
      "Iteration 55150, Loss: 0.004209740553\n",
      "Iteration 55160, Loss: 0.004592838231\n",
      "Iteration 55170, Loss: 0.005214870907\n",
      "Iteration 55180, Loss: 0.005449462682\n",
      "Iteration 55190, Loss: 0.004645654000\n",
      "Iteration 55200, Loss: 0.004224240314\n",
      "Iteration 55210, Loss: 0.004286243115\n",
      "Iteration 55220, Loss: 0.004242874682\n",
      "Iteration 55230, Loss: 0.004211578984\n",
      "Iteration 55240, Loss: 0.004214444663\n",
      "Iteration 55250, Loss: 0.004386919551\n",
      "Iteration 55260, Loss: 0.007413556799\n",
      "Iteration 55270, Loss: 0.005074881483\n",
      "Iteration 55280, Loss: 0.004274633247\n",
      "Iteration 55290, Loss: 0.004249232821\n",
      "Iteration 55300, Loss: 0.004219092429\n",
      "Iteration 55310, Loss: 0.004237431102\n",
      "Iteration 55320, Loss: 0.004377732985\n",
      "Iteration 55330, Loss: 0.006792014930\n",
      "Iteration 55340, Loss: 0.005179964937\n",
      "Iteration 55350, Loss: 0.004351883195\n",
      "Iteration 55360, Loss: 0.004226885270\n",
      "Iteration 55370, Loss: 0.004225971643\n",
      "Iteration 55380, Loss: 0.004221989773\n",
      "Iteration 55390, Loss: 0.004239816684\n",
      "Iteration 55400, Loss: 0.007878942415\n",
      "Iteration 55410, Loss: 0.006104461383\n",
      "Iteration 55420, Loss: 0.004830684979\n",
      "Iteration 55430, Loss: 0.004238511436\n",
      "Iteration 55440, Loss: 0.004317145329\n",
      "Iteration 55450, Loss: 0.004248976242\n",
      "Iteration 55460, Loss: 0.004220834468\n",
      "Iteration 55470, Loss: 0.004200574476\n",
      "Iteration 55480, Loss: 0.004286936484\n",
      "Iteration 55490, Loss: 0.004835700616\n",
      "Iteration 55500, Loss: 0.007056321949\n",
      "Iteration 55510, Loss: 0.004582017194\n",
      "Iteration 55520, Loss: 0.004270364530\n",
      "Iteration 55530, Loss: 0.004265364259\n",
      "Iteration 55540, Loss: 0.004201089032\n",
      "Iteration 55550, Loss: 0.004282473586\n",
      "Iteration 55560, Loss: 0.004911094904\n",
      "Iteration 55570, Loss: 0.004617790226\n",
      "Iteration 55580, Loss: 0.004287108779\n",
      "Iteration 55590, Loss: 0.004235665314\n",
      "Iteration 55600, Loss: 0.004311496858\n",
      "Iteration 55610, Loss: 0.004245712887\n",
      "Iteration 55620, Loss: 0.004860873334\n",
      "Iteration 55630, Loss: 0.004833139479\n",
      "Iteration 55640, Loss: 0.004344756249\n",
      "Iteration 55650, Loss: 0.004426565487\n",
      "Iteration 55660, Loss: 0.004275362007\n",
      "Iteration 55670, Loss: 0.004197124857\n",
      "Iteration 55680, Loss: 0.004271571524\n",
      "Iteration 55690, Loss: 0.005954160355\n",
      "Iteration 55700, Loss: 0.004737311974\n",
      "Iteration 55710, Loss: 0.004616859835\n",
      "Iteration 55720, Loss: 0.004326311406\n",
      "Iteration 55730, Loss: 0.004258244764\n",
      "Iteration 55740, Loss: 0.004202918150\n",
      "Iteration 55750, Loss: 0.004229201004\n",
      "Iteration 55760, Loss: 0.006717729382\n",
      "Iteration 55770, Loss: 0.006125794724\n",
      "Iteration 55780, Loss: 0.004249891266\n",
      "Iteration 55790, Loss: 0.004449010827\n",
      "Iteration 55800, Loss: 0.004220698029\n",
      "Iteration 55810, Loss: 0.004203054588\n",
      "Iteration 55820, Loss: 0.004204939120\n",
      "Iteration 55830, Loss: 0.004263916053\n",
      "Iteration 55840, Loss: 0.006516883150\n",
      "Iteration 55850, Loss: 0.004830068443\n",
      "Iteration 55860, Loss: 0.004633525852\n",
      "Iteration 55870, Loss: 0.004346478730\n",
      "Iteration 55880, Loss: 0.004275258631\n",
      "Iteration 55890, Loss: 0.004191690125\n",
      "Iteration 55900, Loss: 0.004185612779\n",
      "Iteration 55910, Loss: 0.004629488103\n",
      "Iteration 55920, Loss: 0.004797989503\n",
      "Iteration 55930, Loss: 0.005344866775\n",
      "Iteration 55940, Loss: 0.004568044096\n",
      "Iteration 55950, Loss: 0.004248633981\n",
      "Iteration 55960, Loss: 0.004231831990\n",
      "Iteration 55970, Loss: 0.004211523570\n",
      "Iteration 55980, Loss: 0.004184422549\n",
      "Iteration 55990, Loss: 0.004192817491\n",
      "Iteration 56000, Loss: 0.004262776114\n",
      "Iteration 56010, Loss: 0.005725388881\n",
      "Iteration 56020, Loss: 0.004866965115\n",
      "Iteration 56030, Loss: 0.004744484089\n",
      "Iteration 56040, Loss: 0.004215954337\n",
      "Iteration 56050, Loss: 0.004245600663\n",
      "Iteration 56060, Loss: 0.004199395888\n",
      "Iteration 56070, Loss: 0.004206478596\n",
      "Iteration 56080, Loss: 0.004249862861\n",
      "Iteration 56090, Loss: 0.006546773948\n",
      "Iteration 56100, Loss: 0.005180025008\n",
      "Iteration 56110, Loss: 0.004540245514\n",
      "Iteration 56120, Loss: 0.004199216142\n",
      "Iteration 56130, Loss: 0.004178971518\n",
      "Iteration 56140, Loss: 0.004219262861\n",
      "Iteration 56150, Loss: 0.004173065536\n",
      "Iteration 56160, Loss: 0.004172955640\n",
      "Iteration 56170, Loss: 0.004879209213\n",
      "Iteration 56180, Loss: 0.004640016705\n",
      "Iteration 56190, Loss: 0.004392400850\n",
      "Iteration 56200, Loss: 0.004338180181\n",
      "Iteration 56210, Loss: 0.004356833175\n",
      "Iteration 56220, Loss: 0.004189606290\n",
      "Iteration 56230, Loss: 0.004183374345\n",
      "Iteration 56240, Loss: 0.004170643631\n",
      "Iteration 56250, Loss: 0.004174788482\n",
      "Iteration 56260, Loss: 0.004208362196\n",
      "Iteration 56270, Loss: 0.005875003058\n",
      "Iteration 56280, Loss: 0.004652384203\n",
      "Iteration 56290, Loss: 0.004646631423\n",
      "Iteration 56300, Loss: 0.004278758075\n",
      "Iteration 56310, Loss: 0.004212668166\n",
      "Iteration 56320, Loss: 0.004207857884\n",
      "Iteration 56330, Loss: 0.004179059993\n",
      "Iteration 56340, Loss: 0.004306897987\n",
      "Iteration 56350, Loss: 0.009067909792\n",
      "Iteration 56360, Loss: 0.004555261694\n",
      "Iteration 56370, Loss: 0.004615094513\n",
      "Iteration 56380, Loss: 0.004196672700\n",
      "Iteration 56390, Loss: 0.004191598389\n",
      "Iteration 56400, Loss: 0.004175018054\n",
      "Iteration 56410, Loss: 0.004181278870\n",
      "Iteration 56420, Loss: 0.004350637086\n",
      "Iteration 56430, Loss: 0.007166404277\n",
      "Iteration 56440, Loss: 0.004934982862\n",
      "Iteration 56450, Loss: 0.004283356946\n",
      "Iteration 56460, Loss: 0.004282550421\n",
      "Iteration 56470, Loss: 0.004208817147\n",
      "Iteration 56480, Loss: 0.004190675914\n",
      "Iteration 56490, Loss: 0.004314643797\n",
      "Iteration 56500, Loss: 0.008233477362\n",
      "Iteration 56510, Loss: 0.004947583657\n",
      "Iteration 56520, Loss: 0.004344266839\n",
      "Iteration 56530, Loss: 0.004328948446\n",
      "Iteration 56540, Loss: 0.004218643997\n",
      "Iteration 56550, Loss: 0.004189370666\n",
      "Iteration 56560, Loss: 0.004169822671\n",
      "Iteration 56570, Loss: 0.004663935397\n",
      "Iteration 56580, Loss: 0.005274708848\n",
      "Iteration 56590, Loss: 0.004723807331\n",
      "Iteration 56600, Loss: 0.004320494831\n",
      "Iteration 56610, Loss: 0.004229592625\n",
      "Iteration 56620, Loss: 0.004185098689\n",
      "Iteration 56630, Loss: 0.004168605898\n",
      "Iteration 56640, Loss: 0.004156673327\n",
      "Iteration 56650, Loss: 0.004154915921\n",
      "Iteration 56660, Loss: 0.004409615882\n",
      "Iteration 56670, Loss: 0.005776605569\n",
      "Iteration 56680, Loss: 0.005159094930\n",
      "Iteration 56690, Loss: 0.004499292001\n",
      "Iteration 56700, Loss: 0.004345879424\n",
      "Iteration 56710, Loss: 0.004216573667\n",
      "Iteration 56720, Loss: 0.004181144293\n",
      "Iteration 56730, Loss: 0.004169016611\n",
      "Iteration 56740, Loss: 0.004157351330\n",
      "Iteration 56750, Loss: 0.004151462577\n",
      "Iteration 56760, Loss: 0.004150404129\n",
      "Iteration 56770, Loss: 0.004222051706\n",
      "Iteration 56780, Loss: 0.011622605845\n",
      "Iteration 56790, Loss: 0.004322055727\n",
      "Iteration 56800, Loss: 0.004301742651\n",
      "Iteration 56810, Loss: 0.004374722484\n",
      "Iteration 56820, Loss: 0.004192867316\n",
      "Iteration 56830, Loss: 0.004197662231\n",
      "Iteration 56840, Loss: 0.004170395900\n",
      "Iteration 56850, Loss: 0.004150149878\n",
      "Iteration 56860, Loss: 0.004162505269\n",
      "Iteration 56870, Loss: 0.004347926471\n",
      "Iteration 56880, Loss: 0.007668625098\n",
      "Iteration 56890, Loss: 0.004659348167\n",
      "Iteration 56900, Loss: 0.004256487824\n",
      "Iteration 56910, Loss: 0.004259756301\n",
      "Iteration 56920, Loss: 0.004204068333\n",
      "Iteration 56930, Loss: 0.004153985064\n",
      "Iteration 56940, Loss: 0.004174514674\n",
      "Iteration 56950, Loss: 0.004979909398\n",
      "Iteration 56960, Loss: 0.004286459647\n",
      "Iteration 56970, Loss: 0.004691946320\n",
      "Iteration 56980, Loss: 0.004325444810\n",
      "Iteration 56990, Loss: 0.004174649250\n",
      "Iteration 57000, Loss: 0.004169005901\n",
      "Iteration 57010, Loss: 0.004162310157\n",
      "Iteration 57020, Loss: 0.004246941768\n",
      "Iteration 57030, Loss: 0.007277132012\n",
      "Iteration 57040, Loss: 0.005286339205\n",
      "Iteration 57050, Loss: 0.004333256744\n",
      "Iteration 57060, Loss: 0.004213868175\n",
      "Iteration 57070, Loss: 0.004179218784\n",
      "Iteration 57080, Loss: 0.004185294732\n",
      "Iteration 57090, Loss: 0.004158067983\n",
      "Iteration 57100, Loss: 0.004629981238\n",
      "Iteration 57110, Loss: 0.005016785581\n",
      "Iteration 57120, Loss: 0.004973434843\n",
      "Iteration 57130, Loss: 0.004211525433\n",
      "Iteration 57140, Loss: 0.004267399665\n",
      "Iteration 57150, Loss: 0.004214125220\n",
      "Iteration 57160, Loss: 0.004170138855\n",
      "Iteration 57170, Loss: 0.004195933230\n",
      "Iteration 57180, Loss: 0.004709870089\n",
      "Iteration 57190, Loss: 0.004519899376\n",
      "Iteration 57200, Loss: 0.006885953248\n",
      "Iteration 57210, Loss: 0.004378009588\n",
      "Iteration 57220, Loss: 0.004421830177\n",
      "Iteration 57230, Loss: 0.004203493707\n",
      "Iteration 57240, Loss: 0.004180003423\n",
      "Iteration 57250, Loss: 0.004139873665\n",
      "Iteration 57260, Loss: 0.004174523987\n",
      "Iteration 57270, Loss: 0.004615994170\n",
      "Iteration 57280, Loss: 0.005497163162\n",
      "Iteration 57290, Loss: 0.004229499493\n",
      "Iteration 57300, Loss: 0.004245428834\n",
      "Iteration 57310, Loss: 0.004208989907\n",
      "Iteration 57320, Loss: 0.004176686052\n",
      "Iteration 57330, Loss: 0.004137173761\n",
      "Iteration 57340, Loss: 0.004138719756\n",
      "Iteration 57350, Loss: 0.005133010447\n",
      "Iteration 57360, Loss: 0.005982710049\n",
      "Iteration 57370, Loss: 0.004523663782\n",
      "Iteration 57380, Loss: 0.004301010165\n",
      "Iteration 57390, Loss: 0.004170585424\n",
      "Iteration 57400, Loss: 0.004227919038\n",
      "Iteration 57410, Loss: 0.004159614909\n",
      "Iteration 57420, Loss: 0.004145065788\n",
      "Iteration 57430, Loss: 0.004137290642\n",
      "Iteration 57440, Loss: 0.004135357216\n",
      "Iteration 57450, Loss: 0.004181093071\n",
      "Iteration 57460, Loss: 0.006976274308\n",
      "Iteration 57470, Loss: 0.006162768230\n",
      "Iteration 57480, Loss: 0.004731623456\n",
      "Iteration 57490, Loss: 0.004280380439\n",
      "Iteration 57500, Loss: 0.004180026241\n",
      "Iteration 57510, Loss: 0.004142616875\n",
      "Iteration 57520, Loss: 0.004139156081\n",
      "Iteration 57530, Loss: 0.004130870569\n",
      "Iteration 57540, Loss: 0.004125720821\n",
      "Iteration 57550, Loss: 0.004267870449\n",
      "Iteration 57560, Loss: 0.009329989552\n",
      "Iteration 57570, Loss: 0.005578991026\n",
      "Iteration 57580, Loss: 0.004660671111\n",
      "Iteration 57590, Loss: 0.004244028591\n",
      "Iteration 57600, Loss: 0.004208121914\n",
      "Iteration 57610, Loss: 0.004143042956\n",
      "Iteration 57620, Loss: 0.004130991176\n",
      "Iteration 57630, Loss: 0.004132393282\n",
      "Iteration 57640, Loss: 0.004124427680\n",
      "Iteration 57650, Loss: 0.004133869894\n",
      "Iteration 57660, Loss: 0.005359412171\n",
      "Iteration 57670, Loss: 0.005003900267\n",
      "Iteration 57680, Loss: 0.004382332787\n",
      "Iteration 57690, Loss: 0.004359167069\n",
      "Iteration 57700, Loss: 0.004219693132\n",
      "Iteration 57710, Loss: 0.004167465493\n",
      "Iteration 57720, Loss: 0.004139491823\n",
      "Iteration 57730, Loss: 0.004123763181\n",
      "Iteration 57740, Loss: 0.004124043044\n",
      "Iteration 57750, Loss: 0.004263172392\n",
      "Iteration 57760, Loss: 0.008337410167\n",
      "Iteration 57770, Loss: 0.004268723540\n",
      "Iteration 57780, Loss: 0.004490014631\n",
      "Iteration 57790, Loss: 0.004233847372\n",
      "Iteration 57800, Loss: 0.004143822007\n",
      "Iteration 57810, Loss: 0.004139555618\n",
      "Iteration 57820, Loss: 0.004131570924\n",
      "Iteration 57830, Loss: 0.004139502998\n",
      "Iteration 57840, Loss: 0.004995303694\n",
      "Iteration 57850, Loss: 0.004214192275\n",
      "Iteration 57860, Loss: 0.004957176745\n",
      "Iteration 57870, Loss: 0.004234908614\n",
      "Iteration 57880, Loss: 0.004183471669\n",
      "Iteration 57890, Loss: 0.004134861287\n",
      "Iteration 57900, Loss: 0.004122623242\n",
      "Iteration 57910, Loss: 0.004118024837\n",
      "Iteration 57920, Loss: 0.004136344418\n",
      "Iteration 57930, Loss: 0.007224751171\n",
      "Iteration 57940, Loss: 0.006651620846\n",
      "Iteration 57950, Loss: 0.004976207856\n",
      "Iteration 57960, Loss: 0.004326516297\n",
      "Iteration 57970, Loss: 0.004167285748\n",
      "Iteration 57980, Loss: 0.004141896497\n",
      "Iteration 57990, Loss: 0.004118687473\n",
      "Iteration 58000, Loss: 0.004124453291\n",
      "Iteration 58010, Loss: 0.004119643010\n",
      "Iteration 58020, Loss: 0.004359041341\n",
      "Iteration 58030, Loss: 0.007297967095\n",
      "Iteration 58040, Loss: 0.004292265512\n",
      "Iteration 58050, Loss: 0.004306665622\n",
      "Iteration 58060, Loss: 0.004253421444\n",
      "Iteration 58070, Loss: 0.004183116835\n",
      "Iteration 58080, Loss: 0.004123135470\n",
      "Iteration 58090, Loss: 0.004177145660\n",
      "Iteration 58100, Loss: 0.005871187896\n",
      "Iteration 58110, Loss: 0.004555200227\n",
      "Iteration 58120, Loss: 0.004643924069\n",
      "Iteration 58130, Loss: 0.004198072013\n",
      "Iteration 58140, Loss: 0.004165603779\n",
      "Iteration 58150, Loss: 0.004165926483\n",
      "Iteration 58160, Loss: 0.004179961979\n",
      "Iteration 58170, Loss: 0.005437149201\n",
      "Iteration 58180, Loss: 0.004282898735\n",
      "Iteration 58190, Loss: 0.004602827132\n",
      "Iteration 58200, Loss: 0.004255884327\n",
      "Iteration 58210, Loss: 0.004169203341\n",
      "Iteration 58220, Loss: 0.004135662690\n",
      "Iteration 58230, Loss: 0.004228162114\n",
      "Iteration 58240, Loss: 0.006766186561\n",
      "Iteration 58250, Loss: 0.005035943352\n",
      "Iteration 58260, Loss: 0.004437151365\n",
      "Iteration 58270, Loss: 0.004199664574\n",
      "Iteration 58280, Loss: 0.004184346180\n",
      "Iteration 58290, Loss: 0.004110980779\n",
      "Iteration 58300, Loss: 0.004106927663\n",
      "Iteration 58310, Loss: 0.004124355968\n",
      "Iteration 58320, Loss: 0.011982282624\n",
      "Iteration 58330, Loss: 0.004933778662\n",
      "Iteration 58340, Loss: 0.004300854634\n",
      "Iteration 58350, Loss: 0.004209976643\n",
      "Iteration 58360, Loss: 0.004205098841\n",
      "Iteration 58370, Loss: 0.004140779842\n",
      "Iteration 58380, Loss: 0.004126044456\n",
      "Iteration 58390, Loss: 0.004112016410\n",
      "Iteration 58400, Loss: 0.004102480132\n",
      "Iteration 58410, Loss: 0.004100179300\n",
      "Iteration 58420, Loss: 0.004098151810\n",
      "Iteration 58430, Loss: 0.004253437277\n",
      "Iteration 58440, Loss: 0.008717928082\n",
      "Iteration 58450, Loss: 0.005803329870\n",
      "Iteration 58460, Loss: 0.004670388531\n",
      "Iteration 58470, Loss: 0.004328147974\n",
      "Iteration 58480, Loss: 0.004204928875\n",
      "Iteration 58490, Loss: 0.004119093996\n",
      "Iteration 58500, Loss: 0.004113059025\n",
      "Iteration 58510, Loss: 0.004105745349\n",
      "Iteration 58520, Loss: 0.004097925499\n",
      "Iteration 58530, Loss: 0.004098869860\n",
      "Iteration 58540, Loss: 0.004128228873\n",
      "Iteration 58550, Loss: 0.005433205515\n",
      "Iteration 58560, Loss: 0.004443095531\n",
      "Iteration 58570, Loss: 0.004719486926\n",
      "Iteration 58580, Loss: 0.004184767604\n",
      "Iteration 58590, Loss: 0.004143147264\n",
      "Iteration 58600, Loss: 0.004106991459\n",
      "Iteration 58610, Loss: 0.004111437127\n",
      "Iteration 58620, Loss: 0.004101688508\n",
      "Iteration 58630, Loss: 0.004297183361\n",
      "Iteration 58640, Loss: 0.007917452604\n",
      "Iteration 58650, Loss: 0.004499080591\n",
      "Iteration 58660, Loss: 0.004201880191\n",
      "Iteration 58670, Loss: 0.004297926556\n",
      "Iteration 58680, Loss: 0.004118436482\n",
      "Iteration 58690, Loss: 0.004112229217\n",
      "Iteration 58700, Loss: 0.004110994283\n",
      "Iteration 58710, Loss: 0.004113871139\n",
      "Iteration 58720, Loss: 0.004579058383\n",
      "Iteration 58730, Loss: 0.005142610986\n",
      "Iteration 58740, Loss: 0.004444423597\n",
      "Iteration 58750, Loss: 0.004309830256\n",
      "Iteration 58760, Loss: 0.004159109201\n",
      "Iteration 58770, Loss: 0.004129037261\n",
      "Iteration 58780, Loss: 0.004111219198\n",
      "Iteration 58790, Loss: 0.004164905287\n",
      "Iteration 58800, Loss: 0.006288847886\n",
      "Iteration 58810, Loss: 0.005242075305\n",
      "Iteration 58820, Loss: 0.004426050466\n",
      "Iteration 58830, Loss: 0.004181796219\n",
      "Iteration 58840, Loss: 0.004127375316\n",
      "Iteration 58850, Loss: 0.004118839279\n",
      "Iteration 58860, Loss: 0.004096870311\n",
      "Iteration 58870, Loss: 0.004200776573\n",
      "Iteration 58880, Loss: 0.010252859443\n",
      "Iteration 58890, Loss: 0.004292189144\n",
      "Iteration 58900, Loss: 0.004439915530\n",
      "Iteration 58910, Loss: 0.004275264684\n",
      "Iteration 58920, Loss: 0.004146926105\n",
      "Iteration 58930, Loss: 0.004101993982\n",
      "Iteration 58940, Loss: 0.004098684527\n",
      "Iteration 58950, Loss: 0.004121110309\n",
      "Iteration 58960, Loss: 0.004937725607\n",
      "Iteration 58970, Loss: 0.004317819141\n",
      "Iteration 58980, Loss: 0.004534191918\n",
      "Iteration 58990, Loss: 0.004277397878\n",
      "Iteration 59000, Loss: 0.004152933601\n",
      "Iteration 59010, Loss: 0.004136438016\n",
      "Iteration 59020, Loss: 0.004090669099\n",
      "Iteration 59030, Loss: 0.004083644599\n",
      "Iteration 59040, Loss: 0.004092425574\n",
      "Iteration 59050, Loss: 0.008349974640\n",
      "Iteration 59060, Loss: 0.005762080196\n",
      "Iteration 59070, Loss: 0.004393226467\n",
      "Iteration 59080, Loss: 0.004225498065\n",
      "Iteration 59090, Loss: 0.004190700594\n",
      "Iteration 59100, Loss: 0.004142946564\n",
      "Iteration 59110, Loss: 0.004108368885\n",
      "Iteration 59120, Loss: 0.004093543161\n",
      "Iteration 59130, Loss: 0.004083619919\n",
      "Iteration 59140, Loss: 0.004083723295\n",
      "Iteration 59150, Loss: 0.004176895134\n",
      "Iteration 59160, Loss: 0.007965177298\n",
      "Iteration 59170, Loss: 0.005138528533\n",
      "Iteration 59180, Loss: 0.004465410486\n",
      "Iteration 59190, Loss: 0.004148081411\n",
      "Iteration 59200, Loss: 0.004124907311\n",
      "Iteration 59210, Loss: 0.004106399138\n",
      "Iteration 59220, Loss: 0.004087044857\n",
      "Iteration 59230, Loss: 0.004084829241\n",
      "Iteration 59240, Loss: 0.004493490793\n",
      "Iteration 59250, Loss: 0.005446100608\n",
      "Iteration 59260, Loss: 0.004853703082\n",
      "Iteration 59270, Loss: 0.004146291874\n",
      "Iteration 59280, Loss: 0.004239014350\n",
      "Iteration 59290, Loss: 0.004122537095\n",
      "Iteration 59300, Loss: 0.004096073564\n",
      "Iteration 59310, Loss: 0.004080807790\n",
      "Iteration 59320, Loss: 0.004081738181\n",
      "Iteration 59330, Loss: 0.004635612946\n",
      "Iteration 59340, Loss: 0.004602736328\n",
      "Iteration 59350, Loss: 0.005246613640\n",
      "Iteration 59360, Loss: 0.004468218889\n",
      "Iteration 59370, Loss: 0.004107213113\n",
      "Iteration 59380, Loss: 0.004132845905\n",
      "Iteration 59390, Loss: 0.004106876440\n",
      "Iteration 59400, Loss: 0.004078312311\n",
      "Iteration 59410, Loss: 0.004085205961\n",
      "Iteration 59420, Loss: 0.004314162768\n",
      "Iteration 59430, Loss: 0.007076666690\n",
      "Iteration 59440, Loss: 0.004275780171\n",
      "Iteration 59450, Loss: 0.004435868468\n",
      "Iteration 59460, Loss: 0.004102246836\n",
      "Iteration 59470, Loss: 0.004094566219\n",
      "Iteration 59480, Loss: 0.004073167220\n",
      "Iteration 59490, Loss: 0.004112469964\n",
      "Iteration 59500, Loss: 0.004559226334\n",
      "Iteration 59510, Loss: 0.005576551892\n",
      "Iteration 59520, Loss: 0.004231908824\n",
      "Iteration 59530, Loss: 0.004261694849\n",
      "Iteration 59540, Loss: 0.004158678465\n",
      "Iteration 59550, Loss: 0.004077113234\n",
      "Iteration 59560, Loss: 0.004074015655\n",
      "Iteration 59570, Loss: 0.004277657252\n",
      "Iteration 59580, Loss: 0.007373532280\n",
      "Iteration 59590, Loss: 0.005019644741\n",
      "Iteration 59600, Loss: 0.004213062115\n",
      "Iteration 59610, Loss: 0.004171490669\n",
      "Iteration 59620, Loss: 0.004118499346\n",
      "Iteration 59630, Loss: 0.004091109615\n",
      "Iteration 59640, Loss: 0.004076221958\n",
      "Iteration 59650, Loss: 0.004066301975\n",
      "Iteration 59660, Loss: 0.004064081237\n",
      "Iteration 59670, Loss: 0.004290014505\n",
      "Iteration 59680, Loss: 0.005906679668\n",
      "Iteration 59690, Loss: 0.005316668656\n",
      "Iteration 59700, Loss: 0.004271650221\n",
      "Iteration 59710, Loss: 0.004164089449\n",
      "Iteration 59720, Loss: 0.004123721737\n",
      "Iteration 59730, Loss: 0.004092363641\n",
      "Iteration 59740, Loss: 0.004070205614\n",
      "Iteration 59750, Loss: 0.004064701963\n",
      "Iteration 59760, Loss: 0.004062100314\n",
      "Iteration 59770, Loss: 0.004061278421\n",
      "Iteration 59780, Loss: 0.004405029118\n",
      "Iteration 59790, Loss: 0.005800934508\n",
      "Iteration 59800, Loss: 0.005589463748\n",
      "Iteration 59810, Loss: 0.004332690034\n",
      "Iteration 59820, Loss: 0.004134880845\n",
      "Iteration 59830, Loss: 0.004120293073\n",
      "Iteration 59840, Loss: 0.004099963699\n",
      "Iteration 59850, Loss: 0.004073108546\n",
      "Iteration 59860, Loss: 0.004068560433\n",
      "Iteration 59870, Loss: 0.004090517294\n",
      "Iteration 59880, Loss: 0.005615038332\n",
      "Iteration 59890, Loss: 0.004980326630\n",
      "Iteration 59900, Loss: 0.004430644680\n",
      "Iteration 59910, Loss: 0.004255925305\n",
      "Iteration 59920, Loss: 0.004138082732\n",
      "Iteration 59930, Loss: 0.004101430066\n",
      "Iteration 59940, Loss: 0.004079259001\n",
      "Iteration 59950, Loss: 0.004090109374\n",
      "Iteration 59960, Loss: 0.004637099337\n",
      "Iteration 59970, Loss: 0.004927339032\n",
      "Iteration 59980, Loss: 0.004228000995\n",
      "Iteration 59990, Loss: 0.004322193563\n",
      "Iteration 60000, Loss: 0.004135424737\n",
      "Iteration 60010, Loss: 0.004098086152\n",
      "Iteration 60020, Loss: 0.004071813542\n",
      "Iteration 60030, Loss: 0.004120888188\n",
      "Iteration 60040, Loss: 0.007609236520\n",
      "Iteration 60050, Loss: 0.005710989237\n",
      "Iteration 60060, Loss: 0.004425851163\n",
      "Iteration 60070, Loss: 0.004147699103\n",
      "Iteration 60080, Loss: 0.004097030498\n",
      "Iteration 60090, Loss: 0.004081601743\n",
      "Iteration 60100, Loss: 0.004067761824\n",
      "Iteration 60110, Loss: 0.004056192935\n",
      "Iteration 60120, Loss: 0.004293933976\n",
      "Iteration 60130, Loss: 0.007575795986\n",
      "Iteration 60140, Loss: 0.004802903160\n",
      "Iteration 60150, Loss: 0.004141807556\n",
      "Iteration 60160, Loss: 0.004221980926\n",
      "Iteration 60170, Loss: 0.004080048297\n",
      "Iteration 60180, Loss: 0.004079296719\n",
      "Iteration 60190, Loss: 0.004069596529\n",
      "Iteration 60200, Loss: 0.004100447521\n",
      "Iteration 60210, Loss: 0.005765204318\n",
      "Iteration 60220, Loss: 0.004610641394\n",
      "Iteration 60230, Loss: 0.004591270350\n",
      "Iteration 60240, Loss: 0.004132949747\n",
      "Iteration 60250, Loss: 0.004117364995\n",
      "Iteration 60260, Loss: 0.004094715230\n",
      "Iteration 60270, Loss: 0.004052123986\n",
      "Iteration 60280, Loss: 0.004071290605\n",
      "Iteration 60290, Loss: 0.006184979342\n",
      "Iteration 60300, Loss: 0.006319480948\n",
      "Iteration 60310, Loss: 0.004382480402\n",
      "Iteration 60320, Loss: 0.004156512674\n",
      "Iteration 60330, Loss: 0.004188539460\n",
      "Iteration 60340, Loss: 0.004062929191\n",
      "Iteration 60350, Loss: 0.004065123387\n",
      "Iteration 60360, Loss: 0.004051615950\n",
      "Iteration 60370, Loss: 0.004092145711\n",
      "Iteration 60380, Loss: 0.005731925368\n",
      "Iteration 60390, Loss: 0.004843549803\n",
      "Iteration 60400, Loss: 0.004404996522\n",
      "Iteration 60410, Loss: 0.004144924693\n",
      "Iteration 60420, Loss: 0.004138146993\n",
      "Iteration 60430, Loss: 0.004076523241\n",
      "Iteration 60440, Loss: 0.004056596663\n",
      "Iteration 60450, Loss: 0.004136470612\n",
      "Iteration 60460, Loss: 0.008039870299\n",
      "Iteration 60470, Loss: 0.005141020287\n",
      "Iteration 60480, Loss: 0.004460203927\n",
      "Iteration 60490, Loss: 0.004165796563\n",
      "Iteration 60500, Loss: 0.004089107737\n",
      "Iteration 60510, Loss: 0.004048014060\n",
      "Iteration 60520, Loss: 0.004063576926\n",
      "Iteration 60530, Loss: 0.004156114999\n",
      "Iteration 60540, Loss: 0.007420263253\n",
      "Iteration 60550, Loss: 0.005208631046\n",
      "Iteration 60560, Loss: 0.004194672685\n",
      "Iteration 60570, Loss: 0.004137837328\n",
      "Iteration 60580, Loss: 0.004087960813\n",
      "Iteration 60590, Loss: 0.004068172071\n",
      "Iteration 60600, Loss: 0.004087434616\n",
      "Iteration 60610, Loss: 0.006128371693\n",
      "Iteration 60620, Loss: 0.005561774597\n",
      "Iteration 60630, Loss: 0.004211717285\n",
      "Iteration 60640, Loss: 0.004291281570\n",
      "Iteration 60650, Loss: 0.004106885288\n",
      "Iteration 60660, Loss: 0.004064527340\n",
      "Iteration 60670, Loss: 0.004049275536\n",
      "Iteration 60680, Loss: 0.004041492473\n",
      "Iteration 60690, Loss: 0.004280225839\n",
      "Iteration 60700, Loss: 0.007846021093\n",
      "Iteration 60710, Loss: 0.005528517999\n",
      "Iteration 60720, Loss: 0.004276980646\n",
      "Iteration 60730, Loss: 0.004160867538\n",
      "Iteration 60740, Loss: 0.004081165418\n",
      "Iteration 60750, Loss: 0.004063025583\n",
      "Iteration 60760, Loss: 0.004039760679\n",
      "Iteration 60770, Loss: 0.004042896442\n",
      "Iteration 60780, Loss: 0.004141631536\n",
      "Iteration 60790, Loss: 0.007999233902\n",
      "Iteration 60800, Loss: 0.004724158905\n",
      "Iteration 60810, Loss: 0.004478053655\n",
      "Iteration 60820, Loss: 0.004085003398\n",
      "Iteration 60830, Loss: 0.004062031861\n",
      "Iteration 60840, Loss: 0.004045728128\n",
      "Iteration 60850, Loss: 0.004050494637\n",
      "Iteration 60860, Loss: 0.004099278245\n",
      "Iteration 60870, Loss: 0.006614313461\n",
      "Iteration 60880, Loss: 0.005188028328\n",
      "Iteration 60890, Loss: 0.004287810065\n",
      "Iteration 60900, Loss: 0.004157559015\n",
      "Iteration 60910, Loss: 0.004097169265\n",
      "Iteration 60920, Loss: 0.004040878266\n",
      "Iteration 60930, Loss: 0.004063811619\n",
      "Iteration 60940, Loss: 0.004667449743\n",
      "Iteration 60950, Loss: 0.004583660048\n",
      "Iteration 60960, Loss: 0.004374380223\n",
      "Iteration 60970, Loss: 0.004283207003\n",
      "Iteration 60980, Loss: 0.004077044316\n",
      "Iteration 60990, Loss: 0.004061476327\n",
      "Iteration 61000, Loss: 0.004040909931\n",
      "Iteration 61010, Loss: 0.004049592186\n",
      "Iteration 61020, Loss: 0.006056573242\n",
      "Iteration 61030, Loss: 0.005565512460\n",
      "Iteration 61040, Loss: 0.004301354289\n",
      "Iteration 61050, Loss: 0.004313307349\n",
      "Iteration 61060, Loss: 0.004073389340\n",
      "Iteration 61070, Loss: 0.004058764316\n",
      "Iteration 61080, Loss: 0.004038820509\n",
      "Iteration 61090, Loss: 0.004032060970\n",
      "Iteration 61100, Loss: 0.004203480203\n",
      "Iteration 61110, Loss: 0.008823392913\n",
      "Iteration 61120, Loss: 0.004132953472\n",
      "Iteration 61130, Loss: 0.004388863221\n",
      "Iteration 61140, Loss: 0.004102435429\n",
      "Iteration 61150, Loss: 0.004112458322\n",
      "Iteration 61160, Loss: 0.004062645137\n",
      "Iteration 61170, Loss: 0.004031898919\n",
      "Iteration 61180, Loss: 0.004041063134\n",
      "Iteration 61190, Loss: 0.004060407635\n",
      "Iteration 61200, Loss: 0.005633732770\n",
      "Iteration 61210, Loss: 0.007088614628\n",
      "Iteration 61220, Loss: 0.005016211420\n",
      "Iteration 61230, Loss: 0.004401868675\n",
      "Iteration 61240, Loss: 0.004155863076\n",
      "Iteration 61250, Loss: 0.004063428845\n",
      "Iteration 61260, Loss: 0.004046414513\n",
      "Iteration 61270, Loss: 0.004028317984\n",
      "Iteration 61280, Loss: 0.004026243463\n",
      "Iteration 61290, Loss: 0.004021232482\n",
      "Iteration 61300, Loss: 0.004050954245\n",
      "Iteration 61310, Loss: 0.007192181889\n",
      "Iteration 61320, Loss: 0.006206608377\n",
      "Iteration 61330, Loss: 0.004523961805\n",
      "Iteration 61340, Loss: 0.004116319120\n",
      "Iteration 61350, Loss: 0.004109554924\n",
      "Iteration 61360, Loss: 0.004050651100\n",
      "Iteration 61370, Loss: 0.004040875938\n",
      "Iteration 61380, Loss: 0.004024516325\n",
      "Iteration 61390, Loss: 0.004018229432\n",
      "Iteration 61400, Loss: 0.004185148980\n",
      "Iteration 61410, Loss: 0.007971787825\n",
      "Iteration 61420, Loss: 0.004866421688\n",
      "Iteration 61430, Loss: 0.004154385999\n",
      "Iteration 61440, Loss: 0.004110141657\n",
      "Iteration 61450, Loss: 0.004069804214\n",
      "Iteration 61460, Loss: 0.004040246364\n",
      "Iteration 61470, Loss: 0.004025086295\n",
      "Iteration 61480, Loss: 0.004030361306\n",
      "Iteration 61490, Loss: 0.004182088654\n",
      "Iteration 61500, Loss: 0.007922089659\n",
      "Iteration 61510, Loss: 0.004659369588\n",
      "Iteration 61520, Loss: 0.004193440080\n",
      "Iteration 61530, Loss: 0.004124959931\n",
      "Iteration 61540, Loss: 0.004043151625\n",
      "Iteration 61550, Loss: 0.004020119552\n",
      "Iteration 61560, Loss: 0.004028135911\n",
      "Iteration 61570, Loss: 0.004732883070\n",
      "Iteration 61580, Loss: 0.004208100960\n",
      "Iteration 61590, Loss: 0.004968973808\n",
      "Iteration 61600, Loss: 0.004141511396\n",
      "Iteration 61610, Loss: 0.004139637575\n",
      "Iteration 61620, Loss: 0.004046659451\n",
      "Iteration 61630, Loss: 0.004031951539\n",
      "Iteration 61640, Loss: 0.004037144128\n",
      "Iteration 61650, Loss: 0.004393018782\n",
      "Iteration 61660, Loss: 0.005607329309\n",
      "Iteration 61670, Loss: 0.004180905875\n",
      "Iteration 61680, Loss: 0.004327084869\n",
      "Iteration 61690, Loss: 0.004056878854\n",
      "Iteration 61700, Loss: 0.004028930329\n",
      "Iteration 61710, Loss: 0.004049788695\n",
      "Iteration 61720, Loss: 0.004040926229\n",
      "Iteration 61730, Loss: 0.005934034940\n",
      "Iteration 61740, Loss: 0.005582449492\n",
      "Iteration 61750, Loss: 0.004211959895\n",
      "Iteration 61760, Loss: 0.004216781817\n",
      "Iteration 61770, Loss: 0.004085264169\n",
      "Iteration 61780, Loss: 0.004054301884\n",
      "Iteration 61790, Loss: 0.004020541906\n",
      "Iteration 61800, Loss: 0.004158443771\n",
      "Iteration 61810, Loss: 0.007655964233\n",
      "Iteration 61820, Loss: 0.005003830418\n",
      "Iteration 61830, Loss: 0.004044929985\n",
      "Iteration 61840, Loss: 0.004052693956\n",
      "Iteration 61850, Loss: 0.004019746091\n",
      "Iteration 61860, Loss: 0.004089931492\n",
      "Iteration 61870, Loss: 0.004661704414\n",
      "Iteration 61880, Loss: 0.004567543976\n",
      "Iteration 61890, Loss: 0.004059935920\n",
      "Iteration 61900, Loss: 0.004090927076\n",
      "Iteration 61910, Loss: 0.004042578395\n",
      "Iteration 61920, Loss: 0.004065428860\n",
      "Iteration 61930, Loss: 0.004487856291\n",
      "Iteration 61940, Loss: 0.005495271645\n",
      "Iteration 61950, Loss: 0.004139612895\n",
      "Iteration 61960, Loss: 0.004079389852\n",
      "Iteration 61970, Loss: 0.004049820360\n",
      "Iteration 61980, Loss: 0.004040337633\n",
      "Iteration 61990, Loss: 0.004079889040\n",
      "Iteration 62000, Loss: 0.005681828596\n",
      "Iteration 62010, Loss: 0.004974549171\n",
      "Iteration 62020, Loss: 0.004589037970\n",
      "Iteration 62030, Loss: 0.004187854473\n",
      "Iteration 62040, Loss: 0.004097373690\n",
      "Iteration 62050, Loss: 0.004026781768\n",
      "Iteration 62060, Loss: 0.004043360241\n",
      "Iteration 62070, Loss: 0.004698387813\n",
      "Iteration 62080, Loss: 0.004353749100\n",
      "Iteration 62090, Loss: 0.004363518674\n",
      "Iteration 62100, Loss: 0.004269604571\n",
      "Iteration 62110, Loss: 0.004058245569\n",
      "Iteration 62120, Loss: 0.004066722468\n",
      "Iteration 62130, Loss: 0.004035873339\n",
      "Iteration 62140, Loss: 0.004342304543\n",
      "Iteration 62150, Loss: 0.006041751709\n",
      "Iteration 62160, Loss: 0.005213135853\n",
      "Iteration 62170, Loss: 0.004155061673\n",
      "Iteration 62180, Loss: 0.004252636805\n",
      "Iteration 62190, Loss: 0.004065025132\n",
      "Iteration 62200, Loss: 0.004026744515\n",
      "Iteration 62210, Loss: 0.004015125334\n",
      "Iteration 62220, Loss: 0.004123542923\n",
      "Iteration 62230, Loss: 0.007378888316\n",
      "Iteration 62240, Loss: 0.005131791346\n",
      "Iteration 62250, Loss: 0.004075298551\n",
      "Iteration 62260, Loss: 0.004171399865\n",
      "Iteration 62270, Loss: 0.004068291280\n",
      "Iteration 62280, Loss: 0.004006912932\n",
      "Iteration 62290, Loss: 0.004055507947\n",
      "Iteration 62300, Loss: 0.005919932388\n",
      "Iteration 62310, Loss: 0.005007829517\n",
      "Iteration 62320, Loss: 0.004374484532\n",
      "Iteration 62330, Loss: 0.004040159751\n",
      "Iteration 62340, Loss: 0.004059003666\n",
      "Iteration 62350, Loss: 0.004005637486\n",
      "Iteration 62360, Loss: 0.004029866308\n",
      "Iteration 62370, Loss: 0.004954771139\n",
      "Iteration 62380, Loss: 0.004224610049\n",
      "Iteration 62390, Loss: 0.004522653297\n",
      "Iteration 62400, Loss: 0.004175518174\n",
      "Iteration 62410, Loss: 0.004051488824\n",
      "Iteration 62420, Loss: 0.004055827856\n",
      "Iteration 62430, Loss: 0.003997103777\n",
      "Iteration 62440, Loss: 0.004015880637\n",
      "Iteration 62450, Loss: 0.007875674404\n",
      "Iteration 62460, Loss: 0.006019611843\n",
      "Iteration 62470, Loss: 0.004641912878\n",
      "Iteration 62480, Loss: 0.004169691354\n",
      "Iteration 62490, Loss: 0.004071277566\n",
      "Iteration 62500, Loss: 0.003999155480\n",
      "Iteration 62510, Loss: 0.004000312649\n",
      "Iteration 62520, Loss: 0.004000812303\n",
      "Iteration 62530, Loss: 0.004081640393\n",
      "Iteration 62540, Loss: 0.007454964332\n",
      "Iteration 62550, Loss: 0.005221709143\n",
      "Iteration 62560, Loss: 0.004199511837\n",
      "Iteration 62570, Loss: 0.004150096793\n",
      "Iteration 62580, Loss: 0.004067780450\n",
      "Iteration 62590, Loss: 0.004009587690\n",
      "Iteration 62600, Loss: 0.004056303762\n",
      "Iteration 62610, Loss: 0.005040616263\n",
      "Iteration 62620, Loss: 0.004023185931\n",
      "Iteration 62630, Loss: 0.004291422199\n",
      "Iteration 62640, Loss: 0.004221505485\n",
      "Iteration 62650, Loss: 0.004090779461\n",
      "Iteration 62660, Loss: 0.003989374265\n",
      "Iteration 62670, Loss: 0.004135346971\n",
      "Iteration 62680, Loss: 0.007424612530\n",
      "Iteration 62690, Loss: 0.004956075456\n",
      "Iteration 62700, Loss: 0.004077112302\n",
      "Iteration 62710, Loss: 0.004035457037\n",
      "Iteration 62720, Loss: 0.004005863331\n",
      "Iteration 62730, Loss: 0.004040981177\n",
      "Iteration 62740, Loss: 0.004243005998\n",
      "Iteration 62750, Loss: 0.006680609658\n",
      "Iteration 62760, Loss: 0.004657747224\n",
      "Iteration 62770, Loss: 0.004015672486\n",
      "Iteration 62780, Loss: 0.004011648241\n",
      "Iteration 62790, Loss: 0.003996831831\n",
      "Iteration 62800, Loss: 0.004034986254\n",
      "Iteration 62810, Loss: 0.005258952267\n",
      "Iteration 62820, Loss: 0.004526320845\n",
      "Iteration 62830, Loss: 0.004608760588\n",
      "Iteration 62840, Loss: 0.004107862245\n",
      "Iteration 62850, Loss: 0.004017234314\n",
      "Iteration 62860, Loss: 0.003990448546\n",
      "Iteration 62870, Loss: 0.004000577144\n",
      "Iteration 62880, Loss: 0.004475593101\n",
      "Iteration 62890, Loss: 0.005065479316\n",
      "Iteration 62900, Loss: 0.004572248086\n",
      "Iteration 62910, Loss: 0.004104387946\n",
      "Iteration 62920, Loss: 0.004015598446\n",
      "Iteration 62930, Loss: 0.003996537067\n",
      "Iteration 62940, Loss: 0.004000380170\n",
      "Iteration 62950, Loss: 0.004001034889\n",
      "Iteration 62960, Loss: 0.005688812584\n",
      "Iteration 62970, Loss: 0.005214002449\n",
      "Iteration 62980, Loss: 0.004442519043\n",
      "Iteration 62990, Loss: 0.004153853282\n",
      "Iteration 63000, Loss: 0.004055304453\n",
      "Iteration 63010, Loss: 0.004018219654\n",
      "Iteration 63020, Loss: 0.003991898149\n",
      "Iteration 63030, Loss: 0.004068069160\n",
      "Iteration 63040, Loss: 0.006225422025\n",
      "Iteration 63050, Loss: 0.005087072030\n",
      "Iteration 63060, Loss: 0.004319327883\n",
      "Iteration 63070, Loss: 0.004020415246\n",
      "Iteration 63080, Loss: 0.003997366410\n",
      "Iteration 63090, Loss: 0.004042026587\n",
      "Iteration 63100, Loss: 0.004110037349\n",
      "Iteration 63110, Loss: 0.007063249592\n",
      "Iteration 63120, Loss: 0.005040635355\n",
      "Iteration 63130, Loss: 0.004147221334\n",
      "Iteration 63140, Loss: 0.004007892683\n",
      "Iteration 63150, Loss: 0.003987573087\n",
      "Iteration 63160, Loss: 0.004022883251\n",
      "Iteration 63170, Loss: 0.004479368217\n",
      "Iteration 63180, Loss: 0.005212506279\n",
      "Iteration 63190, Loss: 0.004027098883\n",
      "Iteration 63200, Loss: 0.004187160637\n",
      "Iteration 63210, Loss: 0.004069793969\n",
      "Iteration 63220, Loss: 0.003981487826\n",
      "Iteration 63230, Loss: 0.004121076316\n",
      "Iteration 63240, Loss: 0.006672131829\n",
      "Iteration 63250, Loss: 0.004832950886\n",
      "Iteration 63260, Loss: 0.004320729058\n",
      "Iteration 63270, Loss: 0.004052673467\n",
      "Iteration 63280, Loss: 0.004053645767\n",
      "Iteration 63290, Loss: 0.003988425247\n",
      "Iteration 63300, Loss: 0.004591498058\n",
      "Iteration 63310, Loss: 0.005913546775\n",
      "Iteration 63320, Loss: 0.004526488483\n",
      "Iteration 63330, Loss: 0.004198069684\n",
      "Iteration 63340, Loss: 0.004028846510\n",
      "Iteration 63350, Loss: 0.004016383085\n",
      "Iteration 63360, Loss: 0.003977008164\n",
      "Iteration 63370, Loss: 0.003969137091\n",
      "Iteration 63380, Loss: 0.004029449541\n",
      "Iteration 63390, Loss: 0.011882163584\n",
      "Iteration 63400, Loss: 0.005711291451\n",
      "Iteration 63410, Loss: 0.004689157940\n",
      "Iteration 63420, Loss: 0.004106336739\n",
      "Iteration 63430, Loss: 0.004070479888\n",
      "Iteration 63440, Loss: 0.004019137938\n",
      "Iteration 63450, Loss: 0.003993395716\n",
      "Iteration 63460, Loss: 0.003974717110\n",
      "Iteration 63470, Loss: 0.003967456054\n",
      "Iteration 63480, Loss: 0.003963274416\n",
      "Iteration 63490, Loss: 0.003963937517\n",
      "Iteration 63500, Loss: 0.004504836164\n",
      "Iteration 63510, Loss: 0.004265642725\n",
      "Iteration 63520, Loss: 0.004130914807\n",
      "Iteration 63530, Loss: 0.004032698460\n",
      "Iteration 63540, Loss: 0.004017358646\n",
      "Iteration 63550, Loss: 0.004048963543\n",
      "Iteration 63560, Loss: 0.003972299397\n",
      "Iteration 63570, Loss: 0.003969668876\n",
      "Iteration 63580, Loss: 0.003964023199\n",
      "Iteration 63590, Loss: 0.003962137271\n",
      "Iteration 63600, Loss: 0.003978376277\n",
      "Iteration 63610, Loss: 0.005215939600\n",
      "Iteration 63620, Loss: 0.004679200705\n",
      "Iteration 63630, Loss: 0.004430832807\n",
      "Iteration 63640, Loss: 0.004236980807\n",
      "Iteration 63650, Loss: 0.004017598461\n",
      "Iteration 63660, Loss: 0.003986551426\n",
      "Iteration 63670, Loss: 0.003970413469\n",
      "Iteration 63680, Loss: 0.003965822048\n",
      "Iteration 63690, Loss: 0.004081591498\n",
      "Iteration 63700, Loss: 0.007526720408\n",
      "Iteration 63710, Loss: 0.004783997778\n",
      "Iteration 63720, Loss: 0.004218022339\n",
      "Iteration 63730, Loss: 0.004098688252\n",
      "Iteration 63740, Loss: 0.003983115777\n",
      "Iteration 63750, Loss: 0.003978171386\n",
      "Iteration 63760, Loss: 0.003961664159\n",
      "Iteration 63770, Loss: 0.003970487043\n",
      "Iteration 63780, Loss: 0.005696280859\n",
      "Iteration 63790, Loss: 0.006485916674\n",
      "Iteration 63800, Loss: 0.004342394415\n",
      "Iteration 63810, Loss: 0.004055630416\n",
      "Iteration 63820, Loss: 0.004048122093\n",
      "Iteration 63830, Loss: 0.003992625512\n",
      "Iteration 63840, Loss: 0.003966836724\n",
      "Iteration 63850, Loss: 0.003963780589\n",
      "Iteration 63860, Loss: 0.003993189894\n",
      "Iteration 63870, Loss: 0.005489245057\n",
      "Iteration 63880, Loss: 0.004388337489\n",
      "Iteration 63890, Loss: 0.004517314956\n",
      "Iteration 63900, Loss: 0.004024764523\n",
      "Iteration 63910, Loss: 0.004058848601\n",
      "Iteration 63920, Loss: 0.003996479791\n",
      "Iteration 63930, Loss: 0.003978652880\n",
      "Iteration 63940, Loss: 0.004007608630\n",
      "Iteration 63950, Loss: 0.007332822308\n",
      "Iteration 63960, Loss: 0.005572533701\n",
      "Iteration 63970, Loss: 0.004121196922\n",
      "Iteration 63980, Loss: 0.004141528625\n",
      "Iteration 63990, Loss: 0.003992610611\n",
      "Iteration 64000, Loss: 0.003980373032\n",
      "Iteration 64010, Loss: 0.003979384899\n",
      "Iteration 64020, Loss: 0.004063373432\n",
      "Iteration 64030, Loss: 0.006916854531\n",
      "Iteration 64040, Loss: 0.005228796043\n",
      "Iteration 64050, Loss: 0.004039637744\n",
      "Iteration 64060, Loss: 0.004082067870\n",
      "Iteration 64070, Loss: 0.004020037595\n",
      "Iteration 64080, Loss: 0.003973686602\n",
      "Iteration 64090, Loss: 0.003963532392\n",
      "Iteration 64100, Loss: 0.004680643789\n",
      "Iteration 64110, Loss: 0.004339038860\n",
      "Iteration 64120, Loss: 0.004766910337\n",
      "Iteration 64130, Loss: 0.004079755396\n",
      "Iteration 64140, Loss: 0.004001993220\n",
      "Iteration 64150, Loss: 0.003973274957\n",
      "Iteration 64160, Loss: 0.003963232972\n",
      "Iteration 64170, Loss: 0.003977416083\n",
      "Iteration 64180, Loss: 0.005103908945\n",
      "Iteration 64190, Loss: 0.004187136889\n",
      "Iteration 64200, Loss: 0.004648080561\n",
      "Iteration 64210, Loss: 0.004058252554\n",
      "Iteration 64220, Loss: 0.004063398577\n",
      "Iteration 64230, Loss: 0.004001202062\n",
      "Iteration 64240, Loss: 0.003968041390\n",
      "Iteration 64250, Loss: 0.004019507207\n",
      "Iteration 64260, Loss: 0.006448198110\n",
      "Iteration 64270, Loss: 0.005010653753\n",
      "Iteration 64280, Loss: 0.004231012892\n",
      "Iteration 64290, Loss: 0.003976185340\n",
      "Iteration 64300, Loss: 0.003972894512\n",
      "Iteration 64310, Loss: 0.004012620077\n",
      "Iteration 64320, Loss: 0.004229554906\n",
      "Iteration 64330, Loss: 0.006913921796\n",
      "Iteration 64340, Loss: 0.004532448482\n",
      "Iteration 64350, Loss: 0.003968137782\n",
      "Iteration 64360, Loss: 0.003960226662\n",
      "Iteration 64370, Loss: 0.003981366754\n",
      "Iteration 64380, Loss: 0.003947230522\n",
      "Iteration 64390, Loss: 0.004294605926\n",
      "Iteration 64400, Loss: 0.005324204918\n",
      "Iteration 64410, Loss: 0.004806854296\n",
      "Iteration 64420, Loss: 0.004068838898\n",
      "Iteration 64430, Loss: 0.004076987505\n",
      "Iteration 64440, Loss: 0.003976847511\n",
      "Iteration 64450, Loss: 0.003948677331\n",
      "Iteration 64460, Loss: 0.003955902066\n",
      "Iteration 64470, Loss: 0.004104681313\n",
      "Iteration 64480, Loss: 0.007355772424\n",
      "Iteration 64490, Loss: 0.004688411951\n",
      "Iteration 64500, Loss: 0.004098840524\n",
      "Iteration 64510, Loss: 0.004104771186\n",
      "Iteration 64520, Loss: 0.004002575763\n",
      "Iteration 64530, Loss: 0.003949888516\n",
      "Iteration 64540, Loss: 0.004028430674\n",
      "Iteration 64550, Loss: 0.006213466637\n",
      "Iteration 64560, Loss: 0.005028241314\n",
      "Iteration 64570, Loss: 0.004266806412\n",
      "Iteration 64580, Loss: 0.003981402609\n",
      "Iteration 64590, Loss: 0.003973221406\n",
      "Iteration 64600, Loss: 0.003969800659\n",
      "Iteration 64610, Loss: 0.003966014832\n",
      "Iteration 64620, Loss: 0.005064474419\n",
      "Iteration 64630, Loss: 0.004419159144\n",
      "Iteration 64640, Loss: 0.004541114904\n",
      "Iteration 64650, Loss: 0.004097754136\n",
      "Iteration 64660, Loss: 0.003984838258\n",
      "Iteration 64670, Loss: 0.003957727458\n",
      "Iteration 64680, Loss: 0.003956347238\n",
      "Iteration 64690, Loss: 0.003987060394\n",
      "Iteration 64700, Loss: 0.005566889886\n",
      "Iteration 64710, Loss: 0.004603235051\n",
      "Iteration 64720, Loss: 0.004496315960\n",
      "Iteration 64730, Loss: 0.003982646391\n",
      "Iteration 64740, Loss: 0.003987984266\n",
      "Iteration 64750, Loss: 0.003948530648\n",
      "Iteration 64760, Loss: 0.003997662570\n",
      "Iteration 64770, Loss: 0.004913092125\n",
      "Iteration 64780, Loss: 0.004071699455\n",
      "Iteration 64790, Loss: 0.004171800334\n",
      "Iteration 64800, Loss: 0.004167707171\n",
      "Iteration 64810, Loss: 0.004029964563\n",
      "Iteration 64820, Loss: 0.003946728539\n",
      "Iteration 64830, Loss: 0.004063333850\n",
      "Iteration 64840, Loss: 0.007997023873\n",
      "Iteration 64850, Loss: 0.005094115622\n",
      "Iteration 64860, Loss: 0.004201833624\n",
      "Iteration 64870, Loss: 0.004058680497\n",
      "Iteration 64880, Loss: 0.003981259651\n",
      "Iteration 64890, Loss: 0.003963688388\n",
      "Iteration 64900, Loss: 0.003941434901\n",
      "Iteration 64910, Loss: 0.004078690894\n",
      "Iteration 64920, Loss: 0.008189594373\n",
      "Iteration 64930, Loss: 0.004512221087\n",
      "Iteration 64940, Loss: 0.004264609888\n",
      "Iteration 64950, Loss: 0.003984561190\n",
      "Iteration 64960, Loss: 0.003965667915\n",
      "Iteration 64970, Loss: 0.003934949636\n",
      "Iteration 64980, Loss: 0.003933223896\n",
      "Iteration 64990, Loss: 0.004068357404\n",
      "Iteration 65000, Loss: 0.010300891474\n",
      "Iteration 65010, Loss: 0.004678715486\n",
      "Iteration 65020, Loss: 0.003990463447\n",
      "Iteration 65030, Loss: 0.004078617785\n",
      "Iteration 65040, Loss: 0.003977844026\n",
      "Iteration 65050, Loss: 0.003933806904\n",
      "Iteration 65060, Loss: 0.003944462631\n",
      "Iteration 65070, Loss: 0.003950873856\n",
      "Iteration 65080, Loss: 0.005034961272\n",
      "Iteration 65090, Loss: 0.004093361553\n",
      "Iteration 65100, Loss: 0.004639208317\n",
      "Iteration 65110, Loss: 0.004039969295\n",
      "Iteration 65120, Loss: 0.004041264765\n",
      "Iteration 65130, Loss: 0.003960896283\n",
      "Iteration 65140, Loss: 0.003941629548\n",
      "Iteration 65150, Loss: 0.003966854420\n",
      "Iteration 65160, Loss: 0.005442520604\n",
      "Iteration 65170, Loss: 0.004547179211\n",
      "Iteration 65180, Loss: 0.004543825053\n",
      "Iteration 65190, Loss: 0.003975634463\n",
      "Iteration 65200, Loss: 0.003965198994\n",
      "Iteration 65210, Loss: 0.003929029685\n",
      "Iteration 65220, Loss: 0.003993989900\n",
      "Iteration 65230, Loss: 0.004671518691\n",
      "Iteration 65240, Loss: 0.004267234355\n",
      "Iteration 65250, Loss: 0.003967756405\n",
      "Iteration 65260, Loss: 0.004052589182\n",
      "Iteration 65270, Loss: 0.003968806937\n",
      "Iteration 65280, Loss: 0.003978692461\n",
      "Iteration 65290, Loss: 0.004132455215\n",
      "Iteration 65300, Loss: 0.007262562402\n",
      "Iteration 65310, Loss: 0.004536801949\n",
      "Iteration 65320, Loss: 0.004072631709\n",
      "Iteration 65330, Loss: 0.004087728448\n",
      "Iteration 65340, Loss: 0.003991871607\n",
      "Iteration 65350, Loss: 0.003927760292\n",
      "Iteration 65360, Loss: 0.003945585806\n",
      "Iteration 65370, Loss: 0.005722807720\n",
      "Iteration 65380, Loss: 0.006497864146\n",
      "Iteration 65390, Loss: 0.004519212060\n",
      "Iteration 65400, Loss: 0.004237995949\n",
      "Iteration 65410, Loss: 0.004041712731\n",
      "Iteration 65420, Loss: 0.003967093304\n",
      "Iteration 65430, Loss: 0.003930405714\n",
      "Iteration 65440, Loss: 0.003922281321\n",
      "Iteration 65450, Loss: 0.003922083415\n",
      "Iteration 65460, Loss: 0.004094926175\n",
      "Iteration 65470, Loss: 0.008763143793\n",
      "Iteration 65480, Loss: 0.004506957717\n",
      "Iteration 65490, Loss: 0.004232561681\n",
      "Iteration 65500, Loss: 0.004062050954\n",
      "Iteration 65510, Loss: 0.003940471914\n",
      "Iteration 65520, Loss: 0.003938737325\n",
      "Iteration 65530, Loss: 0.003925652709\n",
      "Iteration 65540, Loss: 0.003915803973\n",
      "Iteration 65550, Loss: 0.003938856069\n",
      "Iteration 65560, Loss: 0.007670071907\n",
      "Iteration 65570, Loss: 0.005949167535\n",
      "Iteration 65580, Loss: 0.004552807193\n",
      "Iteration 65590, Loss: 0.004137319978\n",
      "Iteration 65600, Loss: 0.003945736215\n",
      "Iteration 65610, Loss: 0.003947604913\n",
      "Iteration 65620, Loss: 0.003932125401\n",
      "Iteration 65630, Loss: 0.003918439616\n",
      "Iteration 65640, Loss: 0.003911057487\n",
      "Iteration 65650, Loss: 0.003911048640\n",
      "Iteration 65660, Loss: 0.004432274029\n",
      "Iteration 65670, Loss: 0.004597683903\n",
      "Iteration 65680, Loss: 0.004633978941\n",
      "Iteration 65690, Loss: 0.004167497624\n",
      "Iteration 65700, Loss: 0.003996745218\n",
      "Iteration 65710, Loss: 0.003982525785\n",
      "Iteration 65720, Loss: 0.003932357766\n",
      "Iteration 65730, Loss: 0.003918766510\n",
      "Iteration 65740, Loss: 0.003916024230\n",
      "Iteration 65750, Loss: 0.003912298940\n",
      "Iteration 65760, Loss: 0.003946812358\n",
      "Iteration 65770, Loss: 0.007236185484\n",
      "Iteration 65780, Loss: 0.005785685498\n",
      "Iteration 65790, Loss: 0.004221441224\n",
      "Iteration 65800, Loss: 0.003967251629\n",
      "Iteration 65810, Loss: 0.004018570296\n",
      "Iteration 65820, Loss: 0.003959442955\n",
      "Iteration 65830, Loss: 0.003933291882\n",
      "Iteration 65840, Loss: 0.003921140451\n",
      "Iteration 65850, Loss: 0.004055646248\n",
      "Iteration 65860, Loss: 0.008259139955\n",
      "Iteration 65870, Loss: 0.004911456723\n",
      "Iteration 65880, Loss: 0.004109206609\n",
      "Iteration 65890, Loss: 0.004083259497\n",
      "Iteration 65900, Loss: 0.003946875688\n",
      "Iteration 65910, Loss: 0.003945946693\n",
      "Iteration 65920, Loss: 0.003909290303\n",
      "Iteration 65930, Loss: 0.004001625348\n",
      "Iteration 65940, Loss: 0.008139057085\n",
      "Iteration 65950, Loss: 0.004583901260\n",
      "Iteration 65960, Loss: 0.004359792918\n",
      "Iteration 65970, Loss: 0.003931801300\n",
      "Iteration 65980, Loss: 0.003960401751\n",
      "Iteration 65990, Loss: 0.003928256221\n",
      "Iteration 66000, Loss: 0.003916155547\n",
      "Iteration 66010, Loss: 0.003974103369\n",
      "Iteration 66020, Loss: 0.006048361305\n",
      "Iteration 66030, Loss: 0.004792048596\n",
      "Iteration 66040, Loss: 0.004318858963\n",
      "Iteration 66050, Loss: 0.004053262528\n",
      "Iteration 66060, Loss: 0.004004344344\n",
      "Iteration 66070, Loss: 0.003944736905\n",
      "Iteration 66080, Loss: 0.003905340796\n",
      "Iteration 66090, Loss: 0.003931216896\n",
      "Iteration 66100, Loss: 0.006212164648\n",
      "Iteration 66110, Loss: 0.006180887111\n",
      "Iteration 66120, Loss: 0.004163935315\n",
      "Iteration 66130, Loss: 0.004029731732\n",
      "Iteration 66140, Loss: 0.003989661112\n",
      "Iteration 66150, Loss: 0.003935719375\n",
      "Iteration 66160, Loss: 0.003920605406\n",
      "Iteration 66170, Loss: 0.003912984859\n",
      "Iteration 66180, Loss: 0.004116849974\n",
      "Iteration 66190, Loss: 0.007481749635\n",
      "Iteration 66200, Loss: 0.004129211884\n",
      "Iteration 66210, Loss: 0.004326549824\n",
      "Iteration 66220, Loss: 0.003943411168\n",
      "Iteration 66230, Loss: 0.003920204472\n",
      "Iteration 66240, Loss: 0.003914361354\n",
      "Iteration 66250, Loss: 0.003914400935\n",
      "Iteration 66260, Loss: 0.004341340624\n",
      "Iteration 66270, Loss: 0.005391131155\n",
      "Iteration 66280, Loss: 0.004437205847\n",
      "Iteration 66290, Loss: 0.004102244042\n",
      "Iteration 66300, Loss: 0.003969009500\n",
      "Iteration 66310, Loss: 0.003944483586\n",
      "Iteration 66320, Loss: 0.003905451158\n",
      "Iteration 66330, Loss: 0.003928920254\n",
      "Iteration 66340, Loss: 0.004341720138\n",
      "Iteration 66350, Loss: 0.005442389287\n",
      "Iteration 66360, Loss: 0.004844842944\n",
      "Iteration 66370, Loss: 0.004115915857\n",
      "Iteration 66380, Loss: 0.003984003793\n",
      "Iteration 66390, Loss: 0.003926837351\n",
      "Iteration 66400, Loss: 0.003917397931\n",
      "Iteration 66410, Loss: 0.003899326548\n",
      "Iteration 66420, Loss: 0.003919459414\n",
      "Iteration 66430, Loss: 0.006851926912\n",
      "Iteration 66440, Loss: 0.005864118226\n",
      "Iteration 66450, Loss: 0.004198488779\n",
      "Iteration 66460, Loss: 0.004081559833\n",
      "Iteration 66470, Loss: 0.003978156950\n",
      "Iteration 66480, Loss: 0.003931378014\n",
      "Iteration 66490, Loss: 0.003906929400\n",
      "Iteration 66500, Loss: 0.003896299517\n",
      "Iteration 66510, Loss: 0.003895555157\n",
      "Iteration 66520, Loss: 0.004306454677\n",
      "Iteration 66530, Loss: 0.004567872267\n",
      "Iteration 66540, Loss: 0.004665761720\n",
      "Iteration 66550, Loss: 0.004072116688\n",
      "Iteration 66560, Loss: 0.004038264044\n",
      "Iteration 66570, Loss: 0.003913415130\n",
      "Iteration 66580, Loss: 0.003905356396\n",
      "Iteration 66590, Loss: 0.003900332376\n",
      "Iteration 66600, Loss: 0.003893345827\n",
      "Iteration 66610, Loss: 0.003905226942\n",
      "Iteration 66620, Loss: 0.005501991604\n",
      "Iteration 66630, Loss: 0.005769821350\n",
      "Iteration 66640, Loss: 0.004201170988\n",
      "Iteration 66650, Loss: 0.004161501769\n",
      "Iteration 66660, Loss: 0.003983511124\n",
      "Iteration 66670, Loss: 0.003947863821\n",
      "Iteration 66680, Loss: 0.003910472617\n",
      "Iteration 66690, Loss: 0.003896458773\n",
      "Iteration 66700, Loss: 0.003901434597\n",
      "Iteration 66710, Loss: 0.004782542586\n",
      "Iteration 66720, Loss: 0.004174537025\n",
      "Iteration 66730, Loss: 0.004545741715\n",
      "Iteration 66740, Loss: 0.004152740818\n",
      "Iteration 66750, Loss: 0.003927035723\n",
      "Iteration 66760, Loss: 0.003899372881\n",
      "Iteration 66770, Loss: 0.003893615678\n",
      "Iteration 66780, Loss: 0.003907049540\n",
      "Iteration 66790, Loss: 0.004193869885\n",
      "Iteration 66800, Loss: 0.006204783451\n",
      "Iteration 66810, Loss: 0.004056753125\n",
      "Iteration 66820, Loss: 0.004131151829\n",
      "Iteration 66830, Loss: 0.004019090906\n",
      "Iteration 66840, Loss: 0.003959694412\n",
      "Iteration 66850, Loss: 0.003892914392\n",
      "Iteration 66860, Loss: 0.003891049419\n",
      "Iteration 66870, Loss: 0.004676691722\n",
      "Iteration 66880, Loss: 0.004846080672\n",
      "Iteration 66890, Loss: 0.004583667498\n",
      "Iteration 66900, Loss: 0.004074045457\n",
      "Iteration 66910, Loss: 0.004022676032\n",
      "Iteration 66920, Loss: 0.003927041311\n",
      "Iteration 66930, Loss: 0.003897240851\n",
      "Iteration 66940, Loss: 0.003892110195\n",
      "Iteration 66950, Loss: 0.003888347885\n",
      "Iteration 66960, Loss: 0.004007664975\n",
      "Iteration 66970, Loss: 0.008530180901\n",
      "Iteration 66980, Loss: 0.004020579625\n",
      "Iteration 66990, Loss: 0.004206963349\n",
      "Iteration 67000, Loss: 0.003972681705\n",
      "Iteration 67010, Loss: 0.003931568004\n",
      "Iteration 67020, Loss: 0.003908338957\n",
      "Iteration 67030, Loss: 0.003885123180\n",
      "Iteration 67040, Loss: 0.003924891353\n",
      "Iteration 67050, Loss: 0.005872522481\n",
      "Iteration 67060, Loss: 0.005253795534\n",
      "Iteration 67070, Loss: 0.004214070272\n",
      "Iteration 67080, Loss: 0.004031518474\n",
      "Iteration 67090, Loss: 0.003946446814\n",
      "Iteration 67100, Loss: 0.003916583955\n",
      "Iteration 67110, Loss: 0.003880125703\n",
      "Iteration 67120, Loss: 0.003890683642\n",
      "Iteration 67130, Loss: 0.005508971866\n",
      "Iteration 67140, Loss: 0.005617579911\n",
      "Iteration 67150, Loss: 0.004433829803\n",
      "Iteration 67160, Loss: 0.004202730954\n",
      "Iteration 67170, Loss: 0.003983184695\n",
      "Iteration 67180, Loss: 0.003923925571\n",
      "Iteration 67190, Loss: 0.003895818256\n",
      "Iteration 67200, Loss: 0.003883196972\n",
      "Iteration 67210, Loss: 0.003887428902\n",
      "Iteration 67220, Loss: 0.004570694640\n",
      "Iteration 67230, Loss: 0.004149199929\n",
      "Iteration 67240, Loss: 0.004807701800\n",
      "Iteration 67250, Loss: 0.004047231283\n",
      "Iteration 67260, Loss: 0.003927969839\n",
      "Iteration 67270, Loss: 0.003889099928\n",
      "Iteration 67280, Loss: 0.003885158570\n",
      "Iteration 67290, Loss: 0.003894186812\n",
      "Iteration 67300, Loss: 0.004081020597\n",
      "Iteration 67310, Loss: 0.007606786676\n",
      "Iteration 67320, Loss: 0.004273592960\n",
      "Iteration 67330, Loss: 0.004124687519\n",
      "Iteration 67340, Loss: 0.003973078914\n",
      "Iteration 67350, Loss: 0.003900123294\n",
      "Iteration 67360, Loss: 0.003881590674\n",
      "Iteration 67370, Loss: 0.003890864318\n",
      "Iteration 67380, Loss: 0.004282066599\n",
      "Iteration 67390, Loss: 0.004675297067\n",
      "Iteration 67400, Loss: 0.004693053197\n",
      "Iteration 67410, Loss: 0.004058981314\n",
      "Iteration 67420, Loss: 0.003943415824\n",
      "Iteration 67430, Loss: 0.003912481479\n",
      "Iteration 67440, Loss: 0.003888415173\n",
      "Iteration 67450, Loss: 0.003884065198\n",
      "Iteration 67460, Loss: 0.003876130329\n",
      "Iteration 67470, Loss: 0.003880611388\n",
      "Iteration 67480, Loss: 0.004653515760\n",
      "Iteration 67490, Loss: 0.004905322567\n",
      "Iteration 67500, Loss: 0.004349942785\n",
      "Iteration 67510, Loss: 0.004000111949\n",
      "Iteration 67520, Loss: 0.003968517296\n",
      "Iteration 67530, Loss: 0.003894248512\n",
      "Iteration 67540, Loss: 0.003875733819\n",
      "Iteration 67550, Loss: 0.004057139624\n",
      "Iteration 67560, Loss: 0.008142864332\n",
      "Iteration 67570, Loss: 0.004264336079\n",
      "Iteration 67580, Loss: 0.004064849578\n",
      "Iteration 67590, Loss: 0.004030212294\n",
      "Iteration 67600, Loss: 0.003900174284\n",
      "Iteration 67610, Loss: 0.003883184865\n",
      "Iteration 67620, Loss: 0.003885400249\n",
      "Iteration 67630, Loss: 0.003885918530\n",
      "Iteration 67640, Loss: 0.004530895967\n",
      "Iteration 67650, Loss: 0.004086045548\n",
      "Iteration 67660, Loss: 0.004630689509\n",
      "Iteration 67670, Loss: 0.003915044945\n",
      "Iteration 67680, Loss: 0.003979445901\n",
      "Iteration 67690, Loss: 0.003917220514\n",
      "Iteration 67700, Loss: 0.003885277547\n",
      "Iteration 67710, Loss: 0.003923962358\n",
      "Iteration 67720, Loss: 0.005238309968\n",
      "Iteration 67730, Loss: 0.004036403727\n",
      "Iteration 67740, Loss: 0.004395521246\n",
      "Iteration 67750, Loss: 0.003976123407\n",
      "Iteration 67760, Loss: 0.003898177529\n",
      "Iteration 67770, Loss: 0.003899276257\n",
      "Iteration 67780, Loss: 0.003867291147\n",
      "Iteration 67790, Loss: 0.003865817096\n",
      "Iteration 67800, Loss: 0.005048512015\n",
      "Iteration 67810, Loss: 0.006417197641\n",
      "Iteration 67820, Loss: 0.004480261821\n",
      "Iteration 67830, Loss: 0.004030490294\n",
      "Iteration 67840, Loss: 0.003988961224\n",
      "Iteration 67850, Loss: 0.003897162154\n",
      "Iteration 67860, Loss: 0.003886427730\n",
      "Iteration 67870, Loss: 0.003876717761\n",
      "Iteration 67880, Loss: 0.003866719315\n",
      "Iteration 67890, Loss: 0.003862336744\n",
      "Iteration 67900, Loss: 0.003876275849\n",
      "Iteration 67910, Loss: 0.006335637067\n",
      "Iteration 67920, Loss: 0.007029986940\n",
      "Iteration 67930, Loss: 0.004503725562\n",
      "Iteration 67940, Loss: 0.004021328874\n",
      "Iteration 67950, Loss: 0.003915468697\n",
      "Iteration 67960, Loss: 0.003886735067\n",
      "Iteration 67970, Loss: 0.003877339885\n",
      "Iteration 67980, Loss: 0.003866188927\n",
      "Iteration 67990, Loss: 0.003858580254\n",
      "Iteration 68000, Loss: 0.003858134383\n",
      "Iteration 68010, Loss: 0.004173963331\n",
      "Iteration 68020, Loss: 0.004864613060\n",
      "Iteration 68030, Loss: 0.004581759684\n",
      "Iteration 68040, Loss: 0.004440843128\n",
      "Iteration 68050, Loss: 0.004016356543\n",
      "Iteration 68060, Loss: 0.003945047036\n",
      "Iteration 68070, Loss: 0.003888109932\n",
      "Iteration 68080, Loss: 0.003870104207\n",
      "Iteration 68090, Loss: 0.003860316472\n",
      "Iteration 68100, Loss: 0.003856624011\n",
      "Iteration 68110, Loss: 0.003860214027\n",
      "Iteration 68120, Loss: 0.004589674063\n",
      "Iteration 68130, Loss: 0.004112251569\n",
      "Iteration 68140, Loss: 0.004467861261\n",
      "Iteration 68150, Loss: 0.004269313533\n",
      "Iteration 68160, Loss: 0.003994879778\n",
      "Iteration 68170, Loss: 0.003907635808\n",
      "Iteration 68180, Loss: 0.003877089592\n",
      "Iteration 68190, Loss: 0.003864143277\n",
      "Iteration 68200, Loss: 0.003861623816\n",
      "Iteration 68210, Loss: 0.003987698350\n",
      "Iteration 68220, Loss: 0.007995047607\n",
      "Iteration 68230, Loss: 0.004195162095\n",
      "Iteration 68240, Loss: 0.004306310788\n",
      "Iteration 68250, Loss: 0.003890983760\n",
      "Iteration 68260, Loss: 0.003910284024\n",
      "Iteration 68270, Loss: 0.003881615819\n",
      "Iteration 68280, Loss: 0.003863849910\n",
      "Iteration 68290, Loss: 0.003857124131\n",
      "Iteration 68300, Loss: 0.003999430221\n",
      "Iteration 68310, Loss: 0.010189524852\n",
      "Iteration 68320, Loss: 0.005282641388\n",
      "Iteration 68330, Loss: 0.004246172961\n",
      "Iteration 68340, Loss: 0.003963185474\n",
      "Iteration 68350, Loss: 0.003899935167\n",
      "Iteration 68360, Loss: 0.003872582223\n",
      "Iteration 68370, Loss: 0.003864784027\n",
      "Iteration 68380, Loss: 0.003856068477\n",
      "Iteration 68390, Loss: 0.003851352958\n",
      "Iteration 68400, Loss: 0.003892913228\n",
      "Iteration 68410, Loss: 0.008628454059\n",
      "Iteration 68420, Loss: 0.005405777134\n",
      "Iteration 68430, Loss: 0.004367643502\n",
      "Iteration 68440, Loss: 0.003999839071\n",
      "Iteration 68450, Loss: 0.003937825095\n",
      "Iteration 68460, Loss: 0.003887418192\n",
      "Iteration 68470, Loss: 0.003857536009\n",
      "Iteration 68480, Loss: 0.003874486778\n",
      "Iteration 68490, Loss: 0.004769234918\n",
      "Iteration 68500, Loss: 0.005707651377\n",
      "Iteration 68510, Loss: 0.004365009256\n",
      "Iteration 68520, Loss: 0.004060151521\n",
      "Iteration 68530, Loss: 0.003916724585\n",
      "Iteration 68540, Loss: 0.004071131814\n",
      "Iteration 68550, Loss: 0.005863373168\n",
      "Iteration 68560, Loss: 0.004178625066\n",
      "Iteration 68570, Loss: 0.004030317999\n",
      "Iteration 68580, Loss: 0.003900982672\n",
      "Iteration 68590, Loss: 0.003902910277\n",
      "Iteration 68600, Loss: 0.003891724627\n",
      "Iteration 68610, Loss: 0.006206826307\n",
      "Iteration 68620, Loss: 0.005978250876\n",
      "Iteration 68630, Loss: 0.004256974440\n",
      "Iteration 68640, Loss: 0.004015737213\n",
      "Iteration 68650, Loss: 0.003922148142\n",
      "Iteration 68660, Loss: 0.003864549333\n",
      "Iteration 68670, Loss: 0.003856051946\n",
      "Iteration 68680, Loss: 0.003850962734\n",
      "Iteration 68690, Loss: 0.004096184857\n",
      "Iteration 68700, Loss: 0.006834311411\n",
      "Iteration 68710, Loss: 0.004622378387\n",
      "Iteration 68720, Loss: 0.004089914728\n",
      "Iteration 68730, Loss: 0.003978099208\n",
      "Iteration 68740, Loss: 0.003882603487\n",
      "Iteration 68750, Loss: 0.003868447384\n",
      "Iteration 68760, Loss: 0.003848711029\n",
      "Iteration 68770, Loss: 0.003854921088\n",
      "Iteration 68780, Loss: 0.005067477003\n",
      "Iteration 68790, Loss: 0.004812937696\n",
      "Iteration 68800, Loss: 0.004175614100\n",
      "Iteration 68810, Loss: 0.004056104459\n",
      "Iteration 68820, Loss: 0.003964781296\n",
      "Iteration 68830, Loss: 0.003869466251\n",
      "Iteration 68840, Loss: 0.003862132784\n",
      "Iteration 68850, Loss: 0.003858400276\n",
      "Iteration 68860, Loss: 0.004269582685\n",
      "Iteration 68870, Loss: 0.005248461850\n",
      "Iteration 68880, Loss: 0.004252277780\n",
      "Iteration 68890, Loss: 0.004044010770\n",
      "Iteration 68900, Loss: 0.003939598333\n",
      "Iteration 68910, Loss: 0.003875854891\n",
      "Iteration 68920, Loss: 0.003857403761\n",
      "Iteration 68930, Loss: 0.003865298815\n",
      "Iteration 68940, Loss: 0.004889144562\n",
      "Iteration 68950, Loss: 0.004262694623\n",
      "Iteration 68960, Loss: 0.004258539528\n",
      "Iteration 68970, Loss: 0.004051179625\n",
      "Iteration 68980, Loss: 0.003882235615\n",
      "Iteration 68990, Loss: 0.003865231527\n",
      "Iteration 69000, Loss: 0.003854413517\n",
      "Iteration 69010, Loss: 0.003841550322\n",
      "Iteration 69020, Loss: 0.003842794569\n",
      "Iteration 69030, Loss: 0.005465880036\n",
      "Iteration 69040, Loss: 0.006788838655\n",
      "Iteration 69050, Loss: 0.004803883377\n",
      "Iteration 69060, Loss: 0.003976910375\n",
      "Iteration 69070, Loss: 0.003911543172\n",
      "Iteration 69080, Loss: 0.003881741315\n",
      "Iteration 69090, Loss: 0.003860207275\n",
      "Iteration 69100, Loss: 0.003844859777\n",
      "Iteration 69110, Loss: 0.003839979880\n",
      "Iteration 69120, Loss: 0.003927172627\n",
      "Iteration 69130, Loss: 0.007609953173\n",
      "Iteration 69140, Loss: 0.004938340746\n",
      "Iteration 69150, Loss: 0.004081478342\n",
      "Iteration 69160, Loss: 0.003977853805\n",
      "Iteration 69170, Loss: 0.003874729620\n",
      "Iteration 69180, Loss: 0.003857563017\n",
      "Iteration 69190, Loss: 0.003834922099\n",
      "Iteration 69200, Loss: 0.003894839669\n",
      "Iteration 69210, Loss: 0.008989168331\n",
      "Iteration 69220, Loss: 0.004848584998\n",
      "Iteration 69230, Loss: 0.004182271659\n",
      "Iteration 69240, Loss: 0.003980942536\n",
      "Iteration 69250, Loss: 0.003862263635\n",
      "Iteration 69260, Loss: 0.003858011449\n",
      "Iteration 69270, Loss: 0.003839871148\n",
      "Iteration 69280, Loss: 0.003846788080\n",
      "Iteration 69290, Loss: 0.004500142299\n",
      "Iteration 69300, Loss: 0.004164119251\n",
      "Iteration 69310, Loss: 0.004533946980\n",
      "Iteration 69320, Loss: 0.003979116213\n",
      "Iteration 69330, Loss: 0.003880278207\n",
      "Iteration 69340, Loss: 0.003867215011\n",
      "Iteration 69350, Loss: 0.003845882136\n",
      "Iteration 69360, Loss: 0.003841919126\n",
      "Iteration 69370, Loss: 0.004369852133\n",
      "Iteration 69380, Loss: 0.004555436783\n",
      "Iteration 69390, Loss: 0.004751499277\n",
      "Iteration 69400, Loss: 0.004072945099\n",
      "Iteration 69410, Loss: 0.003893290414\n",
      "Iteration 69420, Loss: 0.003865807317\n",
      "Iteration 69430, Loss: 0.003849290079\n",
      "Iteration 69440, Loss: 0.003833374707\n",
      "Iteration 69450, Loss: 0.003829136025\n",
      "Iteration 69460, Loss: 0.004278895445\n",
      "Iteration 69470, Loss: 0.004491936415\n",
      "Iteration 69480, Loss: 0.004353540018\n",
      "Iteration 69490, Loss: 0.004155652132\n",
      "Iteration 69500, Loss: 0.003998816013\n",
      "Iteration 69510, Loss: 0.003878754796\n",
      "Iteration 69520, Loss: 0.003843761049\n",
      "Iteration 69530, Loss: 0.003834402421\n",
      "Iteration 69540, Loss: 0.003833432915\n",
      "Iteration 69550, Loss: 0.003898223862\n",
      "Iteration 69560, Loss: 0.006805812009\n",
      "Iteration 69570, Loss: 0.005356533919\n",
      "Iteration 69580, Loss: 0.003965589684\n",
      "Iteration 69590, Loss: 0.003973735031\n",
      "Iteration 69600, Loss: 0.003908748738\n",
      "Iteration 69610, Loss: 0.003867918393\n",
      "Iteration 69620, Loss: 0.003866475075\n",
      "Iteration 69630, Loss: 0.004446193110\n",
      "Iteration 69640, Loss: 0.004447698127\n",
      "Iteration 69650, Loss: 0.004177205730\n",
      "Iteration 69660, Loss: 0.004043404944\n",
      "Iteration 69670, Loss: 0.003871202469\n",
      "Iteration 69680, Loss: 0.003856149502\n",
      "Iteration 69690, Loss: 0.003824945772\n",
      "Iteration 69700, Loss: 0.003827002831\n",
      "Iteration 69710, Loss: 0.004014524166\n",
      "Iteration 69720, Loss: 0.007487902418\n",
      "Iteration 69730, Loss: 0.004769806284\n",
      "Iteration 69740, Loss: 0.004198527429\n",
      "Iteration 69750, Loss: 0.004022497684\n",
      "Iteration 69760, Loss: 0.003925493453\n",
      "Iteration 69770, Loss: 0.003873366164\n",
      "Iteration 69780, Loss: 0.003838335164\n",
      "Iteration 69790, Loss: 0.003830381203\n",
      "Iteration 69800, Loss: 0.003827121109\n",
      "Iteration 69810, Loss: 0.003879616968\n",
      "Iteration 69820, Loss: 0.005658078473\n",
      "Iteration 69830, Loss: 0.004389925394\n",
      "Iteration 69840, Loss: 0.004269255325\n",
      "Iteration 69850, Loss: 0.003942797892\n",
      "Iteration 69860, Loss: 0.003883749479\n",
      "Iteration 69870, Loss: 0.003855057759\n",
      "Iteration 69880, Loss: 0.004194429144\n",
      "Iteration 69890, Loss: 0.006468735635\n",
      "Iteration 69900, Loss: 0.004162821453\n",
      "Iteration 69910, Loss: 0.003907911945\n",
      "Iteration 69920, Loss: 0.003878027899\n",
      "Iteration 69930, Loss: 0.003836609889\n",
      "Iteration 69940, Loss: 0.003913688473\n",
      "Iteration 69950, Loss: 0.005408167839\n",
      "Iteration 69960, Loss: 0.004101098515\n",
      "Iteration 69970, Loss: 0.004170021042\n",
      "Iteration 69980, Loss: 0.003944464028\n",
      "Iteration 69990, Loss: 0.003921661992\n",
      "Iteration 70000, Loss: 0.004142171238\n",
      "Iteration 70010, Loss: 0.004969464615\n",
      "Iteration 70020, Loss: 0.003853368340\n",
      "Iteration 70030, Loss: 0.004017447587\n",
      "Iteration 70040, Loss: 0.003839756362\n",
      "Iteration 70050, Loss: 0.004110336304\n",
      "Iteration 70060, Loss: 0.007629907690\n",
      "Iteration 70070, Loss: 0.004109372385\n",
      "Iteration 70080, Loss: 0.003935097717\n",
      "Iteration 70090, Loss: 0.003890748369\n",
      "Iteration 70100, Loss: 0.003884960897\n",
      "Iteration 70110, Loss: 0.003828474088\n",
      "Iteration 70120, Loss: 0.003845467232\n",
      "Iteration 70130, Loss: 0.004510691389\n",
      "Iteration 70140, Loss: 0.004169712774\n",
      "Iteration 70150, Loss: 0.004528196994\n",
      "Iteration 70160, Loss: 0.003998895176\n",
      "Iteration 70170, Loss: 0.003936038353\n",
      "Iteration 70180, Loss: 0.003832093906\n",
      "Iteration 70190, Loss: 0.003830234986\n",
      "Iteration 70200, Loss: 0.003856860334\n",
      "Iteration 70210, Loss: 0.006589286961\n",
      "Iteration 70220, Loss: 0.005521967076\n",
      "Iteration 70230, Loss: 0.004082791973\n",
      "Iteration 70240, Loss: 0.004008867778\n",
      "Iteration 70250, Loss: 0.003851775546\n",
      "Iteration 70260, Loss: 0.003826256841\n",
      "Iteration 70270, Loss: 0.003821994876\n",
      "Iteration 70280, Loss: 0.003826864064\n",
      "Iteration 70290, Loss: 0.004557117354\n",
      "Iteration 70300, Loss: 0.006068788934\n",
      "Iteration 70310, Loss: 0.004310650751\n",
      "Iteration 70320, Loss: 0.004191050772\n",
      "Iteration 70330, Loss: 0.003913779743\n",
      "Iteration 70340, Loss: 0.003861031029\n",
      "Iteration 70350, Loss: 0.003813487943\n",
      "Iteration 70360, Loss: 0.003900127951\n",
      "Iteration 70370, Loss: 0.008655322716\n",
      "Iteration 70380, Loss: 0.004111101385\n",
      "Iteration 70390, Loss: 0.004119360354\n",
      "Iteration 70400, Loss: 0.003904414829\n",
      "Iteration 70410, Loss: 0.003859435907\n",
      "Iteration 70420, Loss: 0.003818668658\n",
      "Iteration 70430, Loss: 0.003814702388\n",
      "Iteration 70440, Loss: 0.003811816685\n",
      "Iteration 70450, Loss: 0.003999882843\n",
      "Iteration 70460, Loss: 0.007621108554\n",
      "Iteration 70470, Loss: 0.004198824987\n",
      "Iteration 70480, Loss: 0.004061215557\n",
      "Iteration 70490, Loss: 0.003922000527\n",
      "Iteration 70500, Loss: 0.003848163644\n",
      "Iteration 70510, Loss: 0.003815480741\n",
      "Iteration 70520, Loss: 0.003806477413\n",
      "Iteration 70530, Loss: 0.003805308370\n",
      "Iteration 70540, Loss: 0.003986970987\n",
      "Iteration 70550, Loss: 0.007708039135\n",
      "Iteration 70560, Loss: 0.004849734716\n",
      "Iteration 70570, Loss: 0.004121814854\n",
      "Iteration 70580, Loss: 0.003920638934\n",
      "Iteration 70590, Loss: 0.003859180491\n",
      "Iteration 70600, Loss: 0.003819622099\n",
      "Iteration 70610, Loss: 0.003811235074\n",
      "Iteration 70620, Loss: 0.003807253903\n",
      "Iteration 70630, Loss: 0.003845521715\n",
      "Iteration 70640, Loss: 0.006844100077\n",
      "Iteration 70650, Loss: 0.005372157320\n",
      "Iteration 70660, Loss: 0.004048050381\n",
      "Iteration 70670, Loss: 0.003986932337\n",
      "Iteration 70680, Loss: 0.003866205690\n",
      "Iteration 70690, Loss: 0.003835980548\n",
      "Iteration 70700, Loss: 0.003809800372\n",
      "Iteration 70710, Loss: 0.003818244673\n",
      "Iteration 70720, Loss: 0.005414496176\n",
      "Iteration 70730, Loss: 0.005429557059\n",
      "Iteration 70740, Loss: 0.004227555823\n",
      "Iteration 70750, Loss: 0.004017452709\n",
      "Iteration 70760, Loss: 0.003912310582\n",
      "Iteration 70770, Loss: 0.003830778413\n",
      "Iteration 70780, Loss: 0.003814477706\n",
      "Iteration 70790, Loss: 0.003799276659\n",
      "Iteration 70800, Loss: 0.003798430553\n",
      "Iteration 70810, Loss: 0.004218279384\n",
      "Iteration 70820, Loss: 0.004788163584\n",
      "Iteration 70830, Loss: 0.004428936634\n",
      "Iteration 70840, Loss: 0.004074427765\n",
      "Iteration 70850, Loss: 0.003986055963\n",
      "Iteration 70860, Loss: 0.003861808917\n",
      "Iteration 70870, Loss: 0.003818508005\n",
      "Iteration 70880, Loss: 0.003803244559\n",
      "Iteration 70890, Loss: 0.003801868297\n",
      "Iteration 70900, Loss: 0.003900123294\n",
      "Iteration 70910, Loss: 0.008144165389\n",
      "Iteration 70920, Loss: 0.004758762196\n",
      "Iteration 70930, Loss: 0.004010863602\n",
      "Iteration 70940, Loss: 0.003910142463\n",
      "Iteration 70950, Loss: 0.003860151628\n",
      "Iteration 70960, Loss: 0.003831223818\n",
      "Iteration 70970, Loss: 0.003829125082\n",
      "Iteration 70980, Loss: 0.004490218591\n",
      "Iteration 70990, Loss: 0.004301787354\n",
      "Iteration 71000, Loss: 0.004358624574\n",
      "Iteration 71010, Loss: 0.003950068261\n",
      "Iteration 71020, Loss: 0.003881857730\n",
      "Iteration 71030, Loss: 0.003841449507\n",
      "Iteration 71040, Loss: 0.003811967792\n",
      "Iteration 71050, Loss: 0.003792460077\n",
      "Iteration 71060, Loss: 0.004096957389\n",
      "Iteration 71070, Loss: 0.006128399633\n",
      "Iteration 71080, Loss: 0.004842529073\n",
      "Iteration 71090, Loss: 0.003989169840\n",
      "Iteration 71100, Loss: 0.003895151429\n",
      "Iteration 71110, Loss: 0.003841426922\n",
      "Iteration 71120, Loss: 0.003808121663\n",
      "Iteration 71130, Loss: 0.003803657601\n",
      "Iteration 71140, Loss: 0.003792776028\n",
      "Iteration 71150, Loss: 0.003790634684\n",
      "Iteration 71160, Loss: 0.003925316967\n",
      "Iteration 71170, Loss: 0.008713032119\n",
      "Iteration 71180, Loss: 0.004760997370\n",
      "Iteration 71190, Loss: 0.004068578128\n",
      "Iteration 71200, Loss: 0.003918779548\n",
      "Iteration 71210, Loss: 0.003826612607\n",
      "Iteration 71220, Loss: 0.003802321618\n",
      "Iteration 71230, Loss: 0.003798112972\n",
      "Iteration 71240, Loss: 0.003791445866\n",
      "Iteration 71250, Loss: 0.003787277034\n",
      "Iteration 71260, Loss: 0.003962348215\n",
      "Iteration 71270, Loss: 0.007712331600\n",
      "Iteration 71280, Loss: 0.005093327723\n",
      "Iteration 71290, Loss: 0.004080886021\n",
      "Iteration 71300, Loss: 0.003932298627\n",
      "Iteration 71310, Loss: 0.003860798199\n",
      "Iteration 71320, Loss: 0.003815184115\n",
      "Iteration 71330, Loss: 0.003798770718\n",
      "Iteration 71340, Loss: 0.003790723626\n",
      "Iteration 71350, Loss: 0.003801865503\n",
      "Iteration 71360, Loss: 0.004863918759\n",
      "Iteration 71370, Loss: 0.003933500964\n",
      "Iteration 71380, Loss: 0.004505113699\n",
      "Iteration 71390, Loss: 0.003931826912\n",
      "Iteration 71400, Loss: 0.003877247684\n",
      "Iteration 71410, Loss: 0.003797712969\n",
      "Iteration 71420, Loss: 0.003793490585\n",
      "Iteration 71430, Loss: 0.003798415652\n",
      "Iteration 71440, Loss: 0.003919144161\n",
      "Iteration 71450, Loss: 0.008986609057\n",
      "Iteration 71460, Loss: 0.004266935401\n",
      "Iteration 71470, Loss: 0.004103390500\n",
      "Iteration 71480, Loss: 0.003994052298\n",
      "Iteration 71490, Loss: 0.003863883205\n",
      "Iteration 71500, Loss: 0.003817521967\n",
      "Iteration 71510, Loss: 0.003797072917\n",
      "Iteration 71520, Loss: 0.003810659749\n",
      "Iteration 71530, Loss: 0.004255244974\n",
      "Iteration 71540, Loss: 0.005532192532\n",
      "Iteration 71550, Loss: 0.004356211983\n",
      "Iteration 71560, Loss: 0.004293432925\n",
      "Iteration 71570, Loss: 0.003864292055\n",
      "Iteration 71580, Loss: 0.003834891133\n",
      "Iteration 71590, Loss: 0.003838890698\n",
      "Iteration 71600, Loss: 0.004121040925\n",
      "Iteration 71610, Loss: 0.006680479273\n",
      "Iteration 71620, Loss: 0.004174825735\n",
      "Iteration 71630, Loss: 0.003838486969\n",
      "Iteration 71640, Loss: 0.003844619263\n",
      "Iteration 71650, Loss: 0.003783513559\n",
      "Iteration 71660, Loss: 0.003858743003\n",
      "Iteration 71670, Loss: 0.004957457073\n",
      "Iteration 71680, Loss: 0.003855452873\n",
      "Iteration 71690, Loss: 0.004181649536\n",
      "Iteration 71700, Loss: 0.003922754899\n",
      "Iteration 71710, Loss: 0.003831260605\n",
      "Iteration 71720, Loss: 0.003800753504\n",
      "Iteration 71730, Loss: 0.003792810952\n",
      "Iteration 71740, Loss: 0.003811419476\n",
      "Iteration 71750, Loss: 0.010348157957\n",
      "Iteration 71760, Loss: 0.004609711468\n",
      "Iteration 71770, Loss: 0.004685801454\n",
      "Iteration 71780, Loss: 0.003989267629\n",
      "Iteration 71790, Loss: 0.003840735182\n",
      "Iteration 71800, Loss: 0.003825001186\n",
      "Iteration 71810, Loss: 0.003795772558\n",
      "Iteration 71820, Loss: 0.003796608653\n",
      "Iteration 71830, Loss: 0.003810570342\n",
      "Iteration 71840, Loss: 0.003931275103\n",
      "Iteration 71850, Loss: 0.004475808237\n",
      "Iteration 71860, Loss: 0.004153240938\n",
      "Iteration 71870, Loss: 0.003786951303\n",
      "Iteration 71880, Loss: 0.003919914365\n",
      "Iteration 71890, Loss: 0.006655409001\n",
      "Iteration 71900, Loss: 0.004667525180\n",
      "Iteration 71910, Loss: 0.004105580505\n",
      "Iteration 71920, Loss: 0.003836701857\n",
      "Iteration 71930, Loss: 0.003811260685\n",
      "Iteration 71940, Loss: 0.003805767279\n",
      "Iteration 71950, Loss: 0.003796138801\n",
      "Iteration 71960, Loss: 0.007951497100\n",
      "Iteration 71970, Loss: 0.005821985658\n",
      "Iteration 71980, Loss: 0.004162389785\n",
      "Iteration 71990, Loss: 0.003851365065\n",
      "Iteration 72000, Loss: 0.003829136025\n",
      "Iteration 72010, Loss: 0.003801857121\n",
      "Iteration 72020, Loss: 0.003785708686\n",
      "Iteration 72030, Loss: 0.003793503623\n",
      "Iteration 72040, Loss: 0.004193108529\n",
      "Iteration 72050, Loss: 0.005031214561\n",
      "Iteration 72060, Loss: 0.004234733526\n",
      "Iteration 72070, Loss: 0.003944563214\n",
      "Iteration 72080, Loss: 0.003809324233\n",
      "Iteration 72090, Loss: 0.003774651792\n",
      "Iteration 72100, Loss: 0.004762406927\n",
      "Iteration 72110, Loss: 0.004854583181\n",
      "Iteration 72120, Loss: 0.004867460113\n",
      "Iteration 72130, Loss: 0.004037709441\n",
      "Iteration 72140, Loss: 0.003812559415\n",
      "Iteration 72150, Loss: 0.003823945066\n",
      "Iteration 72160, Loss: 0.003789249575\n",
      "Iteration 72170, Loss: 0.003786675166\n",
      "Iteration 72180, Loss: 0.003899153788\n",
      "Iteration 72190, Loss: 0.007511129603\n",
      "Iteration 72200, Loss: 0.004662922118\n",
      "Iteration 72210, Loss: 0.003988730256\n",
      "Iteration 72220, Loss: 0.003898674157\n",
      "Iteration 72230, Loss: 0.003784595290\n",
      "Iteration 72240, Loss: 0.003792897565\n",
      "Iteration 72250, Loss: 0.003775431542\n",
      "Iteration 72260, Loss: 0.004065987654\n",
      "Iteration 72270, Loss: 0.007063518744\n",
      "Iteration 72280, Loss: 0.003910135012\n",
      "Iteration 72290, Loss: 0.004018030129\n",
      "Iteration 72300, Loss: 0.003809622023\n",
      "Iteration 72310, Loss: 0.003806303721\n",
      "Iteration 72320, Loss: 0.003780615982\n",
      "Iteration 72330, Loss: 0.003768572118\n",
      "Iteration 72340, Loss: 0.003817066085\n",
      "Iteration 72350, Loss: 0.009050909430\n",
      "Iteration 72360, Loss: 0.004406597931\n",
      "Iteration 72370, Loss: 0.004105332773\n",
      "Iteration 72380, Loss: 0.003916710149\n",
      "Iteration 72390, Loss: 0.003830728354\n",
      "Iteration 72400, Loss: 0.003816742450\n",
      "Iteration 72410, Loss: 0.003872034838\n",
      "Iteration 72420, Loss: 0.003918675240\n",
      "Iteration 72430, Loss: 0.003766819602\n",
      "Iteration 72440, Loss: 0.003779278602\n",
      "Iteration 72450, Loss: 0.005245107692\n",
      "Iteration 72460, Loss: 0.004770597443\n",
      "Iteration 72470, Loss: 0.004019467626\n",
      "Iteration 72480, Loss: 0.003939627670\n",
      "Iteration 72490, Loss: 0.003839124460\n",
      "Iteration 72500, Loss: 0.003818374127\n",
      "Iteration 72510, Loss: 0.003929771949\n",
      "Iteration 72520, Loss: 0.003864460858\n",
      "Iteration 72530, Loss: 0.003771568649\n",
      "Iteration 72540, Loss: 0.003822266590\n",
      "Iteration 72550, Loss: 0.006069027819\n",
      "Iteration 72560, Loss: 0.006469629705\n",
      "Iteration 72570, Loss: 0.004371622577\n",
      "Iteration 72580, Loss: 0.004005979747\n",
      "Iteration 72590, Loss: 0.003836388933\n",
      "Iteration 72600, Loss: 0.003804041538\n",
      "Iteration 72610, Loss: 0.004026969895\n",
      "Iteration 72620, Loss: 0.003809201764\n",
      "Iteration 72630, Loss: 0.003830868285\n",
      "Iteration 72640, Loss: 0.004056054167\n",
      "Iteration 72650, Loss: 0.006350581069\n",
      "Iteration 72660, Loss: 0.004606950097\n",
      "Iteration 72670, Loss: 0.003798692720\n",
      "Iteration 72680, Loss: 0.003858738579\n",
      "Iteration 72690, Loss: 0.003800556064\n",
      "Iteration 72700, Loss: 0.003760799067\n",
      "Iteration 72710, Loss: 0.003898166120\n",
      "Iteration 72720, Loss: 0.008272257634\n",
      "Iteration 72730, Loss: 0.005090389866\n",
      "Iteration 72740, Loss: 0.004095070995\n",
      "Iteration 72750, Loss: 0.003864722326\n",
      "Iteration 72760, Loss: 0.003777583130\n",
      "Iteration 72770, Loss: 0.003762842622\n",
      "Iteration 72780, Loss: 0.003762466600\n",
      "Iteration 72790, Loss: 0.003819322679\n",
      "Iteration 72800, Loss: 0.007526223548\n",
      "Iteration 72810, Loss: 0.005270386115\n",
      "Iteration 72820, Loss: 0.004098043777\n",
      "Iteration 72830, Loss: 0.003845838364\n",
      "Iteration 72840, Loss: 0.003792029107\n",
      "Iteration 72850, Loss: 0.003775827121\n",
      "Iteration 72860, Loss: 0.003765014466\n",
      "Iteration 72870, Loss: 0.003755010432\n",
      "Iteration 72880, Loss: 0.003770146053\n",
      "Iteration 72890, Loss: 0.008955070749\n",
      "Iteration 72900, Loss: 0.005274858791\n",
      "Iteration 72910, Loss: 0.004060532432\n",
      "Iteration 72920, Loss: 0.004106685985\n",
      "Iteration 72930, Loss: 0.003885226324\n",
      "Iteration 72940, Loss: 0.003806994064\n",
      "Iteration 72950, Loss: 0.003771705786\n",
      "Iteration 72960, Loss: 0.003762240056\n",
      "Iteration 72970, Loss: 0.003757797182\n",
      "Iteration 72980, Loss: 0.003789072856\n",
      "Iteration 72990, Loss: 0.005755241029\n",
      "Iteration 73000, Loss: 0.004439454526\n",
      "Iteration 73010, Loss: 0.004246010445\n",
      "Iteration 73020, Loss: 0.003838383360\n",
      "Iteration 73030, Loss: 0.003835435491\n",
      "Iteration 73040, Loss: 0.003791123396\n",
      "Iteration 73050, Loss: 0.003766239155\n",
      "Iteration 73060, Loss: 0.004067402333\n",
      "Iteration 73070, Loss: 0.007135345601\n",
      "Iteration 73080, Loss: 0.004080889281\n",
      "Iteration 73090, Loss: 0.003989262506\n",
      "Iteration 73100, Loss: 0.003799011000\n",
      "Iteration 73110, Loss: 0.003795571625\n",
      "Iteration 73120, Loss: 0.003765603062\n",
      "Iteration 73130, Loss: 0.003778020153\n",
      "Iteration 73140, Loss: 0.004248068668\n",
      "Iteration 73150, Loss: 0.004950133152\n",
      "Iteration 73160, Loss: 0.004092219286\n",
      "Iteration 73170, Loss: 0.003849769244\n",
      "Iteration 73180, Loss: 0.003776680212\n",
      "Iteration 73190, Loss: 0.003783509601\n",
      "Iteration 73200, Loss: 0.004077501595\n",
      "Iteration 73210, Loss: 0.006015861407\n",
      "Iteration 73220, Loss: 0.004288772587\n",
      "Iteration 73230, Loss: 0.003944785334\n",
      "Iteration 73240, Loss: 0.003874300281\n",
      "Iteration 73250, Loss: 0.003761636792\n",
      "Iteration 73260, Loss: 0.003759875195\n",
      "Iteration 73270, Loss: 0.003752932185\n",
      "Iteration 73280, Loss: 0.003788261674\n",
      "Iteration 73290, Loss: 0.008345149457\n",
      "Iteration 73300, Loss: 0.004807761870\n",
      "Iteration 73310, Loss: 0.003918115050\n",
      "Iteration 73320, Loss: 0.003834657837\n",
      "Iteration 73330, Loss: 0.003780961270\n",
      "Iteration 73340, Loss: 0.003766554408\n",
      "Iteration 73350, Loss: 0.003763890825\n",
      "Iteration 73360, Loss: 0.003745477879\n",
      "Iteration 73370, Loss: 0.003760888940\n",
      "Iteration 73380, Loss: 0.005416796543\n",
      "Iteration 73390, Loss: 0.004978343844\n",
      "Iteration 73400, Loss: 0.004034835380\n",
      "Iteration 73410, Loss: 0.003970118705\n",
      "Iteration 73420, Loss: 0.003780936357\n",
      "Iteration 73430, Loss: 0.003770254785\n",
      "Iteration 73440, Loss: 0.003765904577\n",
      "Iteration 73450, Loss: 0.003844477236\n",
      "Iteration 73460, Loss: 0.007188471965\n",
      "Iteration 73470, Loss: 0.004899800289\n",
      "Iteration 73480, Loss: 0.003995686304\n",
      "Iteration 73490, Loss: 0.003820256563\n",
      "Iteration 73500, Loss: 0.003784038825\n",
      "Iteration 73510, Loss: 0.003748190356\n",
      "Iteration 73520, Loss: 0.003757497296\n",
      "Iteration 73530, Loss: 0.005135874730\n",
      "Iteration 73540, Loss: 0.004439170472\n",
      "Iteration 73550, Loss: 0.004037451465\n",
      "Iteration 73560, Loss: 0.003891742323\n",
      "Iteration 73570, Loss: 0.003785311244\n",
      "Iteration 73580, Loss: 0.003764592344\n",
      "Iteration 73590, Loss: 0.003753009252\n",
      "Iteration 73600, Loss: 0.003750313539\n",
      "Iteration 73610, Loss: 0.004111006856\n",
      "Iteration 73620, Loss: 0.005613443907\n",
      "Iteration 73630, Loss: 0.004127666354\n",
      "Iteration 73640, Loss: 0.003983322531\n",
      "Iteration 73650, Loss: 0.003764186753\n",
      "Iteration 73660, Loss: 0.003770717420\n",
      "Iteration 73670, Loss: 0.003746629227\n",
      "Iteration 73680, Loss: 0.003777121892\n",
      "Iteration 73690, Loss: 0.004726942163\n",
      "Iteration 73700, Loss: 0.007483521476\n",
      "Iteration 73710, Loss: 0.004214465152\n",
      "Iteration 73720, Loss: 0.003954738844\n",
      "Iteration 73730, Loss: 0.003834572388\n",
      "Iteration 73740, Loss: 0.003795537632\n",
      "Iteration 73750, Loss: 0.003771637799\n",
      "Iteration 73760, Loss: 0.003821967868\n",
      "Iteration 73770, Loss: 0.005908064544\n",
      "Iteration 73780, Loss: 0.004728854168\n",
      "Iteration 73790, Loss: 0.004083115608\n",
      "Iteration 73800, Loss: 0.003777122125\n",
      "Iteration 73810, Loss: 0.003780253930\n",
      "Iteration 73820, Loss: 0.003735896666\n",
      "Iteration 73830, Loss: 0.003780572908\n",
      "Iteration 73840, Loss: 0.004988679197\n",
      "Iteration 73850, Loss: 0.003953246865\n",
      "Iteration 73860, Loss: 0.004269541707\n",
      "Iteration 73870, Loss: 0.003876339411\n",
      "Iteration 73880, Loss: 0.003793017939\n",
      "Iteration 73890, Loss: 0.003775388002\n",
      "Iteration 73900, Loss: 0.003796816105\n",
      "Iteration 73910, Loss: 0.004278941080\n",
      "Iteration 73920, Loss: 0.004974838346\n",
      "Iteration 73930, Loss: 0.003884012112\n",
      "Iteration 73940, Loss: 0.003899928182\n",
      "Iteration 73950, Loss: 0.003859357210\n",
      "Iteration 73960, Loss: 0.003769094823\n",
      "Iteration 73970, Loss: 0.003848594148\n",
      "Iteration 73980, Loss: 0.006848025136\n",
      "Iteration 73990, Loss: 0.004575970583\n",
      "Iteration 74000, Loss: 0.004101128783\n",
      "Iteration 74010, Loss: 0.003853993956\n",
      "Iteration 74020, Loss: 0.003779420163\n",
      "Iteration 74030, Loss: 0.003747758921\n",
      "Iteration 74040, Loss: 0.003823375097\n",
      "Iteration 74050, Loss: 0.009634208865\n",
      "Iteration 74060, Loss: 0.004187331069\n",
      "Iteration 74070, Loss: 0.003996393178\n",
      "Iteration 74080, Loss: 0.003889052197\n",
      "Iteration 74090, Loss: 0.003771564690\n",
      "Iteration 74100, Loss: 0.003790748771\n",
      "Iteration 74110, Loss: 0.003749163589\n",
      "Iteration 74120, Loss: 0.003743165405\n",
      "Iteration 74130, Loss: 0.003734435886\n",
      "Iteration 74140, Loss: 0.003848889377\n",
      "Iteration 74150, Loss: 0.009367961437\n",
      "Iteration 74160, Loss: 0.004094745032\n",
      "Iteration 74170, Loss: 0.004062221851\n",
      "Iteration 74180, Loss: 0.003931001294\n",
      "Iteration 74190, Loss: 0.003876780160\n",
      "Iteration 74200, Loss: 0.003782321233\n",
      "Iteration 74210, Loss: 0.003767224262\n",
      "Iteration 74220, Loss: 0.003733304795\n",
      "Iteration 74230, Loss: 0.003731747391\n",
      "Iteration 74240, Loss: 0.004625421949\n",
      "Iteration 74250, Loss: 0.004519805778\n",
      "Iteration 74260, Loss: 0.004093596246\n",
      "Iteration 74270, Loss: 0.004000987392\n",
      "Iteration 74280, Loss: 0.004035202321\n",
      "Iteration 74290, Loss: 0.003804733977\n",
      "Iteration 74300, Loss: 0.003759732237\n",
      "Iteration 74310, Loss: 0.003740459681\n",
      "Iteration 74320, Loss: 0.003739928361\n",
      "Iteration 74330, Loss: 0.003800274571\n",
      "Iteration 74340, Loss: 0.005455839913\n",
      "Iteration 74350, Loss: 0.004151240923\n",
      "Iteration 74360, Loss: 0.004137493670\n",
      "Iteration 74370, Loss: 0.003881930839\n",
      "Iteration 74380, Loss: 0.003766761860\n",
      "Iteration 74390, Loss: 0.003777740523\n",
      "Iteration 74400, Loss: 0.003923766315\n",
      "Iteration 74410, Loss: 0.006202107761\n",
      "Iteration 74420, Loss: 0.004115468822\n",
      "Iteration 74430, Loss: 0.004126970191\n",
      "Iteration 74440, Loss: 0.003831955371\n",
      "Iteration 74450, Loss: 0.003740697168\n",
      "Iteration 74460, Loss: 0.003825203283\n",
      "Iteration 74470, Loss: 0.007913630456\n",
      "Iteration 74480, Loss: 0.005200627726\n",
      "Iteration 74490, Loss: 0.004050638527\n",
      "Iteration 74500, Loss: 0.003882865189\n",
      "Iteration 74510, Loss: 0.003790857736\n",
      "Iteration 74520, Loss: 0.003763700603\n",
      "Iteration 74530, Loss: 0.003749762662\n",
      "Iteration 74540, Loss: 0.003790691961\n",
      "Iteration 74550, Loss: 0.005896345712\n",
      "Iteration 74560, Loss: 0.004580446985\n",
      "Iteration 74570, Loss: 0.003884879174\n",
      "Iteration 74580, Loss: 0.003944052383\n",
      "Iteration 74590, Loss: 0.003790236777\n",
      "Iteration 74600, Loss: 0.003767868970\n",
      "Iteration 74610, Loss: 0.003835211275\n",
      "Iteration 74620, Loss: 0.005185258575\n",
      "Iteration 74630, Loss: 0.003805644345\n",
      "Iteration 74640, Loss: 0.003890484571\n",
      "Iteration 74650, Loss: 0.003816589480\n",
      "Iteration 74660, Loss: 0.003723240690\n",
      "Iteration 74670, Loss: 0.003793615382\n",
      "Iteration 74680, Loss: 0.005343437660\n",
      "Iteration 74690, Loss: 0.004204271827\n",
      "Iteration 74700, Loss: 0.003997981548\n",
      "Iteration 74710, Loss: 0.003861229168\n",
      "Iteration 74720, Loss: 0.003759481711\n",
      "Iteration 74730, Loss: 0.003740056651\n",
      "Iteration 74740, Loss: 0.003725384362\n",
      "Iteration 74750, Loss: 0.004306451883\n",
      "Iteration 74760, Loss: 0.003936374094\n",
      "Iteration 74770, Loss: 0.004215818830\n",
      "Iteration 74780, Loss: 0.003835053183\n",
      "Iteration 74790, Loss: 0.003848479362\n",
      "Iteration 74800, Loss: 0.003804214764\n",
      "Iteration 74810, Loss: 0.003751853481\n",
      "Iteration 74820, Loss: 0.003743516281\n",
      "Iteration 74830, Loss: 0.003761966713\n",
      "Iteration 74840, Loss: 0.003964371048\n",
      "Iteration 74850, Loss: 0.005750688724\n",
      "Iteration 74860, Loss: 0.004327728879\n",
      "Iteration 74870, Loss: 0.003986178432\n",
      "Iteration 74880, Loss: 0.003833164461\n",
      "Iteration 74890, Loss: 0.003751096549\n",
      "Iteration 74900, Loss: 0.003907502163\n",
      "Iteration 74910, Loss: 0.004325905815\n",
      "Iteration 74920, Loss: 0.004726042971\n",
      "Iteration 74930, Loss: 0.004576921463\n",
      "Iteration 74940, Loss: 0.004195815418\n",
      "Iteration 74950, Loss: 0.003921069205\n",
      "Iteration 74960, Loss: 0.003758453764\n",
      "Iteration 74970, Loss: 0.003751228331\n",
      "Iteration 74980, Loss: 0.003715771949\n",
      "Iteration 74990, Loss: 0.003717036918\n",
      "Iteration 75000, Loss: 0.004244892858\n",
      "Iteration 75010, Loss: 0.004415007308\n",
      "Iteration 75020, Loss: 0.004584269598\n",
      "Iteration 75030, Loss: 0.004044414498\n",
      "Iteration 75040, Loss: 0.003860452445\n",
      "Iteration 75050, Loss: 0.003744479502\n",
      "Iteration 75060, Loss: 0.003734906437\n",
      "Iteration 75070, Loss: 0.003717696993\n",
      "Iteration 75080, Loss: 0.003810739378\n",
      "Iteration 75090, Loss: 0.007516121492\n",
      "Iteration 75100, Loss: 0.004855360836\n",
      "Iteration 75110, Loss: 0.003922252450\n",
      "Iteration 75120, Loss: 0.003801912069\n",
      "Iteration 75130, Loss: 0.003775379155\n",
      "Iteration 75140, Loss: 0.003723079804\n",
      "Iteration 75150, Loss: 0.003742104862\n",
      "Iteration 75160, Loss: 0.005327145103\n",
      "Iteration 75170, Loss: 0.004512173124\n",
      "Iteration 75180, Loss: 0.004206248093\n",
      "Iteration 75190, Loss: 0.003902188502\n",
      "Iteration 75200, Loss: 0.003793583019\n",
      "Iteration 75210, Loss: 0.003744671121\n",
      "Iteration 75220, Loss: 0.003706154181\n",
      "Iteration 75230, Loss: 0.003705404699\n",
      "Iteration 75240, Loss: 0.003887282684\n",
      "Iteration 75250, Loss: 0.007951384410\n",
      "Iteration 75260, Loss: 0.004627008457\n",
      "Iteration 75270, Loss: 0.004102401901\n",
      "Iteration 75280, Loss: 0.003920990974\n",
      "Iteration 75290, Loss: 0.003795036580\n",
      "Iteration 75300, Loss: 0.003755066078\n",
      "Iteration 75310, Loss: 0.003723630682\n",
      "Iteration 75320, Loss: 0.003726871451\n",
      "Iteration 75330, Loss: 0.004029092379\n",
      "Iteration 75340, Loss: 0.004321357235\n",
      "Iteration 75350, Loss: 0.003901635529\n",
      "Iteration 75360, Loss: 0.003763617016\n",
      "Iteration 75370, Loss: 0.003703374881\n",
      "Iteration 75380, Loss: 0.004036914557\n",
      "Iteration 75390, Loss: 0.005806408357\n",
      "Iteration 75400, Loss: 0.004743462428\n",
      "Iteration 75410, Loss: 0.003960998263\n",
      "Iteration 75420, Loss: 0.003838463686\n",
      "Iteration 75430, Loss: 0.003758910345\n",
      "Iteration 75440, Loss: 0.003708301345\n",
      "Iteration 75450, Loss: 0.003706325544\n",
      "Iteration 75460, Loss: 0.003700384172\n",
      "Iteration 75470, Loss: 0.003758012084\n",
      "Iteration 75480, Loss: 0.010064085945\n",
      "Iteration 75490, Loss: 0.004353278782\n",
      "Iteration 75500, Loss: 0.004257325549\n",
      "Iteration 75510, Loss: 0.003815180389\n",
      "Iteration 75520, Loss: 0.003769519506\n",
      "Iteration 75530, Loss: 0.003720028559\n",
      "Iteration 75540, Loss: 0.003705174662\n",
      "Iteration 75550, Loss: 0.003704895964\n",
      "Iteration 75560, Loss: 0.003905062098\n",
      "Iteration 75570, Loss: 0.006944192573\n",
      "Iteration 75580, Loss: 0.004359852057\n",
      "Iteration 75590, Loss: 0.003921126015\n",
      "Iteration 75600, Loss: 0.003796385368\n",
      "Iteration 75610, Loss: 0.003758817445\n",
      "Iteration 75620, Loss: 0.003711151192\n",
      "Iteration 75630, Loss: 0.003706842428\n",
      "Iteration 75640, Loss: 0.004060572479\n",
      "Iteration 75650, Loss: 0.005355961155\n",
      "Iteration 75660, Loss: 0.004669351969\n",
      "Iteration 75670, Loss: 0.003931557294\n",
      "Iteration 75680, Loss: 0.003782203887\n",
      "Iteration 75690, Loss: 0.003757427679\n",
      "Iteration 75700, Loss: 0.003705000505\n",
      "Iteration 75710, Loss: 0.003700599540\n",
      "Iteration 75720, Loss: 0.003735310631\n",
      "Iteration 75730, Loss: 0.005897776224\n",
      "Iteration 75740, Loss: 0.004644470289\n",
      "Iteration 75750, Loss: 0.004109666683\n",
      "Iteration 75760, Loss: 0.003868027357\n",
      "Iteration 75770, Loss: 0.003772136755\n",
      "Iteration 75780, Loss: 0.003717511194\n",
      "Iteration 75790, Loss: 0.003711944446\n",
      "Iteration 75800, Loss: 0.003715319559\n",
      "Iteration 75810, Loss: 0.005828132387\n",
      "Iteration 75820, Loss: 0.006083117332\n",
      "Iteration 75830, Loss: 0.004358851351\n",
      "Iteration 75840, Loss: 0.003896672744\n",
      "Iteration 75850, Loss: 0.003740348853\n",
      "Iteration 75860, Loss: 0.003713845741\n",
      "Iteration 75870, Loss: 0.003701055422\n",
      "Iteration 75880, Loss: 0.003695355961\n",
      "Iteration 75890, Loss: 0.003721009241\n",
      "Iteration 75900, Loss: 0.007969793864\n",
      "Iteration 75910, Loss: 0.005134249572\n",
      "Iteration 75920, Loss: 0.004716333002\n",
      "Iteration 75930, Loss: 0.004359855782\n",
      "Iteration 75940, Loss: 0.003769606352\n",
      "Iteration 75950, Loss: 0.005018040538\n",
      "Iteration 75960, Loss: 0.004544561263\n",
      "Iteration 75970, Loss: 0.003870276501\n",
      "Iteration 75980, Loss: 0.003854130628\n",
      "Iteration 75990, Loss: 0.003746732837\n",
      "Iteration 76000, Loss: 0.004033501260\n",
      "Iteration 76010, Loss: 0.005350636318\n",
      "Iteration 76020, Loss: 0.004231266677\n",
      "Iteration 76030, Loss: 0.003882884048\n",
      "Iteration 76040, Loss: 0.003777131904\n",
      "Iteration 76050, Loss: 0.003696993692\n",
      "Iteration 76060, Loss: 0.003689450677\n",
      "Iteration 76070, Loss: 0.004057397600\n",
      "Iteration 76080, Loss: 0.004744273145\n",
      "Iteration 76090, Loss: 0.004564144649\n",
      "Iteration 76100, Loss: 0.003994791768\n",
      "Iteration 76110, Loss: 0.003814279335\n",
      "Iteration 76120, Loss: 0.003736046376\n",
      "Iteration 76130, Loss: 0.003701755311\n",
      "Iteration 76140, Loss: 0.003692048369\n",
      "Iteration 76150, Loss: 0.003693078877\n",
      "Iteration 76160, Loss: 0.003694500774\n",
      "Iteration 76170, Loss: 0.004158126190\n",
      "Iteration 76180, Loss: 0.004640291445\n",
      "Iteration 76190, Loss: 0.004497476853\n",
      "Iteration 76200, Loss: 0.003853003960\n",
      "Iteration 76210, Loss: 0.003844467457\n",
      "Iteration 76220, Loss: 0.003710274585\n",
      "Iteration 76230, Loss: 0.003705313196\n",
      "Iteration 76240, Loss: 0.003688421100\n",
      "Iteration 76250, Loss: 0.003737329971\n",
      "Iteration 76260, Loss: 0.006482897326\n",
      "Iteration 76270, Loss: 0.005526226945\n",
      "Iteration 76280, Loss: 0.004104161169\n",
      "Iteration 76290, Loss: 0.003759977408\n",
      "Iteration 76300, Loss: 0.003743107896\n",
      "Iteration 76310, Loss: 0.003700371599\n",
      "Iteration 76320, Loss: 0.003694473300\n",
      "Iteration 76330, Loss: 0.003685365198\n",
      "Iteration 76340, Loss: 0.003763215384\n",
      "Iteration 76350, Loss: 0.011657893658\n",
      "Iteration 76360, Loss: 0.004564881790\n",
      "Iteration 76370, Loss: 0.004403491504\n",
      "Iteration 76380, Loss: 0.003899141215\n",
      "Iteration 76390, Loss: 0.003757458646\n",
      "Iteration 76400, Loss: 0.003713556333\n",
      "Iteration 76410, Loss: 0.003702747403\n",
      "Iteration 76420, Loss: 0.003686087206\n",
      "Iteration 76430, Loss: 0.003684355412\n",
      "Iteration 76440, Loss: 0.003803449450\n",
      "Iteration 76450, Loss: 0.008129425347\n",
      "Iteration 76460, Loss: 0.004245049320\n",
      "Iteration 76470, Loss: 0.003997394815\n",
      "Iteration 76480, Loss: 0.003805852029\n",
      "Iteration 76490, Loss: 0.003713657148\n",
      "Iteration 76500, Loss: 0.003692459781\n",
      "Iteration 76510, Loss: 0.003683041781\n",
      "Iteration 76520, Loss: 0.003821272636\n",
      "Iteration 76530, Loss: 0.008469182998\n",
      "Iteration 76540, Loss: 0.004279143643\n",
      "Iteration 76550, Loss: 0.003952589817\n",
      "Iteration 76560, Loss: 0.003838687669\n",
      "Iteration 76570, Loss: 0.003725143615\n",
      "Iteration 76580, Loss: 0.003718110500\n",
      "Iteration 76590, Loss: 0.003715083934\n",
      "Iteration 76600, Loss: 0.005142910872\n",
      "Iteration 76610, Loss: 0.004060878418\n",
      "Iteration 76620, Loss: 0.004152693786\n",
      "Iteration 76630, Loss: 0.003899241798\n",
      "Iteration 76640, Loss: 0.003760405118\n",
      "Iteration 76650, Loss: 0.003717685351\n",
      "Iteration 76660, Loss: 0.003695484949\n",
      "Iteration 76670, Loss: 0.003828700166\n",
      "Iteration 76680, Loss: 0.006360233296\n",
      "Iteration 76690, Loss: 0.004364293534\n",
      "Iteration 76700, Loss: 0.003986227326\n",
      "Iteration 76710, Loss: 0.003748490242\n",
      "Iteration 76720, Loss: 0.003723369678\n",
      "Iteration 76730, Loss: 0.003711803118\n",
      "Iteration 76740, Loss: 0.004102593288\n",
      "Iteration 76750, Loss: 0.005434801802\n",
      "Iteration 76760, Loss: 0.005596414208\n",
      "Iteration 76770, Loss: 0.004398790188\n",
      "Iteration 76780, Loss: 0.004104612861\n",
      "Iteration 76790, Loss: 0.003788095433\n",
      "Iteration 76800, Loss: 0.003741037566\n",
      "Iteration 76810, Loss: 0.003697454231\n",
      "Iteration 76820, Loss: 0.003683578456\n",
      "Iteration 76830, Loss: 0.003712869948\n",
      "Iteration 76840, Loss: 0.005342402961\n",
      "Iteration 76850, Loss: 0.004199588206\n",
      "Iteration 76860, Loss: 0.004170501139\n",
      "Iteration 76870, Loss: 0.003740641987\n",
      "Iteration 76880, Loss: 0.003725174814\n",
      "Iteration 76890, Loss: 0.003735391889\n",
      "Iteration 76900, Loss: 0.003709673183\n",
      "Iteration 76910, Loss: 0.004768990912\n",
      "Iteration 76920, Loss: 0.003784823697\n",
      "Iteration 76930, Loss: 0.004290485755\n",
      "Iteration 76940, Loss: 0.003870926099\n",
      "Iteration 76950, Loss: 0.003732734825\n",
      "Iteration 76960, Loss: 0.003704155097\n",
      "Iteration 76970, Loss: 0.003676312044\n",
      "Iteration 76980, Loss: 0.003670332953\n",
      "Iteration 76990, Loss: 0.003675399581\n",
      "Iteration 77000, Loss: 0.007167504169\n",
      "Iteration 77010, Loss: 0.006575663574\n",
      "Iteration 77020, Loss: 0.004135346040\n",
      "Iteration 77030, Loss: 0.004165587015\n",
      "Iteration 77040, Loss: 0.003752626246\n",
      "Iteration 77050, Loss: 0.003707875730\n",
      "Iteration 77060, Loss: 0.003692466766\n",
      "Iteration 77070, Loss: 0.003685594536\n",
      "Iteration 77080, Loss: 0.003683857387\n",
      "Iteration 77090, Loss: 0.003802291350\n",
      "Iteration 77100, Loss: 0.004610361531\n",
      "Iteration 77110, Loss: 0.003822330385\n",
      "Iteration 77120, Loss: 0.003818816505\n",
      "Iteration 77130, Loss: 0.004287161399\n",
      "Iteration 77140, Loss: 0.004445184022\n",
      "Iteration 77150, Loss: 0.004097698722\n",
      "Iteration 77160, Loss: 0.003820207901\n",
      "Iteration 77170, Loss: 0.003705292707\n",
      "Iteration 77180, Loss: 0.003677095752\n",
      "Iteration 77190, Loss: 0.004289242439\n",
      "Iteration 77200, Loss: 0.004338975530\n",
      "Iteration 77210, Loss: 0.003945717122\n",
      "Iteration 77220, Loss: 0.003884795122\n",
      "Iteration 77230, Loss: 0.003871160792\n",
      "Iteration 77240, Loss: 0.003934870474\n",
      "Iteration 77250, Loss: 0.003728868673\n",
      "Iteration 77260, Loss: 0.003678400768\n",
      "Iteration 77270, Loss: 0.003673277562\n",
      "Iteration 77280, Loss: 0.003700226080\n",
      "Iteration 77290, Loss: 0.006322932430\n",
      "Iteration 77300, Loss: 0.005472021643\n",
      "Iteration 77310, Loss: 0.004076797049\n",
      "Iteration 77320, Loss: 0.003817964345\n",
      "Iteration 77330, Loss: 0.003722273977\n",
      "Iteration 77340, Loss: 0.003697103355\n",
      "Iteration 77350, Loss: 0.003668732708\n",
      "Iteration 77360, Loss: 0.003675176529\n",
      "Iteration 77370, Loss: 0.004700851161\n",
      "Iteration 77380, Loss: 0.003907441627\n",
      "Iteration 77390, Loss: 0.004321089480\n",
      "Iteration 77400, Loss: 0.003911191132\n",
      "Iteration 77410, Loss: 0.003706533462\n",
      "Iteration 77420, Loss: 0.003686487908\n",
      "Iteration 77430, Loss: 0.003673179774\n",
      "Iteration 77440, Loss: 0.003665641416\n",
      "Iteration 77450, Loss: 0.003668166930\n",
      "Iteration 77460, Loss: 0.004847140051\n",
      "Iteration 77470, Loss: 0.004185703117\n",
      "Iteration 77480, Loss: 0.004178097937\n",
      "Iteration 77490, Loss: 0.003846477484\n",
      "Iteration 77500, Loss: 0.003713093465\n",
      "Iteration 77510, Loss: 0.003689036006\n",
      "Iteration 77520, Loss: 0.003679266665\n",
      "Iteration 77530, Loss: 0.003672549501\n",
      "Iteration 77540, Loss: 0.003946388140\n",
      "Iteration 77550, Loss: 0.005817272235\n",
      "Iteration 77560, Loss: 0.004397342913\n",
      "Iteration 77570, Loss: 0.003896801034\n",
      "Iteration 77580, Loss: 0.003706293646\n",
      "Iteration 77590, Loss: 0.003685676726\n",
      "Iteration 77600, Loss: 0.003841507714\n",
      "Iteration 77610, Loss: 0.006049879361\n",
      "Iteration 77620, Loss: 0.004768349696\n",
      "Iteration 77630, Loss: 0.003890044987\n",
      "Iteration 77640, Loss: 0.003803548403\n",
      "Iteration 77650, Loss: 0.003852819558\n",
      "Iteration 77660, Loss: 0.003688835073\n",
      "Iteration 77670, Loss: 0.003712717909\n",
      "Iteration 77680, Loss: 0.004571000580\n",
      "Iteration 77690, Loss: 0.003815238830\n",
      "Iteration 77700, Loss: 0.004365274217\n",
      "Iteration 77710, Loss: 0.003799538594\n",
      "Iteration 77720, Loss: 0.003877204843\n",
      "Iteration 77730, Loss: 0.003691488877\n",
      "Iteration 77740, Loss: 0.003718453459\n",
      "Iteration 77750, Loss: 0.003779459512\n",
      "Iteration 77760, Loss: 0.006572914310\n",
      "Iteration 77770, Loss: 0.004704328720\n",
      "Iteration 77780, Loss: 0.003788350150\n",
      "Iteration 77790, Loss: 0.003831064096\n",
      "Iteration 77800, Loss: 0.003704469185\n",
      "Iteration 77810, Loss: 0.003687982215\n",
      "Iteration 77820, Loss: 0.003910403233\n",
      "Iteration 77830, Loss: 0.006487993523\n",
      "Iteration 77840, Loss: 0.004549069330\n",
      "Iteration 77850, Loss: 0.003795969998\n",
      "Iteration 77860, Loss: 0.003700007219\n",
      "Iteration 77870, Loss: 0.003682965413\n",
      "Iteration 77880, Loss: 0.003684115596\n",
      "Iteration 77890, Loss: 0.004964850843\n",
      "Iteration 77900, Loss: 0.004139695317\n",
      "Iteration 77910, Loss: 0.004019444808\n",
      "Iteration 77920, Loss: 0.003827607958\n",
      "Iteration 77930, Loss: 0.004025682807\n",
      "Iteration 77940, Loss: 0.003941753879\n",
      "Iteration 77950, Loss: 0.003722139401\n",
      "Iteration 77960, Loss: 0.003776378231\n",
      "Iteration 77970, Loss: 0.004418770783\n",
      "Iteration 77980, Loss: 0.004362653475\n",
      "Iteration 77990, Loss: 0.003814257449\n",
      "Iteration 78000, Loss: 0.003734794678\n",
      "Iteration 78010, Loss: 0.003726501483\n",
      "Iteration 78020, Loss: 0.003721558023\n",
      "Iteration 78030, Loss: 0.005135294050\n",
      "Iteration 78040, Loss: 0.004298400134\n",
      "Iteration 78050, Loss: 0.004306118004\n",
      "Iteration 78060, Loss: 0.003839005018\n",
      "Iteration 78070, Loss: 0.003702498274\n",
      "Iteration 78080, Loss: 0.003672386985\n",
      "Iteration 78090, Loss: 0.003818914294\n",
      "Iteration 78100, Loss: 0.005187188275\n",
      "Iteration 78110, Loss: 0.003900238546\n",
      "Iteration 78120, Loss: 0.003807629459\n",
      "Iteration 78130, Loss: 0.003798553953\n",
      "Iteration 78140, Loss: 0.003707990982\n",
      "Iteration 78150, Loss: 0.003705744864\n",
      "Iteration 78160, Loss: 0.004156361800\n",
      "Iteration 78170, Loss: 0.005680069327\n",
      "Iteration 78180, Loss: 0.004047674127\n",
      "Iteration 78190, Loss: 0.003816824406\n",
      "Iteration 78200, Loss: 0.003782417160\n",
      "Iteration 78210, Loss: 0.003735824022\n",
      "Iteration 78220, Loss: 0.004028372001\n",
      "Iteration 78230, Loss: 0.003671695013\n",
      "Iteration 78240, Loss: 0.003745340509\n",
      "Iteration 78250, Loss: 0.008163957857\n",
      "Iteration 78260, Loss: 0.004878737964\n",
      "Iteration 78270, Loss: 0.004298267420\n",
      "Iteration 78280, Loss: 0.003886474762\n",
      "Iteration 78290, Loss: 0.003687767778\n",
      "Iteration 78300, Loss: 0.003678456414\n",
      "Iteration 78310, Loss: 0.003649240127\n",
      "Iteration 78320, Loss: 0.003648828249\n",
      "Iteration 78330, Loss: 0.003923971206\n",
      "Iteration 78340, Loss: 0.006983698346\n",
      "Iteration 78350, Loss: 0.004637561738\n",
      "Iteration 78360, Loss: 0.004073010758\n",
      "Iteration 78370, Loss: 0.003785580397\n",
      "Iteration 78380, Loss: 0.003686365904\n",
      "Iteration 78390, Loss: 0.003665132681\n",
      "Iteration 78400, Loss: 0.003658747533\n",
      "Iteration 78410, Loss: 0.003751019249\n",
      "Iteration 78420, Loss: 0.006542492658\n",
      "Iteration 78430, Loss: 0.004537434317\n",
      "Iteration 78440, Loss: 0.003825883614\n",
      "Iteration 78450, Loss: 0.003698977875\n",
      "Iteration 78460, Loss: 0.003674688749\n",
      "Iteration 78470, Loss: 0.003685636446\n",
      "Iteration 78480, Loss: 0.004091316834\n",
      "Iteration 78490, Loss: 0.005553026684\n",
      "Iteration 78500, Loss: 0.003869135166\n",
      "Iteration 78510, Loss: 0.003805317450\n",
      "Iteration 78520, Loss: 0.003767091548\n",
      "Iteration 78530, Loss: 0.003655544017\n",
      "Iteration 78540, Loss: 0.003658307716\n",
      "Iteration 78550, Loss: 0.003725387156\n",
      "Iteration 78560, Loss: 0.008582888171\n",
      "Iteration 78570, Loss: 0.004685255699\n",
      "Iteration 78580, Loss: 0.004246229306\n",
      "Iteration 78590, Loss: 0.003904302139\n",
      "Iteration 78600, Loss: 0.003708582604\n",
      "Iteration 78610, Loss: 0.003720671171\n",
      "Iteration 78620, Loss: 0.003650948638\n",
      "Iteration 78630, Loss: 0.003662092611\n",
      "Iteration 78640, Loss: 0.004141615238\n",
      "Iteration 78650, Loss: 0.004824269097\n",
      "Iteration 78660, Loss: 0.004533801228\n",
      "Iteration 78670, Loss: 0.004028405994\n",
      "Iteration 78680, Loss: 0.003764890367\n",
      "Iteration 78690, Loss: 0.003654442262\n",
      "Iteration 78700, Loss: 0.003657831810\n",
      "Iteration 78710, Loss: 0.003642932279\n",
      "Iteration 78720, Loss: 0.004179346375\n",
      "Iteration 78730, Loss: 0.004547412973\n",
      "Iteration 78740, Loss: 0.003865398932\n",
      "Iteration 78750, Loss: 0.003951333463\n",
      "Iteration 78760, Loss: 0.003698971588\n",
      "Iteration 78770, Loss: 0.003674478503\n",
      "Iteration 78780, Loss: 0.003656197339\n",
      "Iteration 78790, Loss: 0.004472550936\n",
      "Iteration 78800, Loss: 0.003773711389\n",
      "Iteration 78810, Loss: 0.004080853425\n",
      "Iteration 78820, Loss: 0.003852079157\n",
      "Iteration 78830, Loss: 0.003781259991\n",
      "Iteration 78840, Loss: 0.003710790304\n",
      "Iteration 78850, Loss: 0.003664296819\n",
      "Iteration 78860, Loss: 0.003652998479\n",
      "Iteration 78870, Loss: 0.003661391558\n",
      "Iteration 78880, Loss: 0.004496561363\n",
      "Iteration 78890, Loss: 0.003887539729\n",
      "Iteration 78900, Loss: 0.003954663407\n",
      "Iteration 78910, Loss: 0.003800977720\n",
      "Iteration 78920, Loss: 0.003661603434\n",
      "Iteration 78930, Loss: 0.003663482144\n",
      "Iteration 78940, Loss: 0.003654180793\n",
      "Iteration 78950, Loss: 0.003882307792\n",
      "Iteration 78960, Loss: 0.005122569855\n",
      "Iteration 78970, Loss: 0.004417714663\n",
      "Iteration 78980, Loss: 0.003958101384\n",
      "Iteration 78990, Loss: 0.003811819246\n",
      "Iteration 79000, Loss: 0.003708854085\n",
      "Iteration 79010, Loss: 0.003681254108\n",
      "Iteration 79020, Loss: 0.003642064519\n",
      "Iteration 79030, Loss: 0.003648452461\n",
      "Iteration 79040, Loss: 0.005921196193\n",
      "Iteration 79050, Loss: 0.005848209839\n",
      "Iteration 79060, Loss: 0.004286379553\n",
      "Iteration 79070, Loss: 0.003953967709\n",
      "Iteration 79080, Loss: 0.003766770242\n",
      "Iteration 79090, Loss: 0.003667140612\n",
      "Iteration 79100, Loss: 0.003659463953\n",
      "Iteration 79110, Loss: 0.003658301197\n",
      "Iteration 79120, Loss: 0.003850667039\n",
      "Iteration 79130, Loss: 0.004433828406\n",
      "Iteration 79140, Loss: 0.003768143011\n",
      "Iteration 79150, Loss: 0.003873739159\n",
      "Iteration 79160, Loss: 0.003662059316\n",
      "Iteration 79170, Loss: 0.003628887702\n",
      "Iteration 79180, Loss: 0.004129348323\n",
      "Iteration 79190, Loss: 0.004398806021\n",
      "Iteration 79200, Loss: 0.004006371368\n",
      "Iteration 79210, Loss: 0.003799230326\n",
      "Iteration 79220, Loss: 0.003752232529\n",
      "Iteration 79230, Loss: 0.003672976280\n",
      "Iteration 79240, Loss: 0.003658963135\n",
      "Iteration 79250, Loss: 0.003649561666\n",
      "Iteration 79260, Loss: 0.003820359241\n",
      "Iteration 79270, Loss: 0.004026503768\n",
      "Iteration 79280, Loss: 0.003947512712\n",
      "Iteration 79290, Loss: 0.004152072128\n",
      "Iteration 79300, Loss: 0.004100002348\n",
      "Iteration 79310, Loss: 0.003657856025\n",
      "Iteration 79320, Loss: 0.004102945328\n",
      "Iteration 79330, Loss: 0.004893144127\n",
      "Iteration 79340, Loss: 0.003891379340\n",
      "Iteration 79350, Loss: 0.003692182247\n",
      "Iteration 79360, Loss: 0.003931778017\n",
      "Iteration 79370, Loss: 0.005000843201\n",
      "Iteration 79380, Loss: 0.003952629399\n",
      "Iteration 79390, Loss: 0.003760500578\n",
      "Iteration 79400, Loss: 0.003664321033\n",
      "Iteration 79410, Loss: 0.003632729407\n",
      "Iteration 79420, Loss: 0.004589934833\n",
      "Iteration 79430, Loss: 0.006111633964\n",
      "Iteration 79440, Loss: 0.005063250661\n",
      "Iteration 79450, Loss: 0.003786775749\n",
      "Iteration 79460, Loss: 0.003950188868\n",
      "Iteration 79470, Loss: 0.003741424764\n",
      "Iteration 79480, Loss: 0.003675132524\n",
      "Iteration 79490, Loss: 0.003647933016\n",
      "Iteration 79500, Loss: 0.003634016961\n",
      "Iteration 79510, Loss: 0.003633220214\n",
      "Iteration 79520, Loss: 0.003819533624\n",
      "Iteration 79530, Loss: 0.004315743688\n",
      "Iteration 79540, Loss: 0.004369944334\n",
      "Iteration 79550, Loss: 0.003821835620\n",
      "Iteration 79560, Loss: 0.003685935400\n",
      "Iteration 79570, Loss: 0.003656997113\n",
      "Iteration 79580, Loss: 0.004611449316\n",
      "Iteration 79590, Loss: 0.004075286444\n",
      "Iteration 79600, Loss: 0.003955489025\n",
      "Iteration 79610, Loss: 0.003896648297\n",
      "Iteration 79620, Loss: 0.003789296141\n",
      "Iteration 79630, Loss: 0.003654718166\n",
      "Iteration 79640, Loss: 0.003697833046\n",
      "Iteration 79650, Loss: 0.003942479379\n",
      "Iteration 79660, Loss: 0.003803930711\n",
      "Iteration 79670, Loss: 0.004365523346\n",
      "Iteration 79680, Loss: 0.004310680088\n",
      "Iteration 79690, Loss: 0.003875776194\n",
      "Iteration 79700, Loss: 0.003687445540\n",
      "Iteration 79710, Loss: 0.003763160668\n",
      "Iteration 79720, Loss: 0.004535420798\n",
      "Iteration 79730, Loss: 0.004076815210\n",
      "Iteration 79740, Loss: 0.004271760583\n",
      "Iteration 79750, Loss: 0.003696705680\n",
      "Iteration 79760, Loss: 0.003750897245\n",
      "Iteration 79770, Loss: 0.003638237482\n",
      "Iteration 79780, Loss: 0.003641932504\n",
      "Iteration 79790, Loss: 0.005268441513\n",
      "Iteration 79800, Loss: 0.005039233714\n",
      "Iteration 79810, Loss: 0.004129439592\n",
      "Iteration 79820, Loss: 0.004003845155\n",
      "Iteration 79830, Loss: 0.003759464715\n",
      "Iteration 79840, Loss: 0.003682886250\n",
      "Iteration 79850, Loss: 0.004820440430\n",
      "Iteration 79860, Loss: 0.003765965113\n",
      "Iteration 79870, Loss: 0.004345539492\n",
      "Iteration 79880, Loss: 0.003862549085\n",
      "Iteration 79890, Loss: 0.003675422631\n",
      "Iteration 79900, Loss: 0.003644691315\n",
      "Iteration 79910, Loss: 0.003618925344\n",
      "Iteration 79920, Loss: 0.003614903660\n",
      "Iteration 79930, Loss: 0.003627362661\n",
      "Iteration 79940, Loss: 0.005500284024\n",
      "Iteration 79950, Loss: 0.005485699512\n",
      "Iteration 79960, Loss: 0.003883165075\n",
      "Iteration 79970, Loss: 0.003791820491\n",
      "Iteration 79980, Loss: 0.003653877415\n",
      "Iteration 79990, Loss: 0.003648382844\n",
      "Iteration 80000, Loss: 0.003622147953\n",
      "Iteration 80010, Loss: 0.003613010515\n",
      "Iteration 80020, Loss: 0.003626037389\n",
      "Iteration 80030, Loss: 0.005965135992\n",
      "Iteration 80040, Loss: 0.006400753278\n",
      "Iteration 80050, Loss: 0.004222336225\n",
      "Iteration 80060, Loss: 0.003955211490\n",
      "Iteration 80070, Loss: 0.003677237546\n",
      "Iteration 80080, Loss: 0.003630346386\n",
      "Iteration 80090, Loss: 0.003620565170\n",
      "Iteration 80100, Loss: 0.003616722999\n",
      "Iteration 80110, Loss: 0.003610556480\n",
      "Iteration 80120, Loss: 0.003824096872\n",
      "Iteration 80130, Loss: 0.007179059088\n",
      "Iteration 80140, Loss: 0.004524038639\n",
      "Iteration 80150, Loss: 0.003734114347\n",
      "Iteration 80160, Loss: 0.003650938626\n",
      "Iteration 80170, Loss: 0.003676468739\n",
      "Iteration 80180, Loss: 0.003621087410\n",
      "Iteration 80190, Loss: 0.003614090150\n",
      "Iteration 80200, Loss: 0.003618138842\n",
      "Iteration 80210, Loss: 0.004463250283\n",
      "Iteration 80220, Loss: 0.003757981583\n",
      "Iteration 80230, Loss: 0.004095977638\n",
      "Iteration 80240, Loss: 0.003847759217\n",
      "Iteration 80250, Loss: 0.003721789690\n",
      "Iteration 80260, Loss: 0.003675422166\n",
      "Iteration 80270, Loss: 0.003900629235\n",
      "Iteration 80280, Loss: 0.003945731092\n",
      "Iteration 80290, Loss: 0.003621494630\n",
      "Iteration 80300, Loss: 0.003629977815\n",
      "Iteration 80310, Loss: 0.005100771785\n",
      "Iteration 80320, Loss: 0.004728966858\n",
      "Iteration 80330, Loss: 0.003919381183\n",
      "Iteration 80340, Loss: 0.003737054300\n",
      "Iteration 80350, Loss: 0.003671112005\n",
      "Iteration 80360, Loss: 0.003644377459\n",
      "Iteration 80370, Loss: 0.003615611000\n",
      "Iteration 80380, Loss: 0.003680034773\n",
      "Iteration 80390, Loss: 0.006255360786\n",
      "Iteration 80400, Loss: 0.003968287259\n",
      "Iteration 80410, Loss: 0.003937802743\n",
      "Iteration 80420, Loss: 0.003706424264\n",
      "Iteration 80430, Loss: 0.003613795619\n",
      "Iteration 80440, Loss: 0.003766704584\n",
      "Iteration 80450, Loss: 0.008084811270\n",
      "Iteration 80460, Loss: 0.004320465494\n",
      "Iteration 80470, Loss: 0.004053022247\n",
      "Iteration 80480, Loss: 0.003709462704\n",
      "Iteration 80490, Loss: 0.003663512180\n",
      "Iteration 80500, Loss: 0.003626951715\n",
      "Iteration 80510, Loss: 0.003677369561\n",
      "Iteration 80520, Loss: 0.005570906214\n",
      "Iteration 80530, Loss: 0.004250861704\n",
      "Iteration 80540, Loss: 0.003975176252\n",
      "Iteration 80550, Loss: 0.003725963412\n",
      "Iteration 80560, Loss: 0.003623646684\n",
      "Iteration 80570, Loss: 0.003643133678\n",
      "Iteration 80580, Loss: 0.004282585811\n",
      "Iteration 80590, Loss: 0.005040664226\n",
      "Iteration 80600, Loss: 0.004071234725\n",
      "Iteration 80610, Loss: 0.003719247878\n",
      "Iteration 80620, Loss: 0.003645008197\n",
      "Iteration 80630, Loss: 0.003774078563\n",
      "Iteration 80640, Loss: 0.006464661099\n",
      "Iteration 80650, Loss: 0.004579244647\n",
      "Iteration 80660, Loss: 0.003862840589\n",
      "Iteration 80670, Loss: 0.003755727783\n",
      "Iteration 80680, Loss: 0.003773623845\n",
      "Iteration 80690, Loss: 0.003738086903\n",
      "Iteration 80700, Loss: 0.003653421765\n",
      "Iteration 80710, Loss: 0.003750641365\n",
      "Iteration 80720, Loss: 0.009790590033\n",
      "Iteration 80730, Loss: 0.005411446095\n",
      "Iteration 80740, Loss: 0.004212644417\n",
      "Iteration 80750, Loss: 0.003774317680\n",
      "Iteration 80760, Loss: 0.003824844258\n",
      "Iteration 80770, Loss: 0.003668268910\n",
      "Iteration 80780, Loss: 0.003624587553\n",
      "Iteration 80790, Loss: 0.003615708323\n",
      "Iteration 80800, Loss: 0.003603707999\n",
      "Iteration 80810, Loss: 0.003598117270\n",
      "Iteration 80820, Loss: 0.003601571079\n",
      "Iteration 80830, Loss: 0.003796402132\n",
      "Iteration 80840, Loss: 0.005962047726\n",
      "Iteration 80850, Loss: 0.004312567879\n",
      "Iteration 80860, Loss: 0.003918672446\n",
      "Iteration 80870, Loss: 0.003676338820\n",
      "Iteration 80880, Loss: 0.003650110215\n",
      "Iteration 80890, Loss: 0.004178370815\n",
      "Iteration 80900, Loss: 0.004640832078\n",
      "Iteration 80910, Loss: 0.003649588674\n",
      "Iteration 80920, Loss: 0.003825553227\n",
      "Iteration 80930, Loss: 0.003848788328\n",
      "Iteration 80940, Loss: 0.004988517612\n",
      "Iteration 80950, Loss: 0.003707442665\n",
      "Iteration 80960, Loss: 0.003689751029\n",
      "Iteration 80970, Loss: 0.003693019738\n",
      "Iteration 80980, Loss: 0.003723534755\n",
      "Iteration 80990, Loss: 0.003667916637\n",
      "Iteration 81000, Loss: 0.010388244875\n",
      "Iteration 81010, Loss: 0.004546388518\n",
      "Iteration 81020, Loss: 0.004209574312\n",
      "Iteration 81030, Loss: 0.003851517802\n",
      "Iteration 81040, Loss: 0.003707868280\n",
      "Iteration 81050, Loss: 0.003675395856\n",
      "Iteration 81060, Loss: 0.003605966223\n",
      "Iteration 81070, Loss: 0.003628162434\n",
      "Iteration 81080, Loss: 0.003662835574\n",
      "Iteration 81090, Loss: 0.003675927641\n",
      "Iteration 81100, Loss: 0.004713327158\n",
      "Iteration 81110, Loss: 0.003752922872\n",
      "Iteration 81120, Loss: 0.003810652066\n",
      "Iteration 81130, Loss: 0.003803027794\n",
      "Iteration 81140, Loss: 0.003970240243\n",
      "Iteration 81150, Loss: 0.003705992829\n",
      "Iteration 81160, Loss: 0.003796126926\n",
      "Iteration 81170, Loss: 0.007117535919\n",
      "Iteration 81180, Loss: 0.004131606780\n",
      "Iteration 81190, Loss: 0.004526987672\n",
      "Iteration 81200, Loss: 0.004029985052\n",
      "Iteration 81210, Loss: 0.003676663153\n",
      "Iteration 81220, Loss: 0.003639980219\n",
      "Iteration 81230, Loss: 0.003595355200\n",
      "Iteration 81240, Loss: 0.003631433006\n",
      "Iteration 81250, Loss: 0.007804769557\n",
      "Iteration 81260, Loss: 0.005069091450\n",
      "Iteration 81270, Loss: 0.004193566740\n",
      "Iteration 81280, Loss: 0.004205206409\n",
      "Iteration 81290, Loss: 0.003830283880\n",
      "Iteration 81300, Loss: 0.003681041766\n",
      "Iteration 81310, Loss: 0.003602726851\n",
      "Iteration 81320, Loss: 0.003605827224\n",
      "Iteration 81330, Loss: 0.003591915127\n",
      "Iteration 81340, Loss: 0.003605518257\n",
      "Iteration 81350, Loss: 0.005002663936\n",
      "Iteration 81360, Loss: 0.004115767777\n",
      "Iteration 81370, Loss: 0.003814887721\n",
      "Iteration 81380, Loss: 0.003879189724\n",
      "Iteration 81390, Loss: 0.003733755089\n",
      "Iteration 81400, Loss: 0.003698728047\n",
      "Iteration 81410, Loss: 0.003656537272\n",
      "Iteration 81420, Loss: 0.003601282835\n",
      "Iteration 81430, Loss: 0.003616202157\n",
      "Iteration 81440, Loss: 0.005450449884\n",
      "Iteration 81450, Loss: 0.004703581799\n",
      "Iteration 81460, Loss: 0.003895182395\n",
      "Iteration 81470, Loss: 0.003670867765\n",
      "Iteration 81480, Loss: 0.003679012181\n",
      "Iteration 81490, Loss: 0.003618808696\n",
      "Iteration 81500, Loss: 0.003604166443\n",
      "Iteration 81510, Loss: 0.003588013584\n",
      "Iteration 81520, Loss: 0.003680353984\n",
      "Iteration 81530, Loss: 0.010370206088\n",
      "Iteration 81540, Loss: 0.004482550547\n",
      "Iteration 81550, Loss: 0.003775537945\n",
      "Iteration 81560, Loss: 0.003632827196\n",
      "Iteration 81570, Loss: 0.003654088359\n",
      "Iteration 81580, Loss: 0.003612472210\n",
      "Iteration 81590, Loss: 0.003712458769\n",
      "Iteration 81600, Loss: 0.003673289670\n",
      "Iteration 81610, Loss: 0.003653522814\n",
      "Iteration 81620, Loss: 0.003781702835\n",
      "Iteration 81630, Loss: 0.007265307009\n",
      "Iteration 81640, Loss: 0.004660100210\n",
      "Iteration 81650, Loss: 0.003925650381\n",
      "Iteration 81660, Loss: 0.003692602972\n",
      "Iteration 81670, Loss: 0.003614136251\n",
      "Iteration 81680, Loss: 0.003612186993\n",
      "Iteration 81690, Loss: 0.003591737477\n",
      "Iteration 81700, Loss: 0.003583708312\n",
      "Iteration 81710, Loss: 0.003658098634\n",
      "Iteration 81720, Loss: 0.009596371092\n",
      "Iteration 81730, Loss: 0.004250103608\n",
      "Iteration 81740, Loss: 0.003918397240\n",
      "Iteration 81750, Loss: 0.003912578803\n",
      "Iteration 81760, Loss: 0.003729105229\n",
      "Iteration 81770, Loss: 0.003637904767\n",
      "Iteration 81780, Loss: 0.003650296712\n",
      "Iteration 81790, Loss: 0.006476232316\n",
      "Iteration 81800, Loss: 0.004234411754\n",
      "Iteration 81810, Loss: 0.003799780738\n",
      "Iteration 81820, Loss: 0.003668938763\n",
      "Iteration 81830, Loss: 0.003626934020\n",
      "Iteration 81840, Loss: 0.003605841659\n",
      "Iteration 81850, Loss: 0.003944760654\n",
      "Iteration 81860, Loss: 0.005586127285\n",
      "Iteration 81870, Loss: 0.004657175392\n",
      "Iteration 81880, Loss: 0.003871149151\n",
      "Iteration 81890, Loss: 0.003733738558\n",
      "Iteration 81900, Loss: 0.003619096708\n",
      "Iteration 81910, Loss: 0.003585899714\n",
      "Iteration 81920, Loss: 0.003580120159\n",
      "Iteration 81930, Loss: 0.003577948548\n",
      "Iteration 81940, Loss: 0.003874742892\n",
      "Iteration 81950, Loss: 0.005459491163\n",
      "Iteration 81960, Loss: 0.004679773934\n",
      "Iteration 81970, Loss: 0.003845360130\n",
      "Iteration 81980, Loss: 0.003660124494\n",
      "Iteration 81990, Loss: 0.003630907042\n",
      "Iteration 82000, Loss: 0.003603239078\n",
      "Iteration 82010, Loss: 0.003580374643\n",
      "Iteration 82020, Loss: 0.003593139816\n",
      "Iteration 82030, Loss: 0.004777150229\n",
      "Iteration 82040, Loss: 0.003770446638\n",
      "Iteration 82050, Loss: 0.004223496187\n",
      "Iteration 82060, Loss: 0.003688440425\n",
      "Iteration 82070, Loss: 0.003678737441\n",
      "Iteration 82080, Loss: 0.003620086238\n",
      "Iteration 82090, Loss: 0.003575091716\n",
      "Iteration 82100, Loss: 0.003653272986\n",
      "Iteration 82110, Loss: 0.006829379126\n",
      "Iteration 82120, Loss: 0.004783312324\n",
      "Iteration 82130, Loss: 0.003990816884\n",
      "Iteration 82140, Loss: 0.003684313502\n",
      "Iteration 82150, Loss: 0.003627070691\n",
      "Iteration 82160, Loss: 0.003585034050\n",
      "Iteration 82170, Loss: 0.003589374712\n",
      "Iteration 82180, Loss: 0.004089011811\n",
      "Iteration 82190, Loss: 0.004434215371\n",
      "Iteration 82200, Loss: 0.004425505642\n",
      "Iteration 82210, Loss: 0.003850948997\n",
      "Iteration 82220, Loss: 0.003606957849\n",
      "Iteration 82230, Loss: 0.003616701346\n",
      "Iteration 82240, Loss: 0.003574203467\n",
      "Iteration 82250, Loss: 0.003569705412\n",
      "Iteration 82260, Loss: 0.003566790372\n",
      "Iteration 82270, Loss: 0.004025212489\n",
      "Iteration 82280, Loss: 0.004403585102\n",
      "Iteration 82290, Loss: 0.005000553560\n",
      "Iteration 82300, Loss: 0.003807111410\n",
      "Iteration 82310, Loss: 0.003704736708\n",
      "Iteration 82320, Loss: 0.003648752812\n",
      "Iteration 82330, Loss: 0.003607059596\n",
      "Iteration 82340, Loss: 0.003581678262\n",
      "Iteration 82350, Loss: 0.003574631410\n",
      "Iteration 82360, Loss: 0.003572968068\n",
      "Iteration 82370, Loss: 0.003757125465\n",
      "Iteration 82380, Loss: 0.005015538540\n",
      "Iteration 82390, Loss: 0.004017496016\n",
      "Iteration 82400, Loss: 0.004091487732\n",
      "Iteration 82410, Loss: 0.003699433990\n",
      "Iteration 82420, Loss: 0.003741957480\n",
      "Iteration 82430, Loss: 0.003587896936\n",
      "Iteration 82440, Loss: 0.003594144946\n",
      "Iteration 82450, Loss: 0.003626096062\n",
      "Iteration 82460, Loss: 0.011902111582\n",
      "Iteration 82470, Loss: 0.003985165618\n",
      "Iteration 82480, Loss: 0.004069265444\n",
      "Iteration 82490, Loss: 0.003857596079\n",
      "Iteration 82500, Loss: 0.003693230450\n",
      "Iteration 82510, Loss: 0.004067217000\n",
      "Iteration 82520, Loss: 0.003739039181\n",
      "Iteration 82530, Loss: 0.003602295648\n",
      "Iteration 82540, Loss: 0.003580239136\n",
      "Iteration 82550, Loss: 0.003570784815\n",
      "Iteration 82560, Loss: 0.003801898565\n",
      "Iteration 82570, Loss: 0.006014280953\n",
      "Iteration 82580, Loss: 0.004278766923\n",
      "Iteration 82590, Loss: 0.003786985297\n",
      "Iteration 82600, Loss: 0.003767109243\n",
      "Iteration 82610, Loss: 0.003624640871\n",
      "Iteration 82620, Loss: 0.003580106888\n",
      "Iteration 82630, Loss: 0.003570399480\n",
      "Iteration 82640, Loss: 0.003573632101\n",
      "Iteration 82650, Loss: 0.003854838433\n",
      "Iteration 82660, Loss: 0.005806507077\n",
      "Iteration 82670, Loss: 0.004030084703\n",
      "Iteration 82680, Loss: 0.003616983537\n",
      "Iteration 82690, Loss: 0.003592587775\n",
      "Iteration 82700, Loss: 0.003626258578\n",
      "Iteration 82710, Loss: 0.003890207503\n",
      "Iteration 82720, Loss: 0.006238401867\n",
      "Iteration 82730, Loss: 0.003926889505\n",
      "Iteration 82740, Loss: 0.003723170841\n",
      "Iteration 82750, Loss: 0.003606663086\n",
      "Iteration 82760, Loss: 0.003591915127\n",
      "Iteration 82770, Loss: 0.003573032562\n",
      "Iteration 82780, Loss: 0.003695405321\n",
      "Iteration 82790, Loss: 0.009245021269\n",
      "Iteration 82800, Loss: 0.004744885489\n",
      "Iteration 82810, Loss: 0.004183238372\n",
      "Iteration 82820, Loss: 0.003852398368\n",
      "Iteration 82830, Loss: 0.003967933822\n",
      "Iteration 82840, Loss: 0.003779202234\n",
      "Iteration 82850, Loss: 0.003665006021\n",
      "Iteration 82860, Loss: 0.003574721981\n",
      "Iteration 82870, Loss: 0.003567427397\n",
      "Iteration 82880, Loss: 0.003731232136\n",
      "Iteration 82890, Loss: 0.008560095914\n",
      "Iteration 82900, Loss: 0.003766772337\n",
      "Iteration 82910, Loss: 0.003816217883\n",
      "Iteration 82920, Loss: 0.003665852128\n",
      "Iteration 82930, Loss: 0.003610249842\n",
      "Iteration 82940, Loss: 0.003566442989\n",
      "Iteration 82950, Loss: 0.003562375670\n",
      "Iteration 82960, Loss: 0.003565066028\n",
      "Iteration 82970, Loss: 0.004377740435\n",
      "Iteration 82980, Loss: 0.003815180622\n",
      "Iteration 82990, Loss: 0.004137250129\n",
      "Iteration 83000, Loss: 0.003733182559\n",
      "Iteration 83010, Loss: 0.003597451141\n",
      "Iteration 83020, Loss: 0.003578347852\n",
      "Iteration 83030, Loss: 0.003568954533\n",
      "Iteration 83040, Loss: 0.003565769177\n",
      "Iteration 83050, Loss: 0.003577657510\n",
      "Iteration 83060, Loss: 0.005432367790\n",
      "Iteration 83070, Loss: 0.004672979005\n",
      "Iteration 83080, Loss: 0.003928248771\n",
      "Iteration 83090, Loss: 0.003710478311\n",
      "Iteration 83100, Loss: 0.003638942027\n",
      "Iteration 83110, Loss: 0.003583352780\n",
      "Iteration 83120, Loss: 0.003568268614\n",
      "Iteration 83130, Loss: 0.003615331603\n",
      "Iteration 83140, Loss: 0.005981248803\n",
      "Iteration 83150, Loss: 0.005343779456\n",
      "Iteration 83160, Loss: 0.003912940156\n",
      "Iteration 83170, Loss: 0.003660448827\n",
      "Iteration 83180, Loss: 0.003623991041\n",
      "Iteration 83190, Loss: 0.003567621112\n",
      "Iteration 83200, Loss: 0.003560614772\n",
      "Iteration 83210, Loss: 0.003569072345\n",
      "Iteration 83220, Loss: 0.007771218661\n",
      "Iteration 83230, Loss: 0.005088421516\n",
      "Iteration 83240, Loss: 0.004410783295\n",
      "Iteration 83250, Loss: 0.003651060630\n",
      "Iteration 83260, Loss: 0.003650247119\n",
      "Iteration 83270, Loss: 0.003597753355\n",
      "Iteration 83280, Loss: 0.003574250499\n",
      "Iteration 83290, Loss: 0.003557215678\n",
      "Iteration 83300, Loss: 0.003552441020\n",
      "Iteration 83310, Loss: 0.003680817550\n",
      "Iteration 83320, Loss: 0.009805059992\n",
      "Iteration 83330, Loss: 0.005055851303\n",
      "Iteration 83340, Loss: 0.003915194888\n",
      "Iteration 83350, Loss: 0.003626303514\n",
      "Iteration 83360, Loss: 0.003599315882\n",
      "Iteration 83370, Loss: 0.003570790635\n",
      "Iteration 83380, Loss: 0.003562682308\n",
      "Iteration 83390, Loss: 0.003571833717\n",
      "Iteration 83400, Loss: 0.004314050078\n",
      "Iteration 83410, Loss: 0.003722053021\n",
      "Iteration 83420, Loss: 0.003853740403\n",
      "Iteration 83430, Loss: 0.003730296623\n",
      "Iteration 83440, Loss: 0.003591125133\n",
      "Iteration 83450, Loss: 0.003597891191\n",
      "Iteration 83460, Loss: 0.003547738306\n",
      "Iteration 83470, Loss: 0.003560943529\n",
      "Iteration 83480, Loss: 0.007555313874\n",
      "Iteration 83490, Loss: 0.005653374363\n",
      "Iteration 83500, Loss: 0.005847656168\n",
      "Iteration 83510, Loss: 0.005099458620\n",
      "Iteration 83520, Loss: 0.003718964756\n",
      "Iteration 83530, Loss: 0.003660963383\n",
      "Iteration 83540, Loss: 0.003604922444\n",
      "Iteration 83550, Loss: 0.003619321855\n",
      "Iteration 83560, Loss: 0.003568750108\n",
      "Iteration 83570, Loss: 0.003620793577\n",
      "Iteration 83580, Loss: 0.006457001902\n",
      "Iteration 83590, Loss: 0.004767314531\n",
      "Iteration 83600, Loss: 0.003594988259\n",
      "Iteration 83610, Loss: 0.003751999233\n",
      "Iteration 83620, Loss: 0.003576357849\n",
      "Iteration 83630, Loss: 0.003554064082\n",
      "Iteration 83640, Loss: 0.003563011531\n",
      "Iteration 83650, Loss: 0.003550515976\n",
      "Iteration 83660, Loss: 0.003748733550\n",
      "Iteration 83670, Loss: 0.007014310919\n",
      "Iteration 83680, Loss: 0.004194781184\n",
      "Iteration 83690, Loss: 0.003789597191\n",
      "Iteration 83700, Loss: 0.003720403649\n",
      "Iteration 83710, Loss: 0.003584858961\n",
      "Iteration 83720, Loss: 0.003550446592\n",
      "Iteration 83730, Loss: 0.003552657319\n",
      "Iteration 83740, Loss: 0.003549705260\n",
      "Iteration 83750, Loss: 0.003745924449\n",
      "Iteration 83760, Loss: 0.008002589457\n",
      "Iteration 83770, Loss: 0.004367481917\n",
      "Iteration 83780, Loss: 0.003849955043\n",
      "Iteration 83790, Loss: 0.003764924360\n",
      "Iteration 83800, Loss: 0.003611658700\n",
      "Iteration 83810, Loss: 0.003569627646\n",
      "Iteration 83820, Loss: 0.003558678553\n",
      "Iteration 83830, Loss: 0.003651915817\n",
      "Iteration 83840, Loss: 0.005912712775\n",
      "Iteration 83850, Loss: 0.004360445309\n",
      "Iteration 83860, Loss: 0.003743761918\n",
      "Iteration 83870, Loss: 0.003594053211\n",
      "Iteration 83880, Loss: 0.003578678938\n",
      "Iteration 83890, Loss: 0.003565944033\n",
      "Iteration 83900, Loss: 0.003850808134\n",
      "Iteration 83910, Loss: 0.006034372374\n",
      "Iteration 83920, Loss: 0.004095146433\n",
      "Iteration 83930, Loss: 0.003804016160\n",
      "Iteration 83940, Loss: 0.003688528668\n",
      "Iteration 83950, Loss: 0.003586651525\n",
      "Iteration 83960, Loss: 0.003548625624\n",
      "Iteration 83970, Loss: 0.003584270366\n",
      "Iteration 83980, Loss: 0.006062757690\n",
      "Iteration 83990, Loss: 0.004852588288\n",
      "Iteration 84000, Loss: 0.003745970083\n",
      "Iteration 84010, Loss: 0.003732477315\n",
      "Iteration 84020, Loss: 0.003610889195\n",
      "Iteration 84030, Loss: 0.003578298725\n",
      "Iteration 84040, Loss: 0.003667121520\n",
      "Iteration 84050, Loss: 0.006011526566\n",
      "Iteration 84060, Loss: 0.004375090823\n",
      "Iteration 84070, Loss: 0.003613119712\n",
      "Iteration 84080, Loss: 0.003641757416\n",
      "Iteration 84090, Loss: 0.003576732939\n",
      "Iteration 84100, Loss: 0.003544180887\n",
      "Iteration 84110, Loss: 0.003556749783\n",
      "Iteration 84120, Loss: 0.009130736813\n",
      "Iteration 84130, Loss: 0.004995600320\n",
      "Iteration 84140, Loss: 0.004010707140\n",
      "Iteration 84150, Loss: 0.003692989936\n",
      "Iteration 84160, Loss: 0.003607645165\n",
      "Iteration 84170, Loss: 0.003588083433\n",
      "Iteration 84180, Loss: 0.003562128870\n",
      "Iteration 84190, Loss: 0.003543204861\n",
      "Iteration 84200, Loss: 0.003540402511\n",
      "Iteration 84210, Loss: 0.003773271339\n",
      "Iteration 84220, Loss: 0.005842458457\n",
      "Iteration 84230, Loss: 0.004132404923\n",
      "Iteration 84240, Loss: 0.003689379431\n",
      "Iteration 84250, Loss: 0.003591888119\n",
      "Iteration 84260, Loss: 0.003554188181\n",
      "Iteration 84270, Loss: 0.003543373896\n",
      "Iteration 84280, Loss: 0.003540531965\n",
      "Iteration 84290, Loss: 0.003633689368\n",
      "Iteration 84300, Loss: 0.009553316049\n",
      "Iteration 84310, Loss: 0.003865005914\n",
      "Iteration 84320, Loss: 0.003889844753\n",
      "Iteration 84330, Loss: 0.003646770958\n",
      "Iteration 84340, Loss: 0.003591341665\n",
      "Iteration 84350, Loss: 0.003553623334\n",
      "Iteration 84360, Loss: 0.003545514308\n",
      "Iteration 84370, Loss: 0.003549241461\n",
      "Iteration 84380, Loss: 0.004652490839\n",
      "Iteration 84390, Loss: 0.003738365835\n",
      "Iteration 84400, Loss: 0.004115244839\n",
      "Iteration 84410, Loss: 0.003727277275\n",
      "Iteration 84420, Loss: 0.003641351126\n",
      "Iteration 84430, Loss: 0.003563813167\n",
      "Iteration 84440, Loss: 0.003541640704\n",
      "Iteration 84450, Loss: 0.003566216445\n",
      "Iteration 84460, Loss: 0.004759083502\n",
      "Iteration 84470, Loss: 0.003773642704\n",
      "Iteration 84480, Loss: 0.004094444215\n",
      "Iteration 84490, Loss: 0.003618022660\n",
      "Iteration 84500, Loss: 0.003589856206\n",
      "Iteration 84510, Loss: 0.003561045509\n",
      "Iteration 84520, Loss: 0.003553719493\n",
      "Iteration 84530, Loss: 0.004190444015\n",
      "Iteration 84540, Loss: 0.004397216719\n",
      "Iteration 84550, Loss: 0.004101157654\n",
      "Iteration 84560, Loss: 0.003772887168\n",
      "Iteration 84570, Loss: 0.003607731080\n",
      "Iteration 84580, Loss: 0.003585450351\n",
      "Iteration 84590, Loss: 0.003535895376\n",
      "Iteration 84600, Loss: 0.003580449382\n",
      "Iteration 84610, Loss: 0.006037235260\n",
      "Iteration 84620, Loss: 0.006101302803\n",
      "Iteration 84630, Loss: 0.004291999619\n",
      "Iteration 84640, Loss: 0.003766078502\n",
      "Iteration 84650, Loss: 0.003696227679\n",
      "Iteration 84660, Loss: 0.003559086239\n",
      "Iteration 84670, Loss: 0.003549876157\n",
      "Iteration 84680, Loss: 0.003532065544\n",
      "Iteration 84690, Loss: 0.003526377492\n",
      "Iteration 84700, Loss: 0.003636826528\n",
      "Iteration 84710, Loss: 0.008962159045\n",
      "Iteration 84720, Loss: 0.004999083467\n",
      "Iteration 84730, Loss: 0.004213340580\n",
      "Iteration 84740, Loss: 0.003809721209\n",
      "Iteration 84750, Loss: 0.003733433317\n",
      "Iteration 84760, Loss: 0.004037152044\n",
      "Iteration 84770, Loss: 0.003902802942\n",
      "Iteration 84780, Loss: 0.003680212423\n",
      "Iteration 84790, Loss: 0.003644078039\n",
      "Iteration 84800, Loss: 0.003586557228\n",
      "Iteration 84810, Loss: 0.003538797144\n",
      "Iteration 84820, Loss: 0.003530742368\n",
      "Iteration 84830, Loss: 0.004301641136\n",
      "Iteration 84840, Loss: 0.003672450781\n",
      "Iteration 84850, Loss: 0.004187403247\n",
      "Iteration 84860, Loss: 0.003798705060\n",
      "Iteration 84870, Loss: 0.003604630474\n",
      "Iteration 84880, Loss: 0.003547155065\n",
      "Iteration 84890, Loss: 0.003529197536\n",
      "Iteration 84900, Loss: 0.003542271908\n",
      "Iteration 84910, Loss: 0.003774601733\n",
      "Iteration 84920, Loss: 0.006106750108\n",
      "Iteration 84930, Loss: 0.003937927075\n",
      "Iteration 84940, Loss: 0.003653847147\n",
      "Iteration 84950, Loss: 0.003605342237\n",
      "Iteration 84960, Loss: 0.003570358502\n",
      "Iteration 84970, Loss: 0.003580864053\n",
      "Iteration 84980, Loss: 0.004392931238\n",
      "Iteration 84990, Loss: 0.003795197001\n",
      "Iteration 85000, Loss: 0.004008361604\n",
      "Iteration 85010, Loss: 0.003660780843\n",
      "Iteration 85020, Loss: 0.003670425853\n",
      "Iteration 85030, Loss: 0.003591394750\n",
      "Iteration 85040, Loss: 0.003580195364\n",
      "Iteration 85050, Loss: 0.004281794652\n",
      "Iteration 85060, Loss: 0.003998124041\n",
      "Iteration 85070, Loss: 0.004277470522\n",
      "Iteration 85080, Loss: 0.003685297677\n",
      "Iteration 85090, Loss: 0.003584967460\n",
      "Iteration 85100, Loss: 0.003565366147\n",
      "Iteration 85110, Loss: 0.003527260851\n",
      "Iteration 85120, Loss: 0.003519342514\n",
      "Iteration 85130, Loss: 0.003816985525\n",
      "Iteration 85140, Loss: 0.004495363217\n",
      "Iteration 85150, Loss: 0.004230394028\n",
      "Iteration 85160, Loss: 0.003827645211\n",
      "Iteration 85170, Loss: 0.004385583568\n",
      "Iteration 85180, Loss: 0.003743603360\n",
      "Iteration 85190, Loss: 0.004036278930\n",
      "Iteration 85200, Loss: 0.004091821611\n",
      "Iteration 85210, Loss: 0.003709881566\n",
      "Iteration 85220, Loss: 0.003547536209\n",
      "Iteration 85230, Loss: 0.003570371540\n",
      "Iteration 85240, Loss: 0.003547221655\n",
      "Iteration 85250, Loss: 0.004152113572\n",
      "Iteration 85260, Loss: 0.004209763370\n",
      "Iteration 85270, Loss: 0.003964779899\n",
      "Iteration 85280, Loss: 0.003752721474\n",
      "Iteration 85290, Loss: 0.003532648319\n",
      "Iteration 85300, Loss: 0.003543163417\n",
      "Iteration 85310, Loss: 0.003525638953\n",
      "Iteration 85320, Loss: 0.003518980928\n",
      "Iteration 85330, Loss: 0.003821783932\n",
      "Iteration 85340, Loss: 0.005721524358\n",
      "Iteration 85350, Loss: 0.004503261298\n",
      "Iteration 85360, Loss: 0.003651415231\n",
      "Iteration 85370, Loss: 0.003565341700\n",
      "Iteration 85380, Loss: 0.003551786300\n",
      "Iteration 85390, Loss: 0.003544083331\n",
      "Iteration 85400, Loss: 0.003524239175\n",
      "Iteration 85410, Loss: 0.003522699466\n",
      "Iteration 85420, Loss: 0.003589645959\n",
      "Iteration 85430, Loss: 0.006137503311\n",
      "Iteration 85440, Loss: 0.005388693418\n",
      "Iteration 85450, Loss: 0.003852232127\n",
      "Iteration 85460, Loss: 0.003809280694\n",
      "Iteration 85470, Loss: 0.003591740504\n",
      "Iteration 85480, Loss: 0.003531238064\n",
      "Iteration 85490, Loss: 0.003542422783\n",
      "Iteration 85500, Loss: 0.003739737440\n",
      "Iteration 85510, Loss: 0.007336093113\n",
      "Iteration 85520, Loss: 0.003773404285\n",
      "Iteration 85530, Loss: 0.003779073711\n",
      "Iteration 85540, Loss: 0.003643184900\n",
      "Iteration 85550, Loss: 0.003538353834\n",
      "Iteration 85560, Loss: 0.003533591516\n",
      "Iteration 85570, Loss: 0.003547125496\n",
      "Iteration 85580, Loss: 0.005957545713\n",
      "Iteration 85590, Loss: 0.005115012638\n",
      "Iteration 85600, Loss: 0.003898038529\n",
      "Iteration 85610, Loss: 0.003613272216\n",
      "Iteration 85620, Loss: 0.003579985350\n",
      "Iteration 85630, Loss: 0.003543428378\n",
      "Iteration 85640, Loss: 0.003515672171\n",
      "Iteration 85650, Loss: 0.003512918949\n",
      "Iteration 85660, Loss: 0.003598263254\n",
      "Iteration 85670, Loss: 0.006310985889\n",
      "Iteration 85680, Loss: 0.007500098087\n",
      "Iteration 85690, Loss: 0.004033907317\n",
      "Iteration 85700, Loss: 0.003857754171\n",
      "Iteration 85710, Loss: 0.003674441949\n",
      "Iteration 85720, Loss: 0.003552023554\n",
      "Iteration 85730, Loss: 0.003536901437\n",
      "Iteration 85740, Loss: 0.003543422092\n",
      "Iteration 85750, Loss: 0.003613000736\n",
      "Iteration 85760, Loss: 0.003889289452\n",
      "Iteration 85770, Loss: 0.004348213784\n",
      "Iteration 85780, Loss: 0.003555780044\n",
      "Iteration 85790, Loss: 0.003760993946\n",
      "Iteration 85800, Loss: 0.003628925420\n",
      "Iteration 85810, Loss: 0.004725410603\n",
      "Iteration 85820, Loss: 0.003698934801\n",
      "Iteration 85830, Loss: 0.003679251764\n",
      "Iteration 85840, Loss: 0.003682984272\n",
      "Iteration 85850, Loss: 0.003567198990\n",
      "Iteration 85860, Loss: 0.003630765947\n",
      "Iteration 85870, Loss: 0.005305145867\n",
      "Iteration 85880, Loss: 0.003667139215\n",
      "Iteration 85890, Loss: 0.003862065263\n",
      "Iteration 85900, Loss: 0.003701628651\n",
      "Iteration 85910, Loss: 0.003623963334\n",
      "Iteration 85920, Loss: 0.003517305246\n",
      "Iteration 85930, Loss: 0.003531194292\n",
      "Iteration 85940, Loss: 0.007522738539\n",
      "Iteration 85950, Loss: 0.005593218375\n",
      "Iteration 85960, Loss: 0.004756231327\n",
      "Iteration 85970, Loss: 0.004358557519\n",
      "Iteration 85980, Loss: 0.004106921144\n",
      "Iteration 85990, Loss: 0.003876016941\n",
      "Iteration 86000, Loss: 0.003594073234\n",
      "Iteration 86010, Loss: 0.003572969930\n",
      "Iteration 86020, Loss: 0.003542382037\n",
      "Iteration 86030, Loss: 0.003516627010\n",
      "Iteration 86040, Loss: 0.003520452650\n",
      "Iteration 86050, Loss: 0.003640598850\n",
      "Iteration 86060, Loss: 0.007002400234\n",
      "Iteration 86070, Loss: 0.004070699215\n",
      "Iteration 86080, Loss: 0.003718725173\n",
      "Iteration 86090, Loss: 0.003595298855\n",
      "Iteration 86100, Loss: 0.003515663324\n",
      "Iteration 86110, Loss: 0.003511840478\n",
      "Iteration 86120, Loss: 0.003526531626\n",
      "Iteration 86130, Loss: 0.003797576763\n",
      "Iteration 86140, Loss: 0.005438645836\n",
      "Iteration 86150, Loss: 0.003665599739\n",
      "Iteration 86160, Loss: 0.003627044149\n",
      "Iteration 86170, Loss: 0.003625140060\n",
      "Iteration 86180, Loss: 0.003559249220\n",
      "Iteration 86190, Loss: 0.003522791667\n",
      "Iteration 86200, Loss: 0.003617533483\n",
      "Iteration 86210, Loss: 0.008720220998\n",
      "Iteration 86220, Loss: 0.003815239528\n",
      "Iteration 86230, Loss: 0.003872092813\n",
      "Iteration 86240, Loss: 0.003591527697\n",
      "Iteration 86250, Loss: 0.003595340997\n",
      "Iteration 86260, Loss: 0.003524861299\n",
      "Iteration 86270, Loss: 0.003513670061\n",
      "Iteration 86280, Loss: 0.003603728721\n",
      "Iteration 86290, Loss: 0.007743049879\n",
      "Iteration 86300, Loss: 0.004462686367\n",
      "Iteration 86310, Loss: 0.003954661544\n",
      "Iteration 86320, Loss: 0.003624658333\n",
      "Iteration 86330, Loss: 0.003537338460\n",
      "Iteration 86340, Loss: 0.003531199647\n",
      "Iteration 86350, Loss: 0.003509434173\n",
      "Iteration 86360, Loss: 0.003716100473\n",
      "Iteration 86370, Loss: 0.007044906728\n",
      "Iteration 86380, Loss: 0.004292873666\n",
      "Iteration 86390, Loss: 0.003848045133\n",
      "Iteration 86400, Loss: 0.003703658702\n",
      "Iteration 86410, Loss: 0.003576516872\n",
      "Iteration 86420, Loss: 0.003536929376\n",
      "Iteration 86430, Loss: 0.003501949133\n",
      "Iteration 86440, Loss: 0.003539975733\n",
      "Iteration 86450, Loss: 0.006401794963\n",
      "Iteration 86460, Loss: 0.005455509759\n",
      "Iteration 86470, Loss: 0.003843320301\n",
      "Iteration 86480, Loss: 0.003749529365\n",
      "Iteration 86490, Loss: 0.003619669238\n",
      "Iteration 86500, Loss: 0.003542754101\n",
      "Iteration 86510, Loss: 0.003507127287\n",
      "Iteration 86520, Loss: 0.003619257361\n",
      "Iteration 86530, Loss: 0.008426912129\n",
      "Iteration 86540, Loss: 0.004064697307\n",
      "Iteration 86550, Loss: 0.003651492530\n",
      "Iteration 86560, Loss: 0.003636972280\n",
      "Iteration 86570, Loss: 0.003556185635\n",
      "Iteration 86580, Loss: 0.003512489842\n",
      "Iteration 86590, Loss: 0.003502242500\n",
      "Iteration 86600, Loss: 0.003498170292\n",
      "Iteration 86610, Loss: 0.003553618910\n",
      "Iteration 86620, Loss: 0.010631667450\n",
      "Iteration 86630, Loss: 0.004391396418\n",
      "Iteration 86640, Loss: 0.003906812984\n",
      "Iteration 86650, Loss: 0.003756645368\n",
      "Iteration 86660, Loss: 0.003562596859\n",
      "Iteration 86670, Loss: 0.003514315234\n",
      "Iteration 86680, Loss: 0.003507359419\n",
      "Iteration 86690, Loss: 0.003504834836\n",
      "Iteration 86700, Loss: 0.003919397946\n",
      "Iteration 86710, Loss: 0.006288847886\n",
      "Iteration 86720, Loss: 0.004647394642\n",
      "Iteration 86730, Loss: 0.003568688408\n",
      "Iteration 86740, Loss: 0.003599281888\n",
      "Iteration 86750, Loss: 0.003536948701\n",
      "Iteration 86760, Loss: 0.003558234079\n",
      "Iteration 86770, Loss: 0.003761913162\n",
      "Iteration 86780, Loss: 0.007580300793\n",
      "Iteration 86790, Loss: 0.003637060290\n",
      "Iteration 86800, Loss: 0.003859015182\n",
      "Iteration 86810, Loss: 0.004463130143\n",
      "Iteration 86820, Loss: 0.004030734301\n",
      "Iteration 86830, Loss: 0.003734500380\n",
      "Iteration 86840, Loss: 0.003511869814\n",
      "Iteration 86850, Loss: 0.003688404337\n",
      "Iteration 86860, Loss: 0.007587097585\n",
      "Iteration 86870, Loss: 0.004142346326\n",
      "Iteration 86880, Loss: 0.003815803910\n",
      "Iteration 86890, Loss: 0.003619633848\n",
      "Iteration 86900, Loss: 0.003517330624\n",
      "Iteration 86910, Loss: 0.003507978283\n",
      "Iteration 86920, Loss: 0.003513825359\n",
      "Iteration 86930, Loss: 0.005802537315\n",
      "Iteration 86940, Loss: 0.005132798105\n",
      "Iteration 86950, Loss: 0.003870782908\n",
      "Iteration 86960, Loss: 0.003670000704\n",
      "Iteration 86970, Loss: 0.003533591516\n",
      "Iteration 86980, Loss: 0.003538546152\n",
      "Iteration 86990, Loss: 0.003497781232\n",
      "Iteration 87000, Loss: 0.003488559509\n",
      "Iteration 87010, Loss: 0.003497160971\n",
      "Iteration 87020, Loss: 0.004625655711\n",
      "Iteration 87030, Loss: 0.005212501623\n",
      "Iteration 87040, Loss: 0.003941865172\n",
      "Iteration 87050, Loss: 0.003810261609\n",
      "Iteration 87060, Loss: 0.003562009893\n",
      "Iteration 87070, Loss: 0.003536605975\n",
      "Iteration 87080, Loss: 0.003521517618\n",
      "Iteration 87090, Loss: 0.003803578904\n",
      "Iteration 87100, Loss: 0.003828659421\n",
      "Iteration 87110, Loss: 0.004044307396\n",
      "Iteration 87120, Loss: 0.004518470727\n",
      "Iteration 87130, Loss: 0.003686655080\n",
      "Iteration 87140, Loss: 0.003506186884\n",
      "Iteration 87150, Loss: 0.003662198316\n",
      "Iteration 87160, Loss: 0.005482236855\n",
      "Iteration 87170, Loss: 0.003827825421\n",
      "Iteration 87180, Loss: 0.003644784214\n",
      "Iteration 87190, Loss: 0.003544019535\n",
      "Iteration 87200, Loss: 0.003548353212\n",
      "Iteration 87210, Loss: 0.003599827411\n",
      "Iteration 87220, Loss: 0.007886677049\n",
      "Iteration 87230, Loss: 0.004227871075\n",
      "Iteration 87240, Loss: 0.003963058349\n",
      "Iteration 87250, Loss: 0.003560056211\n",
      "Iteration 87260, Loss: 0.003541593673\n",
      "Iteration 87270, Loss: 0.003507453250\n",
      "Iteration 87280, Loss: 0.003540673526\n",
      "Iteration 87290, Loss: 0.003802839667\n",
      "Iteration 87300, Loss: 0.009024489671\n",
      "Iteration 87310, Loss: 0.004018696491\n",
      "Iteration 87320, Loss: 0.003973468207\n",
      "Iteration 87330, Loss: 0.004198978655\n",
      "Iteration 87340, Loss: 0.004015831277\n",
      "Iteration 87350, Loss: 0.003718164284\n",
      "Iteration 87360, Loss: 0.008140022866\n",
      "Iteration 87370, Loss: 0.003938378766\n",
      "Iteration 87380, Loss: 0.004936868325\n",
      "Iteration 87390, Loss: 0.003733477322\n",
      "Iteration 87400, Loss: 0.003708314616\n",
      "Iteration 87410, Loss: 0.003539880738\n",
      "Iteration 87420, Loss: 0.003516502213\n",
      "Iteration 87430, Loss: 0.003515491961\n",
      "Iteration 87440, Loss: 0.003632231150\n",
      "Iteration 87450, Loss: 0.005805438850\n",
      "Iteration 87460, Loss: 0.004213939887\n",
      "Iteration 87470, Loss: 0.003515500808\n",
      "Iteration 87480, Loss: 0.003592442488\n",
      "Iteration 87490, Loss: 0.003541969694\n",
      "Iteration 87500, Loss: 0.003491208423\n",
      "Iteration 87510, Loss: 0.003580163233\n",
      "Iteration 87520, Loss: 0.005708643235\n",
      "Iteration 87530, Loss: 0.004430823028\n",
      "Iteration 87540, Loss: 0.003650975181\n",
      "Iteration 87550, Loss: 0.003535849974\n",
      "Iteration 87560, Loss: 0.003500366118\n",
      "Iteration 87570, Loss: 0.003508514725\n",
      "Iteration 87580, Loss: 0.003505548695\n",
      "Iteration 87590, Loss: 0.003627010155\n",
      "Iteration 87600, Loss: 0.007235833909\n",
      "Iteration 87610, Loss: 0.005704323296\n",
      "Iteration 87620, Loss: 0.004425991327\n",
      "Iteration 87630, Loss: 0.003608366707\n",
      "Iteration 87640, Loss: 0.003575078212\n",
      "Iteration 87650, Loss: 0.003552098759\n",
      "Iteration 87660, Loss: 0.003497805679\n",
      "Iteration 87670, Loss: 0.003491766984\n",
      "Iteration 87680, Loss: 0.003485052381\n",
      "Iteration 87690, Loss: 0.003597097937\n",
      "Iteration 87700, Loss: 0.003507084912\n",
      "Iteration 87710, Loss: 0.004396373872\n",
      "Iteration 87720, Loss: 0.003622366115\n",
      "Iteration 87730, Loss: 0.004314729944\n",
      "Iteration 87740, Loss: 0.003603928722\n",
      "Iteration 87750, Loss: 0.003588655731\n",
      "Iteration 87760, Loss: 0.003492209828\n",
      "Iteration 87770, Loss: 0.003499523504\n",
      "Iteration 87780, Loss: 0.003478599945\n",
      "Iteration 87790, Loss: 0.003580116201\n",
      "Iteration 87800, Loss: 0.010320807807\n",
      "Iteration 87810, Loss: 0.003976136912\n",
      "Iteration 87820, Loss: 0.003869859967\n",
      "Iteration 87830, Loss: 0.003676934866\n",
      "Iteration 87840, Loss: 0.003534316085\n",
      "Iteration 87850, Loss: 0.003491988406\n",
      "Iteration 87860, Loss: 0.003487098496\n",
      "Iteration 87870, Loss: 0.003492697608\n",
      "Iteration 87880, Loss: 0.004551870748\n",
      "Iteration 87890, Loss: 0.003604459809\n",
      "Iteration 87900, Loss: 0.004047507420\n",
      "Iteration 87910, Loss: 0.003632119158\n",
      "Iteration 87920, Loss: 0.003580685239\n",
      "Iteration 87930, Loss: 0.003498014994\n",
      "Iteration 87940, Loss: 0.003492475953\n",
      "Iteration 87950, Loss: 0.003512979019\n",
      "Iteration 87960, Loss: 0.004859834909\n",
      "Iteration 87970, Loss: 0.003989550285\n",
      "Iteration 87980, Loss: 0.003875507275\n",
      "Iteration 87990, Loss: 0.003647960257\n",
      "Iteration 88000, Loss: 0.003513408825\n",
      "Iteration 88010, Loss: 0.003512959927\n",
      "Iteration 88020, Loss: 0.003511637216\n",
      "Iteration 88030, Loss: 0.004196543247\n",
      "Iteration 88040, Loss: 0.003833299968\n",
      "Iteration 88050, Loss: 0.004154487047\n",
      "Iteration 88060, Loss: 0.003714875784\n",
      "Iteration 88070, Loss: 0.003491166281\n",
      "Iteration 88080, Loss: 0.003507930320\n",
      "Iteration 88090, Loss: 0.003491689218\n",
      "Iteration 88100, Loss: 0.003576890333\n",
      "Iteration 88110, Loss: 0.007481038105\n",
      "Iteration 88120, Loss: 0.004773950670\n",
      "Iteration 88130, Loss: 0.003942667507\n",
      "Iteration 88140, Loss: 0.003688951954\n",
      "Iteration 88150, Loss: 0.003527912311\n",
      "Iteration 88160, Loss: 0.003511916846\n",
      "Iteration 88170, Loss: 0.003474024357\n",
      "Iteration 88180, Loss: 0.003477623453\n",
      "Iteration 88190, Loss: 0.005290526897\n",
      "Iteration 88200, Loss: 0.006328522693\n",
      "Iteration 88210, Loss: 0.004268934950\n",
      "Iteration 88220, Loss: 0.003650026629\n",
      "Iteration 88230, Loss: 0.003584346268\n",
      "Iteration 88240, Loss: 0.003550647059\n",
      "Iteration 88250, Loss: 0.003487470793\n",
      "Iteration 88260, Loss: 0.003478063503\n",
      "Iteration 88270, Loss: 0.003485522931\n",
      "Iteration 88280, Loss: 0.004043057561\n",
      "Iteration 88290, Loss: 0.004367535003\n",
      "Iteration 88300, Loss: 0.003531331429\n",
      "Iteration 88310, Loss: 0.003654959146\n",
      "Iteration 88320, Loss: 0.003545251442\n",
      "Iteration 88330, Loss: 0.003480531974\n",
      "Iteration 88340, Loss: 0.003643398872\n",
      "Iteration 88350, Loss: 0.007936472073\n",
      "Iteration 88360, Loss: 0.003867327236\n",
      "Iteration 88370, Loss: 0.003764797235\n",
      "Iteration 88380, Loss: 0.003629069310\n",
      "Iteration 88390, Loss: 0.003502116771\n",
      "Iteration 88400, Loss: 0.003488578834\n",
      "Iteration 88410, Loss: 0.003483645618\n",
      "Iteration 88420, Loss: 0.004319172353\n",
      "Iteration 88430, Loss: 0.004525090568\n",
      "Iteration 88440, Loss: 0.004251798149\n",
      "Iteration 88450, Loss: 0.003973991144\n",
      "Iteration 88460, Loss: 0.005615431350\n",
      "Iteration 88470, Loss: 0.003808450885\n",
      "Iteration 88480, Loss: 0.003722940572\n",
      "Iteration 88490, Loss: 0.004307548981\n",
      "Iteration 88500, Loss: 0.003682567040\n",
      "Iteration 88510, Loss: 0.003708435223\n",
      "Iteration 88520, Loss: 0.003596960567\n",
      "Iteration 88530, Loss: 0.003519079881\n",
      "Iteration 88540, Loss: 0.003570513101\n",
      "Iteration 88550, Loss: 0.006553906016\n",
      "Iteration 88560, Loss: 0.004481760785\n",
      "Iteration 88570, Loss: 0.003609420499\n",
      "Iteration 88580, Loss: 0.003608270083\n",
      "Iteration 88590, Loss: 0.003481453750\n",
      "Iteration 88600, Loss: 0.003479566891\n",
      "Iteration 88610, Loss: 0.003478397615\n",
      "Iteration 88620, Loss: 0.003817611607\n",
      "Iteration 88630, Loss: 0.005613253452\n",
      "Iteration 88640, Loss: 0.003676316002\n",
      "Iteration 88650, Loss: 0.003659779206\n",
      "Iteration 88660, Loss: 0.003614908783\n",
      "Iteration 88670, Loss: 0.003485233057\n",
      "Iteration 88680, Loss: 0.003473313060\n",
      "Iteration 88690, Loss: 0.003577313852\n",
      "Iteration 88700, Loss: 0.009006092325\n",
      "Iteration 88710, Loss: 0.003883003490\n",
      "Iteration 88720, Loss: 0.003615462454\n",
      "Iteration 88730, Loss: 0.003514259122\n",
      "Iteration 88740, Loss: 0.003522688523\n",
      "Iteration 88750, Loss: 0.003474439261\n",
      "Iteration 88760, Loss: 0.003470820375\n",
      "Iteration 88770, Loss: 0.003778898623\n",
      "Iteration 88780, Loss: 0.005769342184\n",
      "Iteration 88790, Loss: 0.003853990231\n",
      "Iteration 88800, Loss: 0.003576359944\n",
      "Iteration 88810, Loss: 0.003549843561\n",
      "Iteration 88820, Loss: 0.003466582159\n",
      "Iteration 88830, Loss: 0.003609427484\n",
      "Iteration 88840, Loss: 0.006699638441\n",
      "Iteration 88850, Loss: 0.004396918695\n",
      "Iteration 88860, Loss: 0.003698544111\n",
      "Iteration 88870, Loss: 0.003616988659\n",
      "Iteration 88880, Loss: 0.003511784133\n",
      "Iteration 88890, Loss: 0.003491803771\n",
      "Iteration 88900, Loss: 0.003592479043\n",
      "Iteration 88910, Loss: 0.009269836359\n",
      "Iteration 88920, Loss: 0.004082308151\n",
      "Iteration 88930, Loss: 0.005351196043\n",
      "Iteration 88940, Loss: 0.005766474176\n",
      "Iteration 88950, Loss: 0.004950346891\n",
      "Iteration 88960, Loss: 0.004528448917\n",
      "Iteration 88970, Loss: 0.003950355574\n",
      "Iteration 88980, Loss: 0.003629005514\n",
      "Iteration 88990, Loss: 0.003502943087\n",
      "Iteration 89000, Loss: 0.003484267741\n",
      "Iteration 89010, Loss: 0.003469546791\n",
      "Iteration 89020, Loss: 0.003483375069\n",
      "Iteration 89030, Loss: 0.003645763034\n",
      "Iteration 89040, Loss: 0.005230665673\n",
      "Iteration 89050, Loss: 0.003484383691\n",
      "Iteration 89060, Loss: 0.003695050254\n",
      "Iteration 89070, Loss: 0.003468184499\n",
      "Iteration 89080, Loss: 0.003466463881\n",
      "Iteration 89090, Loss: 0.003460866632\n",
      "Iteration 89100, Loss: 0.003467370290\n",
      "Iteration 89110, Loss: 0.004196058027\n",
      "Iteration 89120, Loss: 0.003515626770\n",
      "Iteration 89130, Loss: 0.004084093496\n",
      "Iteration 89140, Loss: 0.003532222938\n",
      "Iteration 89150, Loss: 0.003561175428\n",
      "Iteration 89160, Loss: 0.003491397481\n",
      "Iteration 89170, Loss: 0.003483367851\n",
      "Iteration 89180, Loss: 0.003494903445\n",
      "Iteration 89190, Loss: 0.004649715498\n",
      "Iteration 89200, Loss: 0.003755135462\n",
      "Iteration 89210, Loss: 0.003892356763\n",
      "Iteration 89220, Loss: 0.003587335115\n",
      "Iteration 89230, Loss: 0.003507584101\n",
      "Iteration 89240, Loss: 0.003473237855\n",
      "Iteration 89250, Loss: 0.003500381252\n",
      "Iteration 89260, Loss: 0.004072526935\n",
      "Iteration 89270, Loss: 0.003947154619\n",
      "Iteration 89280, Loss: 0.004189637490\n",
      "Iteration 89290, Loss: 0.003682024544\n",
      "Iteration 89300, Loss: 0.003493379336\n",
      "Iteration 89310, Loss: 0.003489066614\n",
      "Iteration 89320, Loss: 0.003461247077\n",
      "Iteration 89330, Loss: 0.003482090775\n",
      "Iteration 89340, Loss: 0.006401727442\n",
      "Iteration 89350, Loss: 0.005447908770\n",
      "Iteration 89360, Loss: 0.003907329403\n",
      "Iteration 89370, Loss: 0.003604819998\n",
      "Iteration 89380, Loss: 0.003473754041\n",
      "Iteration 89390, Loss: 0.003477640450\n",
      "Iteration 89400, Loss: 0.003463762812\n",
      "Iteration 89410, Loss: 0.003454678925\n",
      "Iteration 89420, Loss: 0.003753856057\n",
      "Iteration 89430, Loss: 0.007358718663\n",
      "Iteration 89440, Loss: 0.005128500983\n",
      "Iteration 89450, Loss: 0.004534188658\n",
      "Iteration 89460, Loss: 0.004400517792\n",
      "Iteration 89470, Loss: 0.005231601186\n",
      "Iteration 89480, Loss: 0.003618178656\n",
      "Iteration 89490, Loss: 0.003698313842\n",
      "Iteration 89500, Loss: 0.003565250430\n",
      "Iteration 89510, Loss: 0.003507355927\n",
      "Iteration 89520, Loss: 0.003461282700\n",
      "Iteration 89530, Loss: 0.003530232469\n",
      "Iteration 89540, Loss: 0.005891611800\n",
      "Iteration 89550, Loss: 0.004649985116\n",
      "Iteration 89560, Loss: 0.003503625980\n",
      "Iteration 89570, Loss: 0.003587574465\n",
      "Iteration 89580, Loss: 0.003462590044\n",
      "Iteration 89590, Loss: 0.003458570223\n",
      "Iteration 89600, Loss: 0.003471439937\n",
      "Iteration 89610, Loss: 0.003556473646\n",
      "Iteration 89620, Loss: 0.006331641693\n",
      "Iteration 89630, Loss: 0.004277329892\n",
      "Iteration 89640, Loss: 0.003572018351\n",
      "Iteration 89650, Loss: 0.003494864795\n",
      "Iteration 89660, Loss: 0.003489748808\n",
      "Iteration 89670, Loss: 0.003499634564\n",
      "Iteration 89680, Loss: 0.003594177775\n",
      "Iteration 89690, Loss: 0.006903410889\n",
      "Iteration 89700, Loss: 0.004035144579\n",
      "Iteration 89710, Loss: 0.003908470273\n",
      "Iteration 89720, Loss: 0.003825280117\n",
      "Iteration 89730, Loss: 0.003538655350\n",
      "Iteration 89740, Loss: 0.003505180357\n",
      "Iteration 89750, Loss: 0.003477952443\n",
      "Iteration 89760, Loss: 0.003740350716\n",
      "Iteration 89770, Loss: 0.006725988816\n",
      "Iteration 89780, Loss: 0.003720041364\n",
      "Iteration 89790, Loss: 0.003773503471\n",
      "Iteration 89800, Loss: 0.003751455806\n",
      "Iteration 89810, Loss: 0.003509846283\n",
      "Iteration 89820, Loss: 0.003474452067\n",
      "Iteration 89830, Loss: 0.003504379420\n",
      "Iteration 89840, Loss: 0.004280043766\n",
      "Iteration 89850, Loss: 0.005067606457\n",
      "Iteration 89860, Loss: 0.003756029531\n",
      "Iteration 89870, Loss: 0.003502177075\n",
      "Iteration 89880, Loss: 0.003489326686\n",
      "Iteration 89890, Loss: 0.003466891125\n",
      "Iteration 89900, Loss: 0.003793243086\n",
      "Iteration 89910, Loss: 0.008000869304\n",
      "Iteration 89920, Loss: 0.003844040912\n",
      "Iteration 89930, Loss: 0.003893862478\n",
      "Iteration 89940, Loss: 0.003541934770\n",
      "Iteration 89950, Loss: 0.003466612892\n",
      "Iteration 89960, Loss: 0.003469899297\n",
      "Iteration 89970, Loss: 0.003455393948\n",
      "Iteration 89980, Loss: 0.004433527123\n",
      "Iteration 89990, Loss: 0.003663167125\n",
      "Iteration 90000, Loss: 0.004195736255\n",
      "Iteration 90010, Loss: 0.003506901441\n",
      "Iteration 90020, Loss: 0.003556433832\n",
      "Iteration 90030, Loss: 0.003504927503\n",
      "Iteration 90040, Loss: 0.003454041900\n",
      "Iteration 90050, Loss: 0.003456093837\n",
      "Iteration 90060, Loss: 0.003770339768\n",
      "Iteration 90070, Loss: 0.005367772654\n",
      "Iteration 90080, Loss: 0.006887887605\n",
      "Iteration 90090, Loss: 0.004470068496\n",
      "Iteration 90100, Loss: 0.003899453441\n",
      "Iteration 90110, Loss: 0.003639664035\n",
      "Iteration 90120, Loss: 0.003496322781\n",
      "Iteration 90130, Loss: 0.003483530134\n",
      "Iteration 90140, Loss: 0.003456107574\n",
      "Iteration 90150, Loss: 0.003446862567\n",
      "Iteration 90160, Loss: 0.003443261376\n",
      "Iteration 90170, Loss: 0.003801111598\n",
      "Iteration 90180, Loss: 0.006669211201\n",
      "Iteration 90190, Loss: 0.003787631402\n",
      "Iteration 90200, Loss: 0.003697308246\n",
      "Iteration 90210, Loss: 0.003542755963\n",
      "Iteration 90220, Loss: 0.003454608377\n",
      "Iteration 90230, Loss: 0.003533581970\n",
      "Iteration 90240, Loss: 0.005030015483\n",
      "Iteration 90250, Loss: 0.003723568982\n",
      "Iteration 90260, Loss: 0.003648189595\n",
      "Iteration 90270, Loss: 0.003630825086\n",
      "Iteration 90280, Loss: 0.003523795400\n",
      "Iteration 90290, Loss: 0.004168609157\n",
      "Iteration 90300, Loss: 0.003943887539\n",
      "Iteration 90310, Loss: 0.003990150057\n",
      "Iteration 90320, Loss: 0.003610526677\n",
      "Iteration 90330, Loss: 0.003578057745\n",
      "Iteration 90340, Loss: 0.003636211623\n",
      "Iteration 90350, Loss: 0.003800005885\n",
      "Iteration 90360, Loss: 0.005475482903\n",
      "Iteration 90370, Loss: 0.003698156914\n",
      "Iteration 90380, Loss: 0.003610551124\n",
      "Iteration 90390, Loss: 0.003498311155\n",
      "Iteration 90400, Loss: 0.003474217607\n",
      "Iteration 90410, Loss: 0.003518054029\n",
      "Iteration 90420, Loss: 0.006219464354\n",
      "Iteration 90430, Loss: 0.005021906458\n",
      "Iteration 90440, Loss: 0.003786118934\n",
      "Iteration 90450, Loss: 0.003583683865\n",
      "Iteration 90460, Loss: 0.003477809019\n",
      "Iteration 90470, Loss: 0.003447572002\n",
      "Iteration 90480, Loss: 0.003443215275\n",
      "Iteration 90490, Loss: 0.003635762725\n",
      "Iteration 90500, Loss: 0.006946827285\n",
      "Iteration 90510, Loss: 0.003777531441\n",
      "Iteration 90520, Loss: 0.003629274201\n",
      "Iteration 90530, Loss: 0.003635529662\n",
      "Iteration 90540, Loss: 0.003518995829\n",
      "Iteration 90550, Loss: 0.003489452414\n",
      "Iteration 90560, Loss: 0.003491669893\n",
      "Iteration 90570, Loss: 0.004058788065\n",
      "Iteration 90580, Loss: 0.004754527472\n",
      "Iteration 90590, Loss: 0.003685071599\n",
      "Iteration 90600, Loss: 0.003519108985\n",
      "Iteration 90610, Loss: 0.003491961397\n",
      "Iteration 90620, Loss: 0.005081068259\n",
      "Iteration 90630, Loss: 0.003860977711\n",
      "Iteration 90640, Loss: 0.003934519831\n",
      "Iteration 90650, Loss: 0.005002785940\n",
      "Iteration 90660, Loss: 0.003909161780\n",
      "Iteration 90670, Loss: 0.005879603326\n",
      "Iteration 90680, Loss: 0.005120690912\n",
      "Iteration 90690, Loss: 0.004928650800\n",
      "Iteration 90700, Loss: 0.004141726997\n",
      "Iteration 90710, Loss: 0.009749736637\n",
      "Iteration 90720, Loss: 0.004051837139\n",
      "Iteration 90730, Loss: 0.004528370220\n",
      "Iteration 90740, Loss: 0.003898445982\n",
      "Iteration 90750, Loss: 0.003594749374\n",
      "Iteration 90760, Loss: 0.003529121634\n",
      "Iteration 90770, Loss: 0.003466845024\n",
      "Iteration 90780, Loss: 0.003459938569\n",
      "Iteration 90790, Loss: 0.003449347569\n",
      "Iteration 90800, Loss: 0.003441649722\n",
      "Iteration 90810, Loss: 0.003443838796\n",
      "Iteration 90820, Loss: 0.003763970220\n",
      "Iteration 90830, Loss: 0.003675887361\n",
      "Iteration 90840, Loss: 0.004008264281\n",
      "Iteration 90850, Loss: 0.003651544685\n",
      "Iteration 90860, Loss: 0.003462581895\n",
      "Iteration 90870, Loss: 0.003477455815\n",
      "Iteration 90880, Loss: 0.003451277968\n",
      "Iteration 90890, Loss: 0.003437832696\n",
      "Iteration 90900, Loss: 0.003495536745\n",
      "Iteration 90910, Loss: 0.005671392195\n",
      "Iteration 90920, Loss: 0.004149686079\n",
      "Iteration 90930, Loss: 0.003518102691\n",
      "Iteration 90940, Loss: 0.003479754087\n",
      "Iteration 90950, Loss: 0.003464490874\n",
      "Iteration 90960, Loss: 0.003435641062\n",
      "Iteration 90970, Loss: 0.003435289953\n",
      "Iteration 90980, Loss: 0.004728688393\n",
      "Iteration 90990, Loss: 0.004474209156\n",
      "Iteration 91000, Loss: 0.003535350319\n",
      "Iteration 91010, Loss: 0.003582353704\n",
      "Iteration 91020, Loss: 0.003576041898\n",
      "Iteration 91030, Loss: 0.003470882075\n",
      "Iteration 91040, Loss: 0.003445362207\n",
      "Iteration 91050, Loss: 0.003436623607\n",
      "Iteration 91060, Loss: 0.003773498116\n",
      "Iteration 91070, Loss: 0.005749172065\n",
      "Iteration 91080, Loss: 0.004190643784\n",
      "Iteration 91090, Loss: 0.003751795506\n",
      "Iteration 91100, Loss: 0.003468442941\n",
      "Iteration 91110, Loss: 0.003444038564\n",
      "Iteration 91120, Loss: 0.003592717694\n",
      "Iteration 91130, Loss: 0.003908894490\n",
      "Iteration 91140, Loss: 0.005332717672\n",
      "Iteration 91150, Loss: 0.003592638066\n",
      "Iteration 91160, Loss: 0.003684796160\n",
      "Iteration 91170, Loss: 0.003533500014\n",
      "Iteration 91180, Loss: 0.003490585368\n",
      "Iteration 91190, Loss: 0.003446980612\n",
      "Iteration 91200, Loss: 0.003647716250\n",
      "Iteration 91210, Loss: 0.008050538599\n",
      "Iteration 91220, Loss: 0.004248448182\n",
      "Iteration 91230, Loss: 0.003898255993\n",
      "Iteration 91240, Loss: 0.003623088123\n",
      "Iteration 91250, Loss: 0.003474852303\n",
      "Iteration 91260, Loss: 0.003442715621\n",
      "Iteration 91270, Loss: 0.003434853861\n",
      "Iteration 91280, Loss: 0.003521756968\n",
      "Iteration 91290, Loss: 0.008702171966\n",
      "Iteration 91300, Loss: 0.004059910774\n",
      "Iteration 91310, Loss: 0.003934776410\n",
      "Iteration 91320, Loss: 0.003488445887\n",
      "Iteration 91330, Loss: 0.003499964019\n",
      "Iteration 91340, Loss: 0.003459939035\n",
      "Iteration 91350, Loss: 0.003437136998\n",
      "Iteration 91360, Loss: 0.003512570169\n",
      "Iteration 91370, Loss: 0.007125226315\n",
      "Iteration 91380, Loss: 0.004442552570\n",
      "Iteration 91390, Loss: 0.003731771139\n",
      "Iteration 91400, Loss: 0.003536875127\n",
      "Iteration 91410, Loss: 0.003486296861\n",
      "Iteration 91420, Loss: 0.003453565296\n",
      "Iteration 91430, Loss: 0.003484898480\n",
      "Iteration 91440, Loss: 0.004482355434\n",
      "Iteration 91450, Loss: 0.004044519737\n",
      "Iteration 91460, Loss: 0.003498084145\n",
      "Iteration 91470, Loss: 0.003470976371\n",
      "Iteration 91480, Loss: 0.003425432136\n",
      "Iteration 91490, Loss: 0.003636974143\n",
      "Iteration 91500, Loss: 0.005705500022\n",
      "Iteration 91510, Loss: 0.004832690116\n",
      "Iteration 91520, Loss: 0.010187948123\n",
      "Iteration 91530, Loss: 0.005549209658\n",
      "Iteration 91540, Loss: 0.005112958606\n",
      "Iteration 91550, Loss: 0.004185043741\n",
      "Iteration 91560, Loss: 0.003982937895\n",
      "Iteration 91570, Loss: 0.012749461457\n",
      "Iteration 91580, Loss: 0.004803869873\n",
      "Iteration 91590, Loss: 0.003964188974\n",
      "Iteration 91600, Loss: 0.003882521065\n",
      "Iteration 91610, Loss: 0.003633379238\n",
      "Iteration 91620, Loss: 0.003557483200\n",
      "Iteration 91630, Loss: 0.003519478254\n",
      "Iteration 91640, Loss: 0.003489921801\n",
      "Iteration 91650, Loss: 0.003469754243\n",
      "Iteration 91660, Loss: 0.003454628633\n",
      "Iteration 91670, Loss: 0.003445228562\n",
      "Iteration 91680, Loss: 0.003438459476\n",
      "Iteration 91690, Loss: 0.003433332546\n",
      "Iteration 91700, Loss: 0.003430691082\n",
      "Iteration 91710, Loss: 0.003753123339\n",
      "Iteration 91720, Loss: 0.003615519963\n",
      "Iteration 91730, Loss: 0.003600467928\n",
      "Iteration 91740, Loss: 0.003749101656\n",
      "Iteration 91750, Loss: 0.003589868778\n",
      "Iteration 91760, Loss: 0.003487968817\n",
      "Iteration 91770, Loss: 0.003444727277\n",
      "Iteration 91780, Loss: 0.003437329084\n",
      "Iteration 91790, Loss: 0.003427578835\n",
      "Iteration 91800, Loss: 0.003423610236\n",
      "Iteration 91810, Loss: 0.003420144785\n",
      "Iteration 91820, Loss: 0.003443034133\n",
      "Iteration 91830, Loss: 0.007176602259\n",
      "Iteration 91840, Loss: 0.004424669780\n",
      "Iteration 91850, Loss: 0.003817574354\n",
      "Iteration 91860, Loss: 0.003486023052\n",
      "Iteration 91870, Loss: 0.003476177575\n",
      "Iteration 91880, Loss: 0.003449825337\n",
      "Iteration 91890, Loss: 0.003441863693\n",
      "Iteration 91900, Loss: 0.003457077313\n",
      "Iteration 91910, Loss: 0.004470956977\n",
      "Iteration 91920, Loss: 0.003576947842\n",
      "Iteration 91930, Loss: 0.003766874084\n",
      "Iteration 91940, Loss: 0.003590616630\n",
      "Iteration 91950, Loss: 0.003459216096\n",
      "Iteration 91960, Loss: 0.003450341988\n",
      "Iteration 91970, Loss: 0.003517480567\n",
      "Iteration 91980, Loss: 0.004545531236\n",
      "Iteration 91990, Loss: 0.003683607560\n",
      "Iteration 92000, Loss: 0.003990577534\n",
      "Iteration 92010, Loss: 0.003583208658\n",
      "Iteration 92020, Loss: 0.003498533508\n",
      "Iteration 92030, Loss: 0.003434837097\n",
      "Iteration 92040, Loss: 0.003855527844\n",
      "Iteration 92050, Loss: 0.003569820197\n",
      "Iteration 92060, Loss: 0.005111633800\n",
      "Iteration 92070, Loss: 0.003883279860\n",
      "Iteration 92080, Loss: 0.003870524932\n",
      "Iteration 92090, Loss: 0.003750284901\n",
      "Iteration 92100, Loss: 0.003497927217\n",
      "Iteration 92110, Loss: 0.003451747354\n",
      "Iteration 92120, Loss: 0.003454967635\n",
      "Iteration 92130, Loss: 0.003504135646\n",
      "Iteration 92140, Loss: 0.006435865536\n",
      "Iteration 92150, Loss: 0.005217219703\n",
      "Iteration 92160, Loss: 0.003559691831\n",
      "Iteration 92170, Loss: 0.003610863118\n",
      "Iteration 92180, Loss: 0.003459210042\n",
      "Iteration 92190, Loss: 0.003618202638\n",
      "Iteration 92200, Loss: 0.003466426628\n",
      "Iteration 92210, Loss: 0.003457919462\n",
      "Iteration 92220, Loss: 0.005835243035\n",
      "Iteration 92230, Loss: 0.005706943572\n",
      "Iteration 92240, Loss: 0.003896956798\n",
      "Iteration 92250, Loss: 0.003568653716\n",
      "Iteration 92260, Loss: 0.003520903178\n",
      "Iteration 92270, Loss: 0.003462499473\n",
      "Iteration 92280, Loss: 0.003441268345\n",
      "Iteration 92290, Loss: 0.003414462088\n",
      "Iteration 92300, Loss: 0.003682844806\n",
      "Iteration 92310, Loss: 0.006429818459\n",
      "Iteration 92320, Loss: 0.004218257964\n",
      "Iteration 92330, Loss: 0.003562399419\n",
      "Iteration 92340, Loss: 0.003578773001\n",
      "Iteration 92350, Loss: 0.003523380030\n",
      "Iteration 92360, Loss: 0.003427929012\n",
      "Iteration 92370, Loss: 0.004049438052\n",
      "Iteration 92380, Loss: 0.005315994844\n",
      "Iteration 92390, Loss: 0.003606337821\n",
      "Iteration 92400, Loss: 0.003720759414\n",
      "Iteration 92410, Loss: 0.003549216781\n",
      "Iteration 92420, Loss: 0.003540772479\n",
      "Iteration 92430, Loss: 0.003409709083\n",
      "Iteration 92440, Loss: 0.003554522991\n",
      "Iteration 92450, Loss: 0.008435169235\n",
      "Iteration 92460, Loss: 0.003894705325\n",
      "Iteration 92470, Loss: 0.003722822294\n",
      "Iteration 92480, Loss: 0.003622975200\n",
      "Iteration 92490, Loss: 0.003464043839\n",
      "Iteration 92500, Loss: 0.003433748148\n",
      "Iteration 92510, Loss: 0.003421866568\n",
      "Iteration 92520, Loss: 0.003482467728\n",
      "Iteration 92530, Loss: 0.007114693057\n",
      "Iteration 92540, Loss: 0.004509632941\n",
      "Iteration 92550, Loss: 0.003686179407\n",
      "Iteration 92560, Loss: 0.003533856245\n",
      "Iteration 92570, Loss: 0.003447867697\n",
      "Iteration 92580, Loss: 0.003432065248\n",
      "Iteration 92590, Loss: 0.003428871743\n",
      "Iteration 92600, Loss: 0.003968615085\n",
      "Iteration 92610, Loss: 0.005451252218\n",
      "Iteration 92620, Loss: 0.003894199384\n",
      "Iteration 92630, Loss: 0.003604205558\n",
      "Iteration 92640, Loss: 0.003477455350\n",
      "Iteration 92650, Loss: 0.003467614762\n",
      "Iteration 92660, Loss: 0.003524156753\n",
      "Iteration 92670, Loss: 0.003997777589\n",
      "Iteration 92680, Loss: 0.004334286787\n",
      "Iteration 92690, Loss: 0.003811186645\n",
      "Iteration 92700, Loss: 0.003495598678\n",
      "Iteration 92710, Loss: 0.003596065333\n",
      "Iteration 92720, Loss: 0.004323596135\n",
      "Iteration 92730, Loss: 0.003845726606\n",
      "Iteration 92740, Loss: 0.003678400768\n",
      "Iteration 92750, Loss: 0.003480343148\n",
      "Iteration 92760, Loss: 0.003715492785\n",
      "Iteration 92770, Loss: 0.005997612607\n",
      "Iteration 92780, Loss: 0.004210003652\n",
      "Iteration 92790, Loss: 0.003572999034\n",
      "Iteration 92800, Loss: 0.003467380302\n",
      "Iteration 92810, Loss: 0.003879063996\n",
      "Iteration 92820, Loss: 0.003732879413\n",
      "Iteration 92830, Loss: 0.004277625121\n",
      "Iteration 92840, Loss: 0.003662931966\n",
      "Iteration 92850, Loss: 0.003639299423\n",
      "Iteration 92860, Loss: 0.003599348478\n",
      "Iteration 92870, Loss: 0.003471923061\n",
      "Iteration 92880, Loss: 0.003546620253\n",
      "Iteration 92890, Loss: 0.009292468429\n",
      "Iteration 92900, Loss: 0.004252027720\n",
      "Iteration 92910, Loss: 0.009199537337\n",
      "Iteration 92920, Loss: 0.005867356434\n",
      "Iteration 92930, Loss: 0.004476678092\n",
      "Iteration 92940, Loss: 0.003790226532\n",
      "Iteration 92950, Loss: 0.003541403916\n",
      "Iteration 92960, Loss: 0.003486428875\n",
      "Iteration 92970, Loss: 0.003434252925\n",
      "Iteration 92980, Loss: 0.003421075642\n",
      "Iteration 92990, Loss: 0.003409437602\n",
      "Iteration 93000, Loss: 0.003407798940\n",
      "Iteration 93010, Loss: 0.003474839730\n",
      "Iteration 93020, Loss: 0.008056174032\n",
      "Iteration 93030, Loss: 0.003717884654\n",
      "Iteration 93040, Loss: 0.003728890093\n",
      "Iteration 93050, Loss: 0.003558156546\n",
      "Iteration 93060, Loss: 0.003437086940\n",
      "Iteration 93070, Loss: 0.003441616194\n",
      "Iteration 93080, Loss: 0.003406438045\n",
      "Iteration 93090, Loss: 0.003497236874\n",
      "Iteration 93100, Loss: 0.005720778368\n",
      "Iteration 93110, Loss: 0.003959685564\n",
      "Iteration 93120, Loss: 0.003609135048\n",
      "Iteration 93130, Loss: 0.003464565612\n",
      "Iteration 93140, Loss: 0.003447519848\n",
      "Iteration 93150, Loss: 0.003711247817\n",
      "Iteration 93160, Loss: 0.005887207575\n",
      "Iteration 93170, Loss: 0.003944858443\n",
      "Iteration 93180, Loss: 0.003491635667\n",
      "Iteration 93190, Loss: 0.003427974414\n",
      "Iteration 93200, Loss: 0.003522084327\n",
      "Iteration 93210, Loss: 0.003598071169\n",
      "Iteration 93220, Loss: 0.007209128235\n",
      "Iteration 93230, Loss: 0.003504499095\n",
      "Iteration 93240, Loss: 0.003911242355\n",
      "Iteration 93250, Loss: 0.003630794818\n",
      "Iteration 93260, Loss: 0.003577444702\n",
      "Iteration 93270, Loss: 0.003475247882\n",
      "Iteration 93280, Loss: 0.003486067057\n",
      "Iteration 93290, Loss: 0.003883313620\n",
      "Iteration 93300, Loss: 0.004530775361\n",
      "Iteration 93310, Loss: 0.003603166435\n",
      "Iteration 93320, Loss: 0.003568887711\n",
      "Iteration 93330, Loss: 0.003509348957\n",
      "Iteration 93340, Loss: 0.003452049335\n",
      "Iteration 93350, Loss: 0.003451702883\n",
      "Iteration 93360, Loss: 0.005713439547\n",
      "Iteration 93370, Loss: 0.004698597360\n",
      "Iteration 93380, Loss: 0.003748594783\n",
      "Iteration 93390, Loss: 0.003582685720\n",
      "Iteration 93400, Loss: 0.003456532257\n",
      "Iteration 93410, Loss: 0.003452463774\n",
      "Iteration 93420, Loss: 0.003679940477\n",
      "Iteration 93430, Loss: 0.004665089771\n",
      "Iteration 93440, Loss: 0.003720523091\n",
      "Iteration 93450, Loss: 0.003596393391\n",
      "Iteration 93460, Loss: 0.003496660152\n",
      "Iteration 93470, Loss: 0.003441396868\n",
      "Iteration 93480, Loss: 0.003419405548\n",
      "Iteration 93490, Loss: 0.004039430525\n",
      "Iteration 93500, Loss: 0.003559485311\n",
      "Iteration 93510, Loss: 0.004638506100\n",
      "Iteration 93520, Loss: 0.004298336804\n",
      "Iteration 93530, Loss: 0.003850829788\n",
      "Iteration 93540, Loss: 0.003989264369\n",
      "Iteration 93550, Loss: 0.003780147526\n",
      "Iteration 93560, Loss: 0.003595056944\n",
      "Iteration 93570, Loss: 0.003527257591\n",
      "Iteration 93580, Loss: 0.003419777378\n",
      "Iteration 93590, Loss: 0.004514774773\n",
      "Iteration 93600, Loss: 0.003856030758\n",
      "Iteration 93610, Loss: 0.003853292670\n",
      "Iteration 93620, Loss: 0.003660446964\n",
      "Iteration 93630, Loss: 0.003437601030\n",
      "Iteration 93640, Loss: 0.003441518871\n",
      "Iteration 93650, Loss: 0.003407290904\n",
      "Iteration 93660, Loss: 0.003400328103\n",
      "Iteration 93670, Loss: 0.003693319857\n",
      "Iteration 93680, Loss: 0.006796835456\n",
      "Iteration 93690, Loss: 0.003752439050\n",
      "Iteration 93700, Loss: 0.003751714481\n",
      "Iteration 93710, Loss: 0.003468266688\n",
      "Iteration 93720, Loss: 0.003463931149\n",
      "Iteration 93730, Loss: 0.003421789967\n",
      "Iteration 93740, Loss: 0.003399984213\n",
      "Iteration 93750, Loss: 0.003566550557\n",
      "Iteration 93760, Loss: 0.006927245297\n",
      "Iteration 93770, Loss: 0.004330199677\n",
      "Iteration 93780, Loss: 0.003660259768\n",
      "Iteration 93790, Loss: 0.003477712395\n",
      "Iteration 93800, Loss: 0.003474779427\n",
      "Iteration 93810, Loss: 0.003403249895\n",
      "Iteration 93820, Loss: 0.003417850938\n",
      "Iteration 93830, Loss: 0.003543260507\n",
      "Iteration 93840, Loss: 0.003763790475\n",
      "Iteration 93850, Loss: 0.004769662861\n",
      "Iteration 93860, Loss: 0.003685395001\n",
      "Iteration 93870, Loss: 0.003431924386\n",
      "Iteration 93880, Loss: 0.003575214650\n",
      "Iteration 93890, Loss: 0.003501250874\n",
      "Iteration 93900, Loss: 0.004332738928\n",
      "Iteration 93910, Loss: 0.003940379247\n",
      "Iteration 93920, Loss: 0.005371194798\n",
      "Iteration 93930, Loss: 0.007422989234\n",
      "Iteration 93940, Loss: 0.004957505967\n",
      "Iteration 93950, Loss: 0.004363236483\n",
      "Iteration 93960, Loss: 0.003597945208\n",
      "Iteration 93970, Loss: 0.003576990217\n",
      "Iteration 93980, Loss: 0.003443538910\n",
      "Iteration 93990, Loss: 0.003418279812\n",
      "Iteration 94000, Loss: 0.003408620367\n",
      "Iteration 94010, Loss: 0.003596351482\n",
      "Iteration 94020, Loss: 0.005690944847\n",
      "Iteration 94030, Loss: 0.003696109401\n",
      "Iteration 94040, Loss: 0.003424211871\n",
      "Iteration 94050, Loss: 0.003511264687\n",
      "Iteration 94060, Loss: 0.003403948853\n",
      "Iteration 94070, Loss: 0.003391852602\n",
      "Iteration 94080, Loss: 0.003437881358\n",
      "Iteration 94090, Loss: 0.003795291996\n",
      "Iteration 94100, Loss: 0.004992146511\n",
      "Iteration 94110, Loss: 0.004000282381\n",
      "Iteration 94120, Loss: 0.003650783328\n",
      "Iteration 94130, Loss: 0.003434069222\n",
      "Iteration 94140, Loss: 0.003426307580\n",
      "Iteration 94150, Loss: 0.003385714488\n",
      "Iteration 94160, Loss: 0.003823715961\n",
      "Iteration 94170, Loss: 0.004266733304\n",
      "Iteration 94180, Loss: 0.004315018188\n",
      "Iteration 94190, Loss: 0.003507057205\n",
      "Iteration 94200, Loss: 0.003482484259\n",
      "Iteration 94210, Loss: 0.003434862942\n",
      "Iteration 94220, Loss: 0.003408356803\n",
      "Iteration 94230, Loss: 0.003442463465\n",
      "Iteration 94240, Loss: 0.004024891648\n",
      "Iteration 94250, Loss: 0.003765497124\n",
      "Iteration 94260, Loss: 0.003631515661\n",
      "Iteration 94270, Loss: 0.003630454652\n",
      "Iteration 94280, Loss: 0.003803644795\n",
      "Iteration 94290, Loss: 0.003519045655\n",
      "Iteration 94300, Loss: 0.003496051300\n",
      "Iteration 94310, Loss: 0.004734955728\n",
      "Iteration 94320, Loss: 0.003965171520\n",
      "Iteration 94330, Loss: 0.004012704361\n",
      "Iteration 94340, Loss: 0.003545752494\n",
      "Iteration 94350, Loss: 0.003525253851\n",
      "Iteration 94360, Loss: 0.003448404139\n",
      "Iteration 94370, Loss: 0.003381625284\n",
      "Iteration 94380, Loss: 0.003378717229\n",
      "Iteration 94390, Loss: 0.006684313063\n",
      "Iteration 94400, Loss: 0.013884628192\n",
      "Iteration 94410, Loss: 0.008756743744\n",
      "Iteration 94420, Loss: 0.005939884577\n",
      "Iteration 94430, Loss: 0.008166821674\n",
      "Iteration 94440, Loss: 0.006396319252\n",
      "Iteration 94450, Loss: 0.005174834747\n",
      "Iteration 94460, Loss: 0.004277422093\n",
      "Iteration 94470, Loss: 0.003875056747\n",
      "Iteration 94480, Loss: 0.003663912183\n",
      "Iteration 94490, Loss: 0.003613084555\n",
      "Iteration 94500, Loss: 0.003546904773\n",
      "Iteration 94510, Loss: 0.003509107744\n",
      "Iteration 94520, Loss: 0.003477237187\n",
      "Iteration 94530, Loss: 0.003449329641\n",
      "Iteration 94540, Loss: 0.003429574426\n",
      "Iteration 94550, Loss: 0.003422317095\n",
      "Iteration 94560, Loss: 0.003992518876\n",
      "Iteration 94570, Loss: 0.003413408995\n",
      "Iteration 94580, Loss: 0.003439757507\n",
      "Iteration 94590, Loss: 0.003631013213\n",
      "Iteration 94600, Loss: 0.004913872108\n",
      "Iteration 94610, Loss: 0.003466367023\n",
      "Iteration 94620, Loss: 0.003507708199\n",
      "Iteration 94630, Loss: 0.003475458128\n",
      "Iteration 94640, Loss: 0.003428184194\n",
      "Iteration 94650, Loss: 0.003437116742\n",
      "Iteration 94660, Loss: 0.004178108647\n",
      "Iteration 94670, Loss: 0.003449191572\n",
      "Iteration 94680, Loss: 0.003701791400\n",
      "Iteration 94690, Loss: 0.003454037942\n",
      "Iteration 94700, Loss: 0.003404722782\n",
      "Iteration 94710, Loss: 0.003443480236\n",
      "Iteration 94720, Loss: 0.003811356612\n",
      "Iteration 94730, Loss: 0.004153296351\n",
      "Iteration 94740, Loss: 0.003488632385\n",
      "Iteration 94750, Loss: 0.003402802860\n",
      "Iteration 94760, Loss: 0.003415000625\n",
      "Iteration 94770, Loss: 0.003411467187\n",
      "Iteration 94780, Loss: 0.005513824522\n",
      "Iteration 94790, Loss: 0.004288351163\n",
      "Iteration 94800, Loss: 0.003456613049\n",
      "Iteration 94810, Loss: 0.003557974240\n",
      "Iteration 94820, Loss: 0.003414397128\n",
      "Iteration 94830, Loss: 0.003385458374\n",
      "Iteration 94840, Loss: 0.003413541242\n",
      "Iteration 94850, Loss: 0.004511525854\n",
      "Iteration 94860, Loss: 0.003700240050\n",
      "Iteration 94870, Loss: 0.004676792771\n",
      "Iteration 94880, Loss: 0.004675843753\n",
      "Iteration 94890, Loss: 0.003547492670\n",
      "Iteration 94900, Loss: 0.003489317838\n",
      "Iteration 94910, Loss: 0.003604440484\n",
      "Iteration 94920, Loss: 0.003410197794\n",
      "Iteration 94930, Loss: 0.003444490489\n",
      "Iteration 94940, Loss: 0.008341176435\n",
      "Iteration 94950, Loss: 0.003599651856\n",
      "Iteration 94960, Loss: 0.003493132535\n",
      "Iteration 94970, Loss: 0.003432302037\n",
      "Iteration 94980, Loss: 0.003409956349\n",
      "Iteration 94990, Loss: 0.003458472900\n",
      "Iteration 95000, Loss: 0.003413383616\n",
      "Iteration 95010, Loss: 0.003425510833\n",
      "Iteration 95020, Loss: 0.003374511376\n",
      "Iteration 95030, Loss: 0.003381459741\n",
      "Iteration 95040, Loss: 0.005790383555\n",
      "Iteration 95050, Loss: 0.006323372945\n",
      "Iteration 95060, Loss: 0.004269354045\n",
      "Iteration 95070, Loss: 0.003734263591\n",
      "Iteration 95080, Loss: 0.003521346254\n",
      "Iteration 95090, Loss: 0.003439252498\n",
      "Iteration 95100, Loss: 0.003453893121\n",
      "Iteration 95110, Loss: 0.003439468797\n",
      "Iteration 95120, Loss: 0.003403667593\n",
      "Iteration 95130, Loss: 0.003375004278\n",
      "Iteration 95140, Loss: 0.003916880582\n",
      "Iteration 95150, Loss: 0.004616356455\n",
      "Iteration 95160, Loss: 0.003675911343\n",
      "Iteration 95170, Loss: 0.003690287936\n",
      "Iteration 95180, Loss: 0.003534888616\n",
      "Iteration 95190, Loss: 0.003618245944\n",
      "Iteration 95200, Loss: 0.003468098352\n",
      "Iteration 95210, Loss: 0.003408940509\n",
      "Iteration 95220, Loss: 0.003392168088\n",
      "Iteration 95230, Loss: 0.003474046942\n",
      "Iteration 95240, Loss: 0.004573878832\n",
      "Iteration 95250, Loss: 0.003447132651\n",
      "Iteration 95260, Loss: 0.003462093184\n",
      "Iteration 95270, Loss: 0.003419173881\n",
      "Iteration 95280, Loss: 0.003376499284\n",
      "Iteration 95290, Loss: 0.003614427755\n",
      "Iteration 95300, Loss: 0.004389147274\n",
      "Iteration 95310, Loss: 0.004150195047\n",
      "Iteration 95320, Loss: 0.003622045042\n",
      "Iteration 95330, Loss: 0.003496804973\n",
      "Iteration 95340, Loss: 0.003533393610\n",
      "Iteration 95350, Loss: 0.003396136919\n",
      "Iteration 95360, Loss: 0.003413587343\n",
      "Iteration 95370, Loss: 0.003410664387\n",
      "Iteration 95380, Loss: 0.004632280674\n",
      "Iteration 95390, Loss: 0.003614775138\n",
      "Iteration 95400, Loss: 0.003724548500\n",
      "Iteration 95410, Loss: 0.003539386438\n",
      "Iteration 95420, Loss: 0.003468494862\n",
      "Iteration 95430, Loss: 0.003367849626\n",
      "Iteration 95440, Loss: 0.003433475969\n",
      "Iteration 95450, Loss: 0.008411182091\n",
      "Iteration 95460, Loss: 0.003795306198\n",
      "Iteration 95470, Loss: 0.006456909236\n",
      "Iteration 95480, Loss: 0.008185916580\n",
      "Iteration 95490, Loss: 0.005629432388\n",
      "Iteration 95500, Loss: 0.004573976155\n",
      "Iteration 95510, Loss: 0.003844938939\n",
      "Iteration 95520, Loss: 0.003496688092\n",
      "Iteration 95530, Loss: 0.003499685321\n",
      "Iteration 95540, Loss: 0.003443609923\n",
      "Iteration 95550, Loss: 0.003399800509\n",
      "Iteration 95560, Loss: 0.003415731015\n",
      "Iteration 95570, Loss: 0.004732955247\n",
      "Iteration 95580, Loss: 0.004494495224\n",
      "Iteration 95590, Loss: 0.003410694422\n",
      "Iteration 95600, Loss: 0.003444820410\n",
      "Iteration 95610, Loss: 0.003438832005\n",
      "Iteration 95620, Loss: 0.003406966105\n",
      "Iteration 95630, Loss: 0.003378516063\n",
      "Iteration 95640, Loss: 0.003457052633\n",
      "Iteration 95650, Loss: 0.004788444377\n",
      "Iteration 95660, Loss: 0.003792133182\n",
      "Iteration 95670, Loss: 0.003546736669\n",
      "Iteration 95680, Loss: 0.003468895564\n",
      "Iteration 95690, Loss: 0.003404715797\n",
      "Iteration 95700, Loss: 0.003467079019\n",
      "Iteration 95710, Loss: 0.003381350078\n",
      "Iteration 95720, Loss: 0.014241073281\n",
      "Iteration 95730, Loss: 0.030544683337\n",
      "Iteration 95740, Loss: 0.013710845262\n",
      "Iteration 95750, Loss: 0.009098073468\n",
      "Iteration 95760, Loss: 0.006364663597\n",
      "Iteration 95770, Loss: 0.005093022715\n",
      "Iteration 95780, Loss: 0.004417717922\n",
      "Iteration 95790, Loss: 0.003987084609\n",
      "Iteration 95800, Loss: 0.003823316656\n",
      "Iteration 95810, Loss: 0.003714913968\n",
      "Iteration 95820, Loss: 0.003638790688\n",
      "Iteration 95830, Loss: 0.003607311286\n",
      "Iteration 95840, Loss: 0.003536775010\n",
      "Iteration 95850, Loss: 0.003506886074\n",
      "Iteration 95860, Loss: 0.003489543917\n",
      "Iteration 95870, Loss: 0.003522869200\n",
      "Iteration 95880, Loss: 0.003446159652\n",
      "Iteration 95890, Loss: 0.003434874117\n",
      "Iteration 95900, Loss: 0.003426927608\n",
      "Iteration 95910, Loss: 0.003425925970\n",
      "Iteration 95920, Loss: 0.003444216447\n",
      "Iteration 95930, Loss: 0.003416142194\n",
      "Iteration 95940, Loss: 0.003394993022\n",
      "Iteration 95950, Loss: 0.003423174843\n",
      "Iteration 95960, Loss: 0.004194221459\n",
      "Iteration 95970, Loss: 0.003701120848\n",
      "Iteration 95980, Loss: 0.003542976454\n",
      "Iteration 95990, Loss: 0.003459149273\n",
      "Iteration 96000, Loss: 0.003401306923\n",
      "Iteration 96010, Loss: 0.003750985023\n",
      "Iteration 96020, Loss: 0.003525687149\n",
      "Iteration 96030, Loss: 0.003407022450\n",
      "Iteration 96040, Loss: 0.003419858636\n",
      "Iteration 96050, Loss: 0.003378711874\n",
      "Iteration 96060, Loss: 0.003513929900\n",
      "Iteration 96070, Loss: 0.005050074309\n",
      "Iteration 96080, Loss: 0.003664707532\n",
      "Iteration 96090, Loss: 0.003443672555\n",
      "Iteration 96100, Loss: 0.003423713613\n",
      "Iteration 96110, Loss: 0.003376175184\n",
      "Iteration 96120, Loss: 0.003596957773\n",
      "Iteration 96130, Loss: 0.005006856751\n",
      "Iteration 96140, Loss: 0.003510681912\n",
      "Iteration 96150, Loss: 0.003465576796\n",
      "Iteration 96160, Loss: 0.003421318484\n",
      "Iteration 96170, Loss: 0.003369600512\n",
      "Iteration 96180, Loss: 0.003505157074\n",
      "Iteration 96190, Loss: 0.006168154068\n",
      "Iteration 96200, Loss: 0.003514759475\n",
      "Iteration 96210, Loss: 0.003668675665\n",
      "Iteration 96220, Loss: 0.003410988720\n",
      "Iteration 96230, Loss: 0.003397729015\n",
      "Iteration 96240, Loss: 0.003390698228\n",
      "Iteration 96250, Loss: 0.003405876923\n",
      "Iteration 96260, Loss: 0.006372667849\n",
      "Iteration 96270, Loss: 0.008152938448\n",
      "Iteration 96280, Loss: 0.004311642610\n",
      "Iteration 96290, Loss: 0.004026484210\n",
      "Iteration 96300, Loss: 0.004549731500\n",
      "Iteration 96310, Loss: 0.005737955216\n",
      "Iteration 96320, Loss: 0.004427052103\n",
      "Iteration 96330, Loss: 0.003803494852\n",
      "Iteration 96340, Loss: 0.003545494517\n",
      "Iteration 96350, Loss: 0.003448210889\n",
      "Iteration 96360, Loss: 0.003383139847\n",
      "Iteration 96370, Loss: 0.003380090930\n",
      "Iteration 96380, Loss: 0.003370882710\n",
      "Iteration 96390, Loss: 0.003371217521\n",
      "Iteration 96400, Loss: 0.003517434234\n",
      "Iteration 96410, Loss: 0.005448259413\n",
      "Iteration 96420, Loss: 0.003647441277\n",
      "Iteration 96430, Loss: 0.003412221326\n",
      "Iteration 96440, Loss: 0.003464420326\n",
      "Iteration 96450, Loss: 0.003376057837\n",
      "Iteration 96460, Loss: 0.003379795002\n",
      "Iteration 96470, Loss: 0.003365061944\n",
      "Iteration 96480, Loss: 0.003359084250\n",
      "Iteration 96490, Loss: 0.003980602138\n",
      "Iteration 96500, Loss: 0.004386510234\n",
      "Iteration 96510, Loss: 0.003967595287\n",
      "Iteration 96520, Loss: 0.003876361763\n",
      "Iteration 96530, Loss: 0.003557835007\n",
      "Iteration 96540, Loss: 0.003446015297\n",
      "Iteration 96550, Loss: 0.003378551453\n",
      "Iteration 96560, Loss: 0.003433032660\n",
      "Iteration 96570, Loss: 0.003456443548\n",
      "Iteration 96580, Loss: 0.003777075559\n",
      "Iteration 96590, Loss: 0.003969382029\n",
      "Iteration 96600, Loss: 0.003819197416\n",
      "Iteration 96610, Loss: 0.003598511918\n",
      "Iteration 96620, Loss: 0.003463952569\n",
      "Iteration 96630, Loss: 0.003649663646\n",
      "Iteration 96640, Loss: 0.003991655074\n",
      "Iteration 96650, Loss: 0.003931502812\n",
      "Iteration 96660, Loss: 0.003397234948\n",
      "Iteration 96670, Loss: 0.003538860474\n",
      "Iteration 96680, Loss: 0.003513687756\n",
      "Iteration 96690, Loss: 0.003410981735\n",
      "Iteration 96700, Loss: 0.003667594865\n",
      "Iteration 96710, Loss: 0.006920150481\n",
      "Iteration 96720, Loss: 0.003422254696\n",
      "Iteration 96730, Loss: 0.003626249032\n",
      "Iteration 96740, Loss: 0.003963437397\n",
      "Iteration 96750, Loss: 0.003548349487\n",
      "Iteration 96760, Loss: 0.003448426723\n",
      "Iteration 96770, Loss: 0.003383532166\n",
      "Iteration 96780, Loss: 0.003588406602\n",
      "Iteration 96790, Loss: 0.006116464268\n",
      "Iteration 96800, Loss: 0.004371007904\n",
      "Iteration 96810, Loss: 0.003720753826\n",
      "Iteration 96820, Loss: 0.003478154307\n",
      "Iteration 96830, Loss: 0.003420918947\n",
      "Iteration 96840, Loss: 0.003371586092\n",
      "Iteration 96850, Loss: 0.003361754585\n",
      "Iteration 96860, Loss: 0.003372053849\n",
      "Iteration 96870, Loss: 0.004991202150\n",
      "Iteration 96880, Loss: 0.003534193384\n",
      "Iteration 96890, Loss: 0.003625173587\n",
      "Iteration 96900, Loss: 0.003492297139\n",
      "Iteration 96910, Loss: 0.003390194383\n",
      "Iteration 96920, Loss: 0.004138044082\n",
      "Iteration 96930, Loss: 0.004788070451\n",
      "Iteration 96940, Loss: 0.003657518653\n",
      "Iteration 96950, Loss: 0.003469099756\n",
      "Iteration 96960, Loss: 0.003425124567\n",
      "Iteration 96970, Loss: 0.003367004450\n",
      "Iteration 96980, Loss: 0.004371391144\n",
      "Iteration 96990, Loss: 0.007286389358\n",
      "Iteration 97000, Loss: 0.006416127086\n",
      "Iteration 97010, Loss: 0.004832778126\n",
      "Iteration 97020, Loss: 0.004449856468\n",
      "Iteration 97030, Loss: 0.005714068189\n",
      "Iteration 97040, Loss: 0.006250327919\n",
      "Iteration 97050, Loss: 0.003845375264\n",
      "Iteration 97060, Loss: 0.003906556405\n",
      "Iteration 97070, Loss: 0.003532661824\n",
      "Iteration 97080, Loss: 0.003483739682\n",
      "Iteration 97090, Loss: 0.003433248494\n",
      "Iteration 97100, Loss: 0.003401057329\n",
      "Iteration 97110, Loss: 0.003386825090\n",
      "Iteration 97120, Loss: 0.003376268083\n",
      "Iteration 97130, Loss: 0.003369003069\n",
      "Iteration 97140, Loss: 0.003363141557\n",
      "Iteration 97150, Loss: 0.003358890302\n",
      "Iteration 97160, Loss: 0.003386167577\n",
      "Iteration 97170, Loss: 0.008109597489\n",
      "Iteration 97180, Loss: 0.004063817672\n",
      "Iteration 97190, Loss: 0.003854114562\n",
      "Iteration 97200, Loss: 0.003495186102\n",
      "Iteration 97210, Loss: 0.003406798001\n",
      "Iteration 97220, Loss: 0.003391765757\n",
      "Iteration 97230, Loss: 0.003372055013\n",
      "Iteration 97240, Loss: 0.003363761352\n",
      "Iteration 97250, Loss: 0.003375660861\n",
      "Iteration 97260, Loss: 0.003894353984\n",
      "Iteration 97270, Loss: 0.003614913905\n",
      "Iteration 97280, Loss: 0.003501837840\n",
      "Iteration 97290, Loss: 0.003407020587\n",
      "Iteration 97300, Loss: 0.003577684285\n",
      "Iteration 97310, Loss: 0.005964244716\n",
      "Iteration 97320, Loss: 0.003843345679\n",
      "Iteration 97330, Loss: 0.003394454252\n",
      "Iteration 97340, Loss: 0.003443828318\n",
      "Iteration 97350, Loss: 0.003439234570\n",
      "Iteration 97360, Loss: 0.003431928344\n",
      "Iteration 97370, Loss: 0.003512152703\n",
      "Iteration 97380, Loss: 0.003628505394\n",
      "Iteration 97390, Loss: 0.005395861343\n",
      "Iteration 97400, Loss: 0.003783281194\n",
      "Iteration 97410, Loss: 0.003398800967\n",
      "Iteration 97420, Loss: 0.003382016206\n",
      "Iteration 97430, Loss: 0.003439209424\n",
      "Iteration 97440, Loss: 0.004379513673\n",
      "Iteration 97450, Loss: 0.003985672258\n",
      "Iteration 97460, Loss: 0.003470171709\n",
      "Iteration 97470, Loss: 0.003404728835\n",
      "Iteration 97480, Loss: 0.003418493317\n",
      "Iteration 97490, Loss: 0.003366233781\n",
      "Iteration 97500, Loss: 0.003967821598\n",
      "Iteration 97510, Loss: 0.003730766475\n",
      "Iteration 97520, Loss: 0.003858624958\n",
      "Iteration 97530, Loss: 0.004927066155\n",
      "Iteration 97540, Loss: 0.003671521554\n",
      "Iteration 97550, Loss: 0.003636749228\n",
      "Iteration 97560, Loss: 0.003542283084\n",
      "Iteration 97570, Loss: 0.003432585159\n",
      "Iteration 97580, Loss: 0.003377552377\n",
      "Iteration 97590, Loss: 0.003399201203\n",
      "Iteration 97600, Loss: 0.004186884966\n",
      "Iteration 97610, Loss: 0.003496554913\n",
      "Iteration 97620, Loss: 0.003970452119\n",
      "Iteration 97630, Loss: 0.003398706671\n",
      "Iteration 97640, Loss: 0.003419300076\n",
      "Iteration 97650, Loss: 0.003352068365\n",
      "Iteration 97660, Loss: 0.003350981278\n",
      "Iteration 97670, Loss: 0.003537718672\n",
      "Iteration 97680, Loss: 0.006317253225\n",
      "Iteration 97690, Loss: 0.003533052979\n",
      "Iteration 97700, Loss: 0.003913985100\n",
      "Iteration 97710, Loss: 0.003598740092\n",
      "Iteration 97720, Loss: 0.003433200065\n",
      "Iteration 97730, Loss: 0.003401259892\n",
      "Iteration 97740, Loss: 0.003346019890\n",
      "Iteration 97750, Loss: 0.003387453733\n",
      "Iteration 97760, Loss: 0.009775882587\n",
      "Iteration 97770, Loss: 0.004502124619\n",
      "Iteration 97780, Loss: 0.005909639876\n",
      "Iteration 97790, Loss: 0.004705077037\n",
      "Iteration 97800, Loss: 0.004150605295\n",
      "Iteration 97810, Loss: 0.003600880271\n",
      "Iteration 97820, Loss: 0.003522844054\n",
      "Iteration 97830, Loss: 0.003413545433\n",
      "Iteration 97840, Loss: 0.003390756436\n",
      "Iteration 97850, Loss: 0.003471648786\n",
      "Iteration 97860, Loss: 0.004097745288\n",
      "Iteration 97870, Loss: 0.003565907711\n",
      "Iteration 97880, Loss: 0.003507251153\n",
      "Iteration 97890, Loss: 0.003556841752\n",
      "Iteration 97900, Loss: 0.003417152213\n",
      "Iteration 97910, Loss: 0.003410473932\n",
      "Iteration 97920, Loss: 0.003430692246\n",
      "Iteration 97930, Loss: 0.005239836406\n",
      "Iteration 97940, Loss: 0.003798360471\n",
      "Iteration 97950, Loss: 0.003731339937\n",
      "Iteration 97960, Loss: 0.003563168459\n",
      "Iteration 97970, Loss: 0.003390120342\n",
      "Iteration 97980, Loss: 0.003406737465\n",
      "Iteration 97990, Loss: 0.004135849420\n",
      "Iteration 98000, Loss: 0.003619115800\n",
      "Iteration 98010, Loss: 0.003713320941\n",
      "Iteration 98020, Loss: 0.003551710397\n",
      "Iteration 98030, Loss: 0.003372969572\n",
      "Iteration 98040, Loss: 0.003352426225\n",
      "Iteration 98050, Loss: 0.003578383708\n",
      "Iteration 98060, Loss: 0.005246995017\n",
      "Iteration 98070, Loss: 0.003748161253\n",
      "Iteration 98080, Loss: 0.003562549362\n",
      "Iteration 98090, Loss: 0.003432198195\n",
      "Iteration 98100, Loss: 0.003365355311\n",
      "Iteration 98110, Loss: 0.003367024707\n",
      "Iteration 98120, Loss: 0.004265591968\n",
      "Iteration 98130, Loss: 0.007430581376\n",
      "Iteration 98140, Loss: 0.005659974646\n",
      "Iteration 98150, Loss: 0.005188782699\n",
      "Iteration 98160, Loss: 0.004380922765\n",
      "Iteration 98170, Loss: 0.003746630857\n",
      "Iteration 98180, Loss: 0.003552114824\n",
      "Iteration 98190, Loss: 0.003431433812\n",
      "Iteration 98200, Loss: 0.003380849957\n",
      "Iteration 98210, Loss: 0.003361648181\n",
      "Iteration 98220, Loss: 0.003350909334\n",
      "Iteration 98230, Loss: 0.003344099270\n",
      "Iteration 98240, Loss: 0.003339485498\n",
      "Iteration 98250, Loss: 0.003586807055\n",
      "Iteration 98260, Loss: 0.003845397383\n",
      "Iteration 98270, Loss: 0.003670912003\n",
      "Iteration 98280, Loss: 0.003408056218\n",
      "Iteration 98290, Loss: 0.003405659925\n",
      "Iteration 98300, Loss: 0.003377377056\n",
      "Iteration 98310, Loss: 0.003362035844\n",
      "Iteration 98320, Loss: 0.003361462848\n",
      "Iteration 98330, Loss: 0.003374621039\n",
      "Iteration 98340, Loss: 0.003493621713\n",
      "Iteration 98350, Loss: 0.003833063412\n",
      "Iteration 98360, Loss: 0.003760379739\n",
      "Iteration 98370, Loss: 0.003527125809\n",
      "Iteration 98380, Loss: 0.003363203257\n",
      "Iteration 98390, Loss: 0.003510433016\n",
      "Iteration 98400, Loss: 0.006388730370\n",
      "Iteration 98410, Loss: 0.003593573812\n",
      "Iteration 98420, Loss: 0.003772354219\n",
      "Iteration 98430, Loss: 0.003359220922\n",
      "Iteration 98440, Loss: 0.003374994965\n",
      "Iteration 98450, Loss: 0.003335615853\n",
      "Iteration 98460, Loss: 0.003551264992\n",
      "Iteration 98470, Loss: 0.006119736470\n",
      "Iteration 98480, Loss: 0.004515555687\n",
      "Iteration 98490, Loss: 0.003468345851\n",
      "Iteration 98500, Loss: 0.003519894555\n",
      "Iteration 98510, Loss: 0.003581733676\n",
      "Iteration 98520, Loss: 0.003457398154\n",
      "Iteration 98530, Loss: 0.004598541651\n",
      "Iteration 98540, Loss: 0.015757925808\n",
      "Iteration 98550, Loss: 0.007401150186\n",
      "Iteration 98560, Loss: 0.004556874745\n",
      "Iteration 98570, Loss: 0.004290511366\n",
      "Iteration 98580, Loss: 0.003909148742\n",
      "Iteration 98590, Loss: 0.003739491338\n",
      "Iteration 98600, Loss: 0.003666859120\n",
      "Iteration 98610, Loss: 0.003641485237\n",
      "Iteration 98620, Loss: 0.009206031449\n",
      "Iteration 98630, Loss: 0.003637639340\n",
      "Iteration 98640, Loss: 0.003536679083\n",
      "Iteration 98650, Loss: 0.003480952000\n",
      "Iteration 98660, Loss: 0.003450752469\n",
      "Iteration 98670, Loss: 0.003444096306\n",
      "Iteration 98680, Loss: 0.003404469462\n",
      "Iteration 98690, Loss: 0.003393220017\n",
      "Iteration 98700, Loss: 0.003379024332\n",
      "Iteration 98710, Loss: 0.003368363250\n",
      "Iteration 98720, Loss: 0.003379385686\n",
      "Iteration 98730, Loss: 0.006248971447\n",
      "Iteration 98740, Loss: 0.003784904256\n",
      "Iteration 98750, Loss: 0.003415937768\n",
      "Iteration 98760, Loss: 0.003423816757\n",
      "Iteration 98770, Loss: 0.003386124969\n",
      "Iteration 98780, Loss: 0.003356084693\n",
      "Iteration 98790, Loss: 0.003357394366\n",
      "Iteration 98800, Loss: 0.003347186139\n",
      "Iteration 98810, Loss: 0.003336008638\n",
      "Iteration 98820, Loss: 0.003348202212\n",
      "Iteration 98830, Loss: 0.006173088215\n",
      "Iteration 98840, Loss: 0.004632791504\n",
      "Iteration 98850, Loss: 0.003820067272\n",
      "Iteration 98860, Loss: 0.003549197223\n",
      "Iteration 98870, Loss: 0.003460384207\n",
      "Iteration 98880, Loss: 0.003476882586\n",
      "Iteration 98890, Loss: 0.003362371121\n",
      "Iteration 98900, Loss: 0.003337696195\n",
      "Iteration 98910, Loss: 0.003354156157\n",
      "Iteration 98920, Loss: 0.005459595472\n",
      "Iteration 98930, Loss: 0.005018350203\n",
      "Iteration 98940, Loss: 0.003606892191\n",
      "Iteration 98950, Loss: 0.003363791155\n",
      "Iteration 98960, Loss: 0.003419826273\n",
      "Iteration 98970, Loss: 0.003387733130\n",
      "Iteration 98980, Loss: 0.003589743748\n",
      "Iteration 98990, Loss: 0.003403308569\n",
      "Iteration 99000, Loss: 0.003361601615\n",
      "Iteration 99010, Loss: 0.005468292162\n",
      "Iteration 99020, Loss: 0.006419043988\n",
      "Iteration 99030, Loss: 0.004245748278\n",
      "Iteration 99040, Loss: 0.003811494447\n",
      "Iteration 99050, Loss: 0.003823339008\n",
      "Iteration 99060, Loss: 0.003503639251\n",
      "Iteration 99070, Loss: 0.003415259765\n",
      "Iteration 99080, Loss: 0.003373238957\n",
      "Iteration 99090, Loss: 0.003350222250\n",
      "Iteration 99100, Loss: 0.003340971190\n",
      "Iteration 99110, Loss: 0.003334711539\n",
      "Iteration 99120, Loss: 0.003430084093\n",
      "Iteration 99130, Loss: 0.008873193525\n",
      "Iteration 99140, Loss: 0.003857001197\n",
      "Iteration 99150, Loss: 0.003583034500\n",
      "Iteration 99160, Loss: 0.003482274245\n",
      "Iteration 99170, Loss: 0.003372207051\n",
      "Iteration 99180, Loss: 0.003351053456\n",
      "Iteration 99190, Loss: 0.003409913508\n",
      "Iteration 99200, Loss: 0.003403052222\n",
      "Iteration 99210, Loss: 0.003359592985\n",
      "Iteration 99220, Loss: 0.003939819057\n",
      "Iteration 99230, Loss: 0.003765670815\n",
      "Iteration 99240, Loss: 0.003785291221\n",
      "Iteration 99250, Loss: 0.003455332015\n",
      "Iteration 99260, Loss: 0.003404702293\n",
      "Iteration 99270, Loss: 0.003412376624\n",
      "Iteration 99280, Loss: 0.003328055376\n",
      "Iteration 99290, Loss: 0.003615627065\n",
      "Iteration 99300, Loss: 0.005536880344\n",
      "Iteration 99310, Loss: 0.004601760302\n",
      "Iteration 99320, Loss: 0.003688829485\n",
      "Iteration 99330, Loss: 0.003516603727\n",
      "Iteration 99340, Loss: 0.003372542094\n",
      "Iteration 99350, Loss: 0.003393284511\n",
      "Iteration 99360, Loss: 0.003694029059\n",
      "Iteration 99370, Loss: 0.003540493548\n",
      "Iteration 99380, Loss: 0.004571091384\n",
      "Iteration 99390, Loss: 0.003438710002\n",
      "Iteration 99400, Loss: 0.003459357191\n",
      "Iteration 99410, Loss: 0.003390403697\n",
      "Iteration 99420, Loss: 0.003346641781\n",
      "Iteration 99430, Loss: 0.003453274257\n",
      "Iteration 99440, Loss: 0.008014379069\n",
      "Iteration 99450, Loss: 0.004920455627\n",
      "Iteration 99460, Loss: 0.003924835473\n",
      "Iteration 99470, Loss: 0.003435864579\n",
      "Iteration 99480, Loss: 0.003405428957\n",
      "Iteration 99490, Loss: 0.003414462786\n",
      "Iteration 99500, Loss: 0.003363503842\n",
      "Iteration 99510, Loss: 0.003351533320\n",
      "Iteration 99520, Loss: 0.003329131752\n",
      "Iteration 99530, Loss: 0.009542008862\n",
      "Iteration 99540, Loss: 0.025293365121\n",
      "Iteration 99550, Loss: 0.012680575252\n",
      "Iteration 99560, Loss: 0.007789311931\n",
      "Iteration 99570, Loss: 0.006122592371\n",
      "Iteration 99580, Loss: 0.005194269586\n",
      "Iteration 99590, Loss: 0.004652908072\n",
      "Iteration 99600, Loss: 0.004348077346\n",
      "Iteration 99610, Loss: 0.008755088784\n",
      "Iteration 99620, Loss: 0.005481113680\n",
      "Iteration 99630, Loss: 0.004562512971\n",
      "Iteration 99640, Loss: 0.004075059202\n",
      "Iteration 99650, Loss: 0.003846816951\n",
      "Iteration 99660, Loss: 0.003771390766\n",
      "Iteration 99670, Loss: 0.003731825389\n",
      "Iteration 99680, Loss: 0.003693075152\n",
      "Iteration 99690, Loss: 0.003654109547\n",
      "Iteration 99700, Loss: 0.003623781493\n",
      "Iteration 99710, Loss: 0.003595675109\n",
      "Iteration 99720, Loss: 0.003569365013\n",
      "Iteration 99730, Loss: 0.003544797422\n",
      "Iteration 99740, Loss: 0.003521669423\n",
      "Iteration 99750, Loss: 0.003499969142\n",
      "Iteration 99760, Loss: 0.003479801351\n",
      "Iteration 99770, Loss: 0.003461280605\n",
      "Iteration 99780, Loss: 0.003444453701\n",
      "Iteration 99790, Loss: 0.003429264762\n",
      "Iteration 99800, Loss: 0.003415595274\n",
      "Iteration 99810, Loss: 0.003403407056\n",
      "Iteration 99820, Loss: 0.003416260704\n",
      "Iteration 99830, Loss: 0.005044665653\n",
      "Iteration 99840, Loss: 0.004756362177\n",
      "Iteration 99850, Loss: 0.003533527954\n",
      "Iteration 99860, Loss: 0.003429129254\n",
      "Iteration 99870, Loss: 0.003438073909\n",
      "Iteration 99880, Loss: 0.003367543453\n",
      "Iteration 99890, Loss: 0.003356181551\n",
      "Iteration 99900, Loss: 0.003353643231\n",
      "Iteration 99910, Loss: 0.003355092369\n",
      "Iteration 99920, Loss: 0.005340693519\n",
      "Iteration 99930, Loss: 0.004761912860\n",
      "Iteration 99940, Loss: 0.003782582702\n",
      "Iteration 99950, Loss: 0.003476403886\n",
      "Iteration 99960, Loss: 0.003355893306\n",
      "Iteration 99970, Loss: 0.003372043138\n",
      "Iteration 99980, Loss: 0.003352633212\n",
      "Iteration 99990, Loss: 0.003333819797\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "net=Pinns()\n",
    "net.train(100000)\n",
    "net.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given validation data\n",
    "x_validation = np.array([1, 8, 9, 10, 14, 23, 37, 59, 65, 80, 95, 110, 123, 124, 125, 126, 129])/129\n",
    "u_validation2 = np.array([0, -0.03717, -0.04192, -0.04775, -0.06434, -0.1015, -0.15662, -0.21090,\n",
    "                          -0.20581, -0.13641, 0.00332, 0.23151, 0.68717, 0.73722, 0.78871, 0.84123, 1])\n",
    "\n",
    "y_validation = np.array([1, 9, 10, 11, 13, 21, 30, 31, 65, 104, 111, 117, 122, 123, 124, 125, 129])/129\n",
    "v_validation2 = np.array([0, 0.09233, 0.10091, 0.10890, 0.12317, 0.16077, 0.17507, 0.17527, 0.05454,\n",
    "                          -0.24533, -0.22445, -0.16914, -0.10313, -0.08864, -0.07391, -0.05906, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u=net.uvp_P[:,0]\n",
    "u=u.reshape(len(net.x), len(net.y)).detach().cpu().numpy()\n",
    "plt.figure(figsize=(8, 5))\n",
    "x=torch.arange(0,1+1/128,1/128)\n",
    "plt.plot(net.x, u[51,:], 'b-', linewidth=2, label='u velocity for PINNs')\n",
    "plt.plot(x_validation, u_validation2, 'ko', label=\"u validation\", markersize=6, linewidth=2)\n",
    "plt.xlabel(\"y\")\n",
    "plt.ylabel(\"U velocity\")\n",
    "plt.title(\"U Velocity Profile along the Vertical Centerline (x = 0.5)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "v=net.uvp_P[:,1]\n",
    "v=v.reshape(len(net.x), len(net.y)).detach().cpu().numpy()\n",
    "plt.figure(figsize=(8, 5))\n",
    "x=torch.arange(0,1+1/128,1/128)\n",
    "plt.plot(net.x, v[:,51], 'b-', linewidth=2, label='v velocity for PINNs')\n",
    "plt.plot(y_validation, v_validation2, 'ko', label=\"v validation\", markersize=6, linewidth=2)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"V velocity\")\n",
    "plt.title(\"V Velocity Profile along the Horizontal Centerline (y = 0.5)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
